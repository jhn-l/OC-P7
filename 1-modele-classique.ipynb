{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, learning_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, auc, precision_score, recall_score, roc_curve, f1_score\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "import mlflow\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "#mlflow.set_tracking_uri(\"http://mlflow-server:5000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "\n",
    "# Définir un nouveau répertoire de suivi, par exemple, un dossier spécifique pour les expériences MLflow\n",
    "tracking_dir = \"/tmp/mlruns\"  # Vous pouvez choisir un autre répertoire si nécessaire\n",
    "os.makedirs(tracking_dir, exist_ok=True)  # Crée le dossier s'il n'existe pas\n",
    "\n",
    "#mlflow.set_tracking_uri(f\"file://{tracking_dir}\")\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracking experiment in mlflow\n",
    "#mlflow.set_experiment(\"1-modele-simple\")\n",
    "#Nom de l'expérience\n",
    "# experiment_name = \"modele-simple\"\n",
    "\n",
    "# from mlflow.tracking import MlflowClient\n",
    "# # Assurer que l'expérience existe et récupérer son ID\n",
    "# client = MlflowClient()\n",
    "# try:\n",
    "#     experiment_id = client.create_experiment(experiment_name)\n",
    "# except mlflow.exceptions.RestException:\n",
    "#     experiment_id = client.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "#experiment_id = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/code/data/archive/'\n",
    "df = pd.read_csv(path + 'training.1600000.processed.noemoticon.csv', sep=',',  encoding='latin-1')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mettre un header au dataframe\n",
    "df.columns = ['target', 'ids', 'date', 'flag', 'user', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer toutes les lignes ayant des doublons dans la colonne 'text', y compris la première occurrence\n",
    "display(df.shape)\n",
    "df = df[~df['text'].duplicated(keep=False)].reset_index(drop=True)\n",
    "\n",
    "# Vérifier le nombre de lignes après suppression\n",
    "print(f\"Nombre de lignes après suppression complète des doublons :\\n {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier s'il y a des valeurs manquantes dans la colonne 'target'\n",
    "missing_target_count = df['target'].isnull().sum()\n",
    "\n",
    "if missing_target_count == 0:\n",
    "    print(\"Toutes les lignes de la colonne 'target' contiennent une valeur.\")\n",
    "else:\n",
    "    print(f\"Il y a {missing_target_count} lignes sans valeur dans la colonne 'target'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voir les valeurs uniques du dataframe\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voir si il y a des valeurs manquantes\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribion des valeurs de la colonne 'target' ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un histogramme des valeurs de la colonne 'target' avec des étiquettes spécifiques et le nombre total de valeurs\n",
    "\n",
    "# Compter les occurrences de chaque valeur unique dans 'target' avec les valeurs 0, 2, et 4\n",
    "target_counts = df['target'].value_counts().reindex([0, 2, 4], fill_value=0)\n",
    "\n",
    "# Configurer le graphique\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(['Negative (0)', 'Neutral (2)', 'Positive (4)'], target_counts, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Ajouter les annotations (le nombre total au-dessus de chaque colonne)\n",
    "for bar, count in zip(bars, target_counts):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{count}', ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "# Personnaliser l'apparence du graphique\n",
    "plt.title(\"Distribution des valeurs de la colonne 'target'\")\n",
    "plt.xlabel(\"Polarity of Tweet\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse du graphique\n",
    "\n",
    "Le graphique ci-dessus montre la distribution des valeurs de la colonne `target` dans votre dataset de tweets, qui est utilisé pour l'analyse de sentiments. Voici une analyse détaillée :\n",
    "\n",
    "1. **Distribution des sentiments** : Le graphique indique qu'il y a deux catégories majoritaires :\n",
    "   - **Negative (0)** : 799,999 tweets\n",
    "   - **Positive (4)** : 800,000 tweets\n",
    "   - **Neutral (2)** : Aucun tweet n'est présent dans cette catégorie.\n",
    "\n",
    "   Cela signifie que le dataset est composé exclusivement de sentiments polarisés, soit négatifs, soit positifs, sans tweets neutres. Cela pourrait poser un défi pour les modèles d'apprentissage, car ils ne verront aucune donnée pour la catégorie \"neutre\".\n",
    "\n",
    "2. **Équilibre des classes** : Les deux classes `Negative` et `Positive` sont presque parfaitement équilibrées, avec seulement un tweet de différence. Cet équilibre est avantageux pour l'apprentissage supervisé car il minimise les risques de biais en faveur d'une classe spécifique.\n",
    "\n",
    "3. **Données manquantes** : L'absence de tweets neutres (valeur `2`) peut poser problème si l'on souhaite modéliser ou prédire cette classe. Si une détection de sentiment neutre est requise, il serait idéal de compléter le dataset avec des tweets de cette catégorie.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Le graphique de distribution des valeurs de la colonne `target` montre une absence de tweets neutres (valeur `2`). Étant donné que seules les catégories **Negative (0)** et **Positive (4)** sont présentes en quantités équilibrées, il est judicieux de simplifier le problème en une **classification binaire** entre sentiments négatifs et positifs.\n",
    "\n",
    "Cette approche de classification binaire sera plus efficace, car il n'y a pas de données pour entraîner un modèle à détecter les tweets neutres.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLFlow\n",
    "\n",
    "- Fonction pour intégrer MLFLOW pour chaque modèle de machine learning testé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer pour un sous-ensemble\n",
    "df_sample = df.sample(frac=0.5, random_state=42)\n",
    "df_data = df_sample[['target', 'text']]\n",
    "df_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les features et labels\n",
    "df_sample['target'] = df_sample['target'].apply(lambda x: 1 if x == 4 else 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['target'].value_counts()\n",
    "df_sample.to_pickle('df_sample.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les stopwords anglais\n",
    "english_stopwords = list(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe de prétraitement et vectorisation des tweets\n",
    "class TweetVectorizer:\n",
    "    def __init__(self, vectorizer_type='tfidf', method='lemmatize'):\n",
    "        self.vectorizer_type = vectorizer_type\n",
    "        self.method = method\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        # Initialiser le vectorizer\n",
    "        if vectorizer_type == 'count':\n",
    "            self.vectorizer = CountVectorizer(stop_words=english_stopwords)\n",
    "        elif vectorizer_type == 'tfidf':\n",
    "            self.vectorizer = TfidfVectorizer(stop_words=english_stopwords)\n",
    "        else:\n",
    "            raise ValueError(\"vectorizer_type doit être 'count' ou 'tfidf'\")\n",
    "    \n",
    "    def clean_tweet(self, tweet):\n",
    "        # Convertir le tweet en minuscules\n",
    "        tweet = tweet.lower()\n",
    "        # Supprimer les URL commencant par 'http' ou 'https' ou 'www'\n",
    "        tweet = re.sub(r'www\\S+', '', tweet)\n",
    "        tweet = re.sub(r'http\\S+', '', tweet)\n",
    "        # Supprimer les mentions\n",
    "        tweet = re.sub(r'@\\w+', '', tweet)\n",
    "        # Supprimer les hashtags\n",
    "        tweet = re.sub(r'#\\w+', '', tweet)\n",
    "        # Supprimer les caractères spéciaux et les chiffres\n",
    "        tweet = re.sub(r'[^A-Za-z ]+', ' ', tweet)\n",
    "        # Supprimer les espaces supplémentaires\n",
    "        tweet = re.sub(r'\\s+', ' ', tweet)\n",
    "        return tweet\n",
    "        \n",
    "    def preprocess(self, tweet):\n",
    "        cleaned_tweet = self.clean_tweet(tweet)\n",
    "        tokens = cleaned_tweet.split()\n",
    "        \n",
    "        # Appliquer le stemming ou la lemmatisation\n",
    "        if self.method == 'stem':\n",
    "            tokens = [self.stemmer.stem(token) for token in tokens]\n",
    "        elif self.method == 'lemmatize':\n",
    "            tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def fit_transform(self, documents):\n",
    "        # Prétraitement des documents\n",
    "        documents_preprocessed = [self.preprocess(doc) for doc in documents]\n",
    "        \n",
    "        # Suivi de la vectorisation dans MLflow\n",
    "        with mlflow.start_run(run_name=\"Vectorization\", nested=True):\n",
    "            mlflow.set_tag(\"Stage\", \"Vectorization\")\n",
    "            mlflow.set_tag(\"vectorizer_type\", self.vectorizer_type)\n",
    "            mlflow.set_tag(\"preprocessing_method\", self.method)\n",
    "            \n",
    "            # Enregistrer le temps de traitement de la vectorisation\n",
    "            start_time = time.time()\n",
    "            self.vectorizer.fit(documents_preprocessed)  # Correction: Entraînement du vectorizer avant la transformation\n",
    "            X = self.vectorizer.transform(documents_preprocessed)  # Transformation des données\n",
    "            \n",
    "            end_time = time.time()\n",
    "            processing_time = end_time - start_time\n",
    "            mlflow.log_metric(\"vectorization_time_seconds\", processing_time)\n",
    "            \n",
    "            # Enregistrer la densité de la matrice de caractéristiques\n",
    "            density = X.nnz / float(X.shape[0] * X.shape[1])\n",
    "            mlflow.log_metric(\"matrix_density\", density)\n",
    "            \n",
    "            # Enregistrer le nombre de caractéristiques\n",
    "            num_features = X.shape[1]\n",
    "            mlflow.log_metric(\"num_features\", num_features)\n",
    "        return X\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        # Transformation des nouveaux documents en utilisant le vectorizer entraîné\n",
    "        documents_preprocessed = [self.preprocess(doc) for doc in documents]\n",
    "        X = self.vectorizer.transform(documents_preprocessed)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe pour la classification avec régression logistique\n",
    "class TweetClassifier:\n",
    "    def __init__(self, vectorizer_type='tfidf', method='lemmatize'):\n",
    "        self.model = LogisticRegression(C=1.0, solver='lbfgs', penalty='l2')\n",
    "        self.vectorizer_type = vectorizer_type\n",
    "        self.method = method\n",
    "\n",
    "    # Méthode pour obtenir le type de vectorizer\n",
    "    def get_vectorizer_type(self):\n",
    "        return self.vectorizer.vectorizer_type\n",
    "\n",
    "    # Méthode pour obtenir la méthode de prétraitement\n",
    "    def get_vectorizer_method(self):\n",
    "        return self.vectorizer.method\n",
    "\n",
    "    def train_and_evaluate(self, X_train, X_val, y_train, y_val):\n",
    "        # Suivi de l'entraînement du modèle dans MLflow comme sous-exécution\n",
    "        with mlflow.start_run(run_name=\"Validation\", nested=True):\n",
    "            mlflow.set_tag(\"Stage\", \"Validation\")\n",
    "            mlflow.set_tag(\"model\", \"Logistic Regression\")\n",
    "            \n",
    "            # Enregistrer les hyperparamètres du modèle\n",
    "            mlflow.log_param(\"C\", self.model.get_params()['C'])\n",
    "            mlflow.log_param(\"solver\", self.model.get_params()['solver'])\n",
    "            mlflow.log_param(\"penalty\", self.model.get_params()['penalty'])\n",
    "            \n",
    "            # Cross-validation sur le set d'entraînement\n",
    "            cv_results = cross_validate(self.model, X_train, y_train, cv=5, \n",
    "                                        scoring=['roc_auc', 'f1', 'accuracy', 'precision', 'recall'],\n",
    "                                        return_train_score=False)\n",
    "            \n",
    "            # Stockage des résultats de cross-validation\n",
    "            metrics_dict = {\n",
    "                \"CrossVal ROC_AUC\": round(cv_results['test_roc_auc'].mean(), 3),\n",
    "                \"CrossVal F1\": round(cv_results['test_f1'].mean(), 3),\n",
    "                \"CrossVal Accuracy\": round(cv_results['test_accuracy'].mean(), 3),\n",
    "                \"CrossVal Precision\": round(cv_results['test_precision'].mean(), 3),\n",
    "                \"CrossVal Recall\": round(cv_results['test_recall'].mean(), 3),\n",
    "                \"CrossVal Fit Time\": round(cv_results['fit_time'].mean(), 3)\n",
    "            }\n",
    "            \n",
    "            # Enregistrer les métriques de cross-validation dans MLflow\n",
    "            mlflow.log_metrics(metrics_dict)\n",
    "            \n",
    "            # Entraînement du modèle sur l'ensemble d'entraînement complet et tracking du temps de fit\n",
    "            start_time_fit = time.time()\n",
    "            self.model.fit(X_train, y_train)\n",
    "            end_time_fit = time.time()\n",
    "            fit_time = end_time_fit - start_time_fit\n",
    "            mlflow.log_metric(\"fit_time_seconds\", fit_time)\n",
    "            \n",
    "            # Prédiction sur l'ensemble de validation et tracking du temps de prédiction\n",
    "            start_time_predict = time.time()\n",
    "            y_val_pred = self.model.predict(X_val)\n",
    "            end_time_predict = time.time()\n",
    "            predict_time = end_time_predict - start_time_predict\n",
    "            \n",
    "            # Calcul des probabilités pour ROC AUC\n",
    "            y_val_prob = self.model.predict_proba(X_val)[:, 1]\n",
    "            \n",
    "            # Calcul des métriques de validation uniquement\n",
    "            validation_metrics = {\n",
    "                \"Validation Accuracy\": round(accuracy_score(y_val, y_val_pred), 3),\n",
    "                \"Validation ROC_AUC\": round(roc_auc_score(y_val, y_val_prob), 3),\n",
    "                \"Validation F1\": round(f1_score(y_val, y_val_pred), 3),\n",
    "                \"Validation Precision\": round(precision_score(y_val, y_val_pred), 3),\n",
    "                \"Validation Recall\": round(recall_score(y_val, y_val_pred), 3),\n",
    "                \"Validation Predict Time\": round(predict_time, 3)\n",
    "            }\n",
    "            \n",
    "            mlflow.log_table(validation_metrics, \"validation_metrics_table.json\")\n",
    "            \n",
    "            # Enregistrer les métriques de validation dans MLflow\n",
    "            mlflow.log_metrics(validation_metrics)\n",
    "            \n",
    "            # Tracer et enregistrer la courbe ROC\n",
    "            fpr, tpr, _ = roc_curve(y_val, y_val_prob)\n",
    "            plt.figure()\n",
    "            plt.plot(fpr, tpr, label=f\"ROC curve (area = {validation_metrics['Validation ROC_AUC']:.2f})\")\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.xlabel(\"False Positive Rate\")\n",
    "            plt.ylabel(\"True Positive Rate\")\n",
    "            plt.title(\"ROC Curve - Validation Set\")\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.savefig(\"roc_curve_val.png\")\n",
    "            mlflow.log_artifact(\"roc_curve_val.png\")\n",
    "            plt.close()\n",
    "            \n",
    "            # Tracer et enregistrer la courbe d'apprentissage\n",
    "            train_sizes, train_scores, val_scores = learning_curve(\n",
    "                self.model, X_train, y_train, cv=5, scoring=\"accuracy\", n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 5)\n",
    "            )\n",
    "            \n",
    "            train_scores_mean = np.mean(train_scores, axis=1)\n",
    "            val_scores_mean = np.mean(val_scores, axis=1)\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.plot(train_sizes, train_scores_mean, label=\"Training score\")\n",
    "            plt.plot(train_sizes, val_scores_mean, label=\"Validation score\")\n",
    "            plt.xlabel(\"Training Set Size\")\n",
    "            plt.ylabel(\"Accuracy\")\n",
    "            plt.title(\"Learning Curve\")\n",
    "            plt.legend(loc=\"best\")\n",
    "            plt.grid()\n",
    "            plt.savefig(\"learning_curve.png\")\n",
    "            mlflow.log_artifact(\"learning_curve.png\")\n",
    "            plt.close()\n",
    "            \n",
    "            # Sauvegarde du modèle pour reproductibilité\n",
    "            mlflow.sklearn.log_model(self.model, \"RegLog_\"+self.method+\"_\"+self.vectorizer_type)\n",
    "            \n",
    "            run_id = mlflow.active_run().info.run_id\n",
    "            result = mlflow.register_model(\n",
    "                model_uri=f\"runs:/{run_id}/model\",\n",
    "                name=f\"Reglog_{self.method}_{self.vectorizer_type}\"\n",
    "            )\n",
    "        \n",
    "            \n",
    "            # Affichage des métriques finales\n",
    "            print(\"Cross-Validation Metrics:\", metrics_dict)\n",
    "            print(\"Validation Metrics:\", validation_metrics)\n",
    "    \n",
    "    def final_evaluation(self, X_test, y_test):\n",
    "        # Évaluation finale sur le set de test\n",
    "        with mlflow.start_run(run_name=\"Final Test Evaluation\", nested=True):\n",
    "            mlflow.set_tag(\"Stage\", \"Final Test Evaluation\")\n",
    "            y_test_pred = self.model.predict(X_test)\n",
    "            test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "            mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "            mlflow.log_text(classification_report(y_test, y_test_pred), \"test_classification_report.txt\")\n",
    "            \n",
    "            # Calcul de la courbe ROC-AUC sur l'ensemble de test\n",
    "            y_test_prob = self.model.predict_proba(X_test)[:, 1]\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_test_prob)\n",
    "            roc_auc_test = auc(fpr, tpr)\n",
    "            mlflow.log_metric(\"test_roc_auc\", roc_auc_test)\n",
    "            metrics_dict = {\n",
    "                \"test_accuracy\": test_accuracy,\n",
    "                \"test_roc_auc\": roc_auc_test\n",
    "            }\n",
    "            mlflow.log_table(metrics_dict, \"test_metrics_table.json\")\n",
    "            mlflow.set_tag(\"test_accuracy\", test_accuracy)\n",
    "            mlflow.set_tag(\"final_evaluation_metric\", \"test_roc_auc\")\n",
    "\n",
    "            # Tracer et enregistrer la courbe ROC pour le set de test\n",
    "            plt.figure()\n",
    "            plt.plot(fpr, tpr, label=f\"ROC curve (area = {roc_auc_test:.2f})\")\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.xlabel(\"False Positive Rate\")\n",
    "            plt.ylabel(\"True Positive Rate\")\n",
    "            plt.title(\"ROC Curve - Test Set\")\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.savefig(\"roc_curve_test.png\")\n",
    "            mlflow.log_artifact(\"roc_curve_test.png\")\n",
    "            plt.close()\n",
    "            \n",
    "            # Enregistrer le modèle final\n",
    "            mlflow.sklearn.log_model(self.model, \"logistic_regression_model\")\n",
    "            print(f\"\\n Test Accuracy : {test_accuracy}\")\n",
    "            print(classification_report(y_test, y_test_pred))\n",
    "            print(f\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des combinaisons à tester\n",
    "configurations = [\n",
    "    {'vectorizer_type': 'count', 'method': 'lemmatize'},\n",
    "    {'vectorizer_type': 'count', 'method': 'stem'},\n",
    "    {'vectorizer_type': 'tfidf', 'method': 'lemmatize'},\n",
    "    {'vectorizer_type': 'tfidf', 'method': 'stem'}\n",
    "]\n",
    "#  mlflow server --backend-store-uri postgresql://postgres:postgres@postgres/mydb --default-artifact-root /tmp/mlru\n",
    "# ns/artifacts --host 0.0.0.0 --port 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparer les données en Train, Validation et Test une seule fois\n",
    "# Utiliser les tweets originaux pour diviser les ensembles de manière cohérente\n",
    "experiment = mlflow.set_experiment(\"modele-simple\")\n",
    "y = df_sample['target']\n",
    "tweets = df_sample['text']\n",
    "y_train_val, y_test = train_test_split(y, test_size=0.2, random_state=42)\n",
    "tweets_train_val, tweets_test = train_test_split(tweets, test_size=0.2, random_state=42)\n",
    "\n",
    "# Rediviser l'ensemble Train + Validation en Train et Validation\n",
    "y_train, y_val = train_test_split(y_train_val, test_size=0.25, random_state=42)\n",
    "tweets_train, tweets_val = train_test_split(tweets_train_val, test_size=0.25, random_state=42)\n",
    "\n",
    "# Boucle pour tester chaque configuration\n",
    "for config in configurations:\n",
    "    # Démarrer une nouvelle exécution parent pour chaque configuration\n",
    "    with mlflow.start_run(run_name=f\"{config['vectorizer_type']} + {config['method']}\", nested=False):\n",
    "        mlflow.set_tag(\"Model_Type\", \"Logistic Regression\")\n",
    "        mlflow.set_tag(\"vectorizer_type\", config['vectorizer_type'])\n",
    "        mlflow.set_tag(\"preprocessing_method\", config['method'])\n",
    "        mlflow.set_tag(\"Model_Type\", \"Logistic Regression\")\n",
    "        mlflow.set_tag(\"vectorizer_type\", config['vectorizer_type'])\n",
    "        mlflow.set_tag(\"preprocessing_method\", config['method'])\n",
    "        print(f\"\\n Testing configuration: {config}\")\n",
    "        \n",
    "        # Enregistrer les paramètres de configuration comme tags\n",
    "        mlflow.set_tag(\"vectorizer_type\", config['vectorizer_type'])\n",
    "        mlflow.set_tag(\"preprocessing_method\", config['method'])\n",
    "        \n",
    "        # Initialiser et appliquer la vectorisation avec TweetVectorizer\n",
    "        tweet_vectorizer = TweetVectorizer(vectorizer_type=config['vectorizer_type'], method=config['method'])\n",
    "        \n",
    "        # Vectorisation des ensembles Train, Validation et Test\n",
    "        X_train = tweet_vectorizer.fit_transform(tweets_train)\n",
    "        X_val = tweet_vectorizer.transform(tweets_val)\n",
    "        X_test = tweet_vectorizer.transform(tweets_test)\n",
    "        \n",
    "        # Initialiser le classificateur\n",
    "        classifier = TweetClassifier(vectorizer_type=config['vectorizer_type'], method=config['method'])\n",
    "        \n",
    "        # Entraîner et évaluer le modèle avec Train et Validation\n",
    "        classifier.train_and_evaluate(X_train, X_val, y_train, y_val)\n",
    "\n",
    "        # Évaluation finale sur le set de Test\n",
    "        classifier.final_evaluation(X_test, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
