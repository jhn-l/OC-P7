{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 16:35:32.059903: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732293332.074200  543575 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732293332.078647  543575 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-22 16:35:32.092105: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlflow.set_tracking_uri(\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "Transformers version: 4.46.2\n"
     ]
    }
   ],
   "source": [
    "# Affichage des versions pour vérification\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   target  20000 non-null  int64 \n",
      " 1   ids     20000 non-null  int64 \n",
      " 2   date    20000 non-null  object\n",
      " 3   flag    20000 non-null  object\n",
      " 4   user    20000 non-null  object\n",
      " 5   text    20000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 937.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Recharger le DataFrame depuis le fichier pickle\n",
    "df_sample = pd.read_pickle('download/df_sample_20000.pkl')\n",
    "df_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    10000\n",
       "1    10000\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " #Vérifier si le DataFrame a au moins 20 000 lignes\n",
    "if len(df_sample) != 20000:\n",
    "    raise ValueError(\"Le DataFrame ne contient pas 20 000 lignes.\")\n",
    "\n",
    "display(df_sample['target'].value_counts())\n",
    "df = df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification du Fine-Tuning\n",
    "\n",
    "#### Résultats Prometteurs avec un Modèle Préentraîné\n",
    "\n",
    "Lors des expérimentations initiales, le modèle préentraîné **`finiteautomata/bertweet-base-sentiment-analysis`** a montré des performances prometteuses sur des tâches similaires d'analyse de sentiments. En effet, ce modèle, conçu spécifiquement pour traiter des données provenant de Twitter, est particulièrement bien adapté pour capturer les nuances linguistiques, les abréviations, et le langage informel souvent présents dans les tweets.\n",
    "\n",
    "Ces performances encourageantes ont motivé l'idée de pousser davantage l'optimisation en effectuant un **fine-tuning** sur notre corpus spécifique de tweets, dans le but de mieux adapter le modèle aux particularités de nos données et de maximiser la performance sur la tâche ciblée.\n",
    "\n",
    "En somme, cette approche vise à exploiter le potentiel déjà démontré du modèle tout en le personnalisant davantage pour répondre aux besoins spécifiques de notre problématique métier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 16:35:44.214349: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "Some layers from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis were not used when initializing TFRobertaModel: ['classifier']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFRobertaModel were not initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis and are newly initialized: ['roberta/pooler/dense/bias:0', 'roberta/pooler/dense/kernel:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.11/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['input_ids', 'attention_mask']. Received: the structure of inputs={'input_ids': '*', 'token_type_ids': '*', 'attention_mask': '*'}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m841s\u001b[0m 833ms/step - accuracy: 0.4907 - loss: 0.7013 - val_accuracy: 0.5048 - val_loss: 0.6975\n",
      "Epoch 2/5\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m783s\u001b[0m 783ms/step - accuracy: 0.5028 - loss: 0.6982 - val_accuracy: 0.5048 - val_loss: 0.6932\n",
      "Epoch 3/5\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m778s\u001b[0m 778ms/step - accuracy: 0.5068 - loss: 0.6993 - val_accuracy: 0.5048 - val_loss: 0.7102\n",
      "Epoch 4/5\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m894s\u001b[0m 870ms/step - accuracy: 0.4992 - loss: 0.6972 - val_accuracy: 0.5048 - val_loss: 0.7068\n",
      "Epoch 5/5\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m833s\u001b[0m 833ms/step - accuracy: 0.4999 - loss: 0.6979 - val_accuracy: 0.5048 - val_loss: 0.6950\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 639ms/step\n",
      "Validation Metrics:\n",
      "Accuracy: 0.505\n",
      "ROC_AUC: 0.501\n",
      "F1: 0.0\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "Predict Time: 160.789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Registered model 'Fine-tuned finiteautomata/bertweet-base-sentiment-analysis' already exists. Creating a new version of this model...\n",
      "2024/11/22 17:47:15 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Fine-tuned finiteautomata/bertweet-base-sentiment-analysis, version 6\n",
      "Created version '6' of model 'Fine-tuned finiteautomata/bertweet-base-sentiment-analysis'.\n",
      "2024/11/22 17:47:15 INFO mlflow.tracking._tracking_service.client: 🏃 View run Fine-tuning finiteautomata/bertweet-base-sentiment-analysis at: http://mlflow-server:5000/#/experiments/6/runs/704f7d5b38a14981a3441ce7fbe30595.\n",
      "2024/11/22 17:47:15 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://mlflow-server:5000/#/experiments/6.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import emoji\n",
    "from transformers import AutoTokenizer, TFAutoModel, AutoConfig\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, f1_score, precision_score, recall_score\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import time\n",
    "\n",
    "# Prétraitement des tweets\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    tweet = emoji.demojize(tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'[^a-zA-Z0-9\\s]', '', tweet)\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    return tweet\n",
    "\n",
    "# Nettoyage des tweets\n",
    "df_sample['cleaned_text'] = df_sample['text'].apply(preprocess_tweet)\n",
    "\n",
    "# Diviser les données en train et test\n",
    "documents = df_sample['cleaned_text']\n",
    "labels = df_sample['target']\n",
    "X_train, X_val, y_train, y_val = train_test_split(documents, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Charger le tokenizer\n",
    "model_name = 'finiteautomata/bertweet-base-sentiment-analysis'\n",
    "#model_name = 'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenisation des données\n",
    "def tokenize_texts(texts, max_length=128):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_texts(X_train, max_length=80)\n",
    "val_encodings = tokenize_texts(X_val, max_length=80)\n",
    "\n",
    "# Préparer les datasets TensorFlow\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    y_train.values\n",
    ")).shuffle(len(X_train)).batch(16)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    y_val.values\n",
    ")).batch(16)\n",
    "\n",
    "# Charger la configuration et le modèle pré-entraîné sans la couche de classification\n",
    "config = AutoConfig.from_pretrained(model_name, output_hidden_states=False)\n",
    "base_model = TFAutoModel.from_pretrained(model_name, config=config)\n",
    "\n",
    "# Dégèle de la dernieère couche du modèle pour accélérer l'entraînement\n",
    "for layer in base_model.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "# Créer un nouveau modèle Keras\n",
    "input_ids = Input(shape=(80,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = Input(shape=(80,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# Utiliser Lambda pour appeler le modèle pré-entraîné avec output_shape spécifié\n",
    "sequence_output = Lambda(\n",
    "    lambda inputs: base_model(input_ids=inputs[0], attention_mask=inputs[1])[0],\n",
    "    output_shape=(80, 768)\n",
    ")([input_ids, attention_mask])\n",
    "\n",
    "cls_token = Lambda(lambda x: x[:, 0, :], output_shape=(768,))(sequence_output)  # Extraire le token [CLS]\n",
    "\n",
    "# Ajouter une couche dense pour la classification binaire\n",
    "output = Dense(1, activation=\"sigmoid\")(cls_token)\n",
    "\n",
    "# Construire le modèle final\n",
    "model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "# Optimiseur et perte\n",
    "learning_rate = 1e-5\n",
    "optimizer = Adam()\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "metric = tf.keras.metrics.BinaryAccuracy('accuracy')\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# Intégration avec MLflow\n",
    "#mlflow.tensorflow.autolog()\n",
    "# Configuration MLflow\n",
    "experiment = mlflow.set_experiment(\"BERT\")\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=f\"Fine-tuning {model_name}\",  nested=True) as run:\n",
    "    mlflow.set_tag(\"model_name\", model_name)\n",
    "    mlflow.log_param(\"epochs\", 5)\n",
    "    mlflow.log_param(\"batch_size\", 16)\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "\n",
    "    # Désactiver l'autolog de TensorFlow\n",
    "    #mlflow.tensorflow.autolog(disable=True)\n",
    "    all_validation_metrics = []\n",
    "    # Entraîner le modèle\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=5\n",
    "    )\n",
    "\n",
    "    # Prédictions sur le jeu de validation\n",
    "    start_time = time.time()\n",
    "    val_logits = model.predict(val_dataset)\n",
    "    predict_time = time.time() - start_time\n",
    "    \n",
    "    # Convertir les probabilités en classes binaires\n",
    "    y_val_prob = val_logits.flatten()  # Pour roc_auc_score, garder les probabilités\n",
    "    y_val_pred = (y_val_prob > 0.5).astype(int) \n",
    "\n",
    "    # Calcul des métriques\n",
    "    validation_metrics = {\n",
    "        \"Accuracy\": round(accuracy_score(y_val, y_val_pred), 3),\n",
    "        \"ROC_AUC\": round(roc_auc_score(y_val, y_val_prob), 3),\n",
    "        \"F1\": round(f1_score(y_val, y_val_pred), 3),\n",
    "        \"Precision\": round(precision_score(y_val, y_val_pred), 3),\n",
    "        \"Recall\": round(recall_score(y_val, y_val_pred), 3),\n",
    "        \"Predict Time\": round(predict_time, 3)\n",
    "    }\n",
    "\n",
    "    # Affichage des métriques\n",
    "    print(\"Validation Metrics:\")\n",
    "    for metric_name, metric_value in validation_metrics.items():\n",
    "        print(f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "    # Log des métriques dans MLflow\n",
    "    for metric_name, metric_value in validation_metrics.items():\n",
    "        mlflow.log_metric(metric_name, metric_value)\n",
    "\n",
    "    mlflow.log_table(validation_metrics, \"3-BERT-ft.json\")\n",
    "    # # Log des métriques finales dans MLflow\n",
    "    # mlflow.log_metric(\"Validation Accuracy\", accuracy)\n",
    "    # mlflow.log_dict(\n",
    "    #     classification_report(y_val, y_val_pred, output_dict=True),\n",
    "    #     \"classification_report.json\"\n",
    "    # )\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    result = mlflow.register_model(\n",
    "        model_uri=f\"runs:/{run_id}/model\",\n",
    "        name=f\"Fine-tuned {model_name}\"\n",
    "    )\n",
    "    \n",
    "    # Construire le lien MLflow correspondant (en supposant que vous avez l'URL de votre serveur MLflow)\n",
    "    active_run = mlflow.active_run()\n",
    "    run_id = active_run.info.run_id\n",
    "    #run_name = active_run.data.tags.get(\"mlflow.runName\")  # Obtenir le nom du run depuis les tags\n",
    "\n",
    "    # Récupérer l'ID de l'expérience active\n",
    "    experiment_id = active_run.info.experiment_id\n",
    "\n",
    "    # Définir l'URL de votre serveur MLflow\n",
    "    mlflow_server_url = \"http://localhost:5000\"  # Remplacez par l'URL réel de votre serveur MLflow\n",
    "\n",
    "    # Construire le lien complet vers le run dans l'interface MLflow\n",
    "    run_link = f\"{mlflow_server_url}/#/experiments/{experiment_id}/runs/{run_id}\"\n",
    "    \n",
    "        # Exemple d'ajout d'un dictionnaire pour chaque configuration\n",
    "    # Ajouter les métriques de validation à la liste\n",
    "    all_validation_metrics.append({\n",
    "        \"run_name\": f\"{model_name} - Fine-tuned\",\n",
    "        **validation_metrics,\n",
    "        \"run_id\": run_link\n",
    "    })\n",
    "    mlflow.log_table(pd.DataFrame(all_validation_metrics), \"3-ft-finiteautomata.json\")\n",
    "    \n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion sur le Fine-Tuning\n",
    "\n",
    "Après avoir tenté de fine-tuner le modèle **`finiteautomata/bertweet-base-sentiment-analysis`**, les résultats obtenus n'ont pas permis d'améliorer les performances initiales du modèle pré-entraîné. En effet, les métriques telles que l'**accuracy**, le **F1-score**, la **précision**, et le **rappel** se sont significativement dégradées après le fine-tuning.\n",
    "\n",
    "Ces résultats suggèrent que le modèle pré-entraîné est déjà très bien ajusté pour la tâche générale d'analyse de sentiments sur des tweets, et qu'il capture efficacement les caractéristiques linguistiques propres à ce type de données. Toutefois, le fine-tuning n'a pas permis d'exploiter pleinement les spécificités de notre corpus.\n",
    "\n",
    "#### Perspectives d'Amélioration\n",
    "\n",
    "Pour aller au-delà des performances actuelles, une étude plus approfondie serait nécessaire, incluant :\n",
    "- Une analyse détaillée des **données d'entraînement**, notamment leur qualité et leur distribution.\n",
    "- Une exploration systématique des **hyperparamètres** (learning rate, batch size, nombre de couches dégélées, etc.).\n",
    "- L'intégration de techniques avancées comme l'**augmentation de données** pour enrichir le corpus.\n",
    "- L'utilisation d'approches comme l'**apprentissage multitâche** ou le **transfer learning sur des tâches adjacentes** pour tirer parti des données disponibles.\n",
    "\n",
    "En résumé, bien que le modèle pré-entraîné reste performant et adapté à notre cas d'usage, il serait pertinent d'approfondir les analyses et les ajustements pour maximiser son potentiel dans un contexte spécifique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    10000\n",
       "1    10000\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_sample['target'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement modèle BERT\n",
    "### Expérimentation avec `bert-base-uncased`\n",
    "\n",
    "Dans le cadre de ce projet, j'ai choisi d'entraîner le modèle **`bert-base-uncased`** sur mon propre jeu de données. Cette expérimentation visait à évaluer si un modèle généraliste pré-entraîné sur un large corpus de textes en anglais pouvait mieux performer que des modèles spécialisés comme **`finiteautomata/bertweet-base-sentiment-analysis`**, qui est spécifiquement conçu pour des données issues de Twitter.\n",
    "\n",
    "L'objectif était de partir d'un modèle robuste et polyvalent, mais sans préjugés spécifiques liés au domaine ou au type de données (comme les tweets), pour voir s'il pourrait mieux s'adapter à mon corpus grâce à un entraînement complet. Cette démarche a permis de comparer les résultats entre un fine-tuning sur un modèle spécialisé et un entraînement complet sur un modèle généraliste, tout en testant leur capacité respective à capturer les nuances propres à mon jeu de données.\n",
    "\n",
    "Les résultats obtenus avec **`bert-base-uncased`** serviront également de point de référence pour juger de l'impact du choix du modèle initial et de la qualité de l'entraînement réalisé.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Charger le tokenizer et le modèle\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de données pour 'documents' et 'labels'\n",
    "documents = df_sample['text']  # Liste de textes à analyser\n",
    "labels = df_sample['target']   # Liste de labels (0 ou 1 pour la classification binaire)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization des données\n",
    "def tokenize_data(documents):\n",
    "    return tokenizer(\n",
    "        documents.tolist(),\n",
    "        max_length=128, padding=True, truncation=True, return_tensors='tf'\n",
    "    )\n",
    "tokens = tokenize_data(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir en NumPy pour train_test_split\n",
    "input_ids_np = tokens['input_ids'].numpy()\n",
    "attention_masks_np = tokens['attention_mask'].numpy()\n",
    "labels_np = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparer les ensembles de données\n",
    "train_input_ids, val_input_ids, train_labels, val_labels = train_test_split(input_ids_np, labels_np, test_size=0.2, random_state=42)\n",
    "train_attention_masks, val_attention_masks = train_test_split(attention_masks_np, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurer la fonction de perte et l'optimiseur\n",
    "loss_fn = SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = Adam(learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir le nombre d'époques et la taille du batch\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "\n",
    "# Définir l'accuracy\n",
    "train_accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "Batch 0 - Loss: 0.6211 - Accuracy: 0.7500\n",
      "Batch 10 - Loss: 0.6312 - Accuracy: 0.5682\n",
      "Batch 20 - Loss: 0.6607 - Accuracy: 0.5952\n",
      "Batch 30 - Loss: 0.6653 - Accuracy: 0.6008\n",
      "Batch 40 - Loss: 0.5985 - Accuracy: 0.6113\n",
      "Batch 50 - Loss: 0.7243 - Accuracy: 0.6140\n",
      "Batch 60 - Loss: 0.5404 - Accuracy: 0.6168\n",
      "Batch 70 - Loss: 0.5786 - Accuracy: 0.6276\n",
      "Batch 80 - Loss: 0.5692 - Accuracy: 0.6427\n",
      "Batch 90 - Loss: 0.5841 - Accuracy: 0.6449\n",
      "Batch 100 - Loss: 0.5579 - Accuracy: 0.6498\n",
      "Batch 110 - Loss: 0.6012 - Accuracy: 0.6548\n",
      "Batch 120 - Loss: 0.6841 - Accuracy: 0.6643\n",
      "Batch 130 - Loss: 0.5828 - Accuracy: 0.6751\n",
      "Batch 140 - Loss: 0.3950 - Accuracy: 0.6826\n",
      "Batch 150 - Loss: 0.6086 - Accuracy: 0.6858\n",
      "Batch 160 - Loss: 0.3317 - Accuracy: 0.6949\n",
      "Batch 170 - Loss: 0.4097 - Accuracy: 0.7036\n",
      "Batch 180 - Loss: 0.4473 - Accuracy: 0.7082\n",
      "Batch 190 - Loss: 0.6451 - Accuracy: 0.7114\n",
      "Batch 200 - Loss: 0.5220 - Accuracy: 0.7170\n",
      "Batch 210 - Loss: 0.4565 - Accuracy: 0.7207\n",
      "Batch 220 - Loss: 0.2578 - Accuracy: 0.7223\n",
      "Batch 230 - Loss: 0.4558 - Accuracy: 0.7267\n",
      "Batch 240 - Loss: 0.6790 - Accuracy: 0.7280\n",
      "Batch 250 - Loss: 0.5306 - Accuracy: 0.7308\n",
      "Batch 260 - Loss: 0.4908 - Accuracy: 0.7316\n",
      "Batch 270 - Loss: 0.4003 - Accuracy: 0.7327\n",
      "Batch 280 - Loss: 0.3969 - Accuracy: 0.7353\n",
      "Batch 290 - Loss: 0.2646 - Accuracy: 0.7399\n",
      "Batch 300 - Loss: 0.5494 - Accuracy: 0.7411\n",
      "Batch 310 - Loss: 0.3747 - Accuracy: 0.7438\n",
      "Batch 320 - Loss: 0.4696 - Accuracy: 0.7457\n",
      "Batch 330 - Loss: 0.5908 - Accuracy: 0.7474\n",
      "Batch 340 - Loss: 0.3948 - Accuracy: 0.7489\n",
      "Batch 350 - Loss: 0.3132 - Accuracy: 0.7511\n",
      "Batch 360 - Loss: 0.3950 - Accuracy: 0.7536\n",
      "Batch 370 - Loss: 0.4074 - Accuracy: 0.7542\n",
      "Batch 380 - Loss: 0.5388 - Accuracy: 0.7539\n",
      "Batch 390 - Loss: 0.1801 - Accuracy: 0.7562\n",
      "Batch 400 - Loss: 0.4253 - Accuracy: 0.7573\n",
      "Batch 410 - Loss: 0.4744 - Accuracy: 0.7581\n",
      "Batch 420 - Loss: 0.5592 - Accuracy: 0.7580\n",
      "Batch 430 - Loss: 0.4693 - Accuracy: 0.7580\n",
      "Batch 440 - Loss: 0.3796 - Accuracy: 0.7589\n",
      "Batch 450 - Loss: 0.4387 - Accuracy: 0.7580\n",
      "Batch 460 - Loss: 0.3422 - Accuracy: 0.7587\n",
      "Batch 470 - Loss: 0.5395 - Accuracy: 0.7588\n",
      "Batch 480 - Loss: 0.4830 - Accuracy: 0.7583\n",
      "Batch 490 - Loss: 0.3336 - Accuracy: 0.7590\n",
      "Batch 500 - Loss: 0.4606 - Accuracy: 0.7606\n",
      "Batch 510 - Loss: 0.6817 - Accuracy: 0.7614\n",
      "Batch 520 - Loss: 0.3601 - Accuracy: 0.7621\n",
      "Batch 530 - Loss: 0.6716 - Accuracy: 0.7615\n",
      "Batch 540 - Loss: 0.5961 - Accuracy: 0.7618\n",
      "Batch 550 - Loss: 0.4559 - Accuracy: 0.7621\n",
      "Batch 560 - Loss: 0.5435 - Accuracy: 0.7627\n",
      "Batch 570 - Loss: 0.3833 - Accuracy: 0.7632\n",
      "Batch 580 - Loss: 0.3997 - Accuracy: 0.7629\n",
      "Batch 590 - Loss: 0.3363 - Accuracy: 0.7637\n",
      "Batch 600 - Loss: 0.3018 - Accuracy: 0.7650\n",
      "Batch 610 - Loss: 0.2754 - Accuracy: 0.7658\n",
      "Batch 620 - Loss: 0.2202 - Accuracy: 0.7671\n",
      "Batch 630 - Loss: 0.1909 - Accuracy: 0.7676\n",
      "Batch 640 - Loss: 0.5474 - Accuracy: 0.7671\n",
      "Batch 650 - Loss: 0.5081 - Accuracy: 0.7680\n",
      "Batch 660 - Loss: 0.5078 - Accuracy: 0.7677\n",
      "Batch 670 - Loss: 0.2394 - Accuracy: 0.7689\n",
      "Batch 680 - Loss: 0.4352 - Accuracy: 0.7692\n",
      "Batch 690 - Loss: 0.4086 - Accuracy: 0.7696\n",
      "Batch 700 - Loss: 0.3861 - Accuracy: 0.7701\n",
      "Batch 710 - Loss: 0.4005 - Accuracy: 0.7713\n",
      "Batch 720 - Loss: 0.4830 - Accuracy: 0.7723\n",
      "Batch 730 - Loss: 0.3468 - Accuracy: 0.7733\n",
      "Batch 740 - Loss: 0.5377 - Accuracy: 0.7740\n",
      "Batch 750 - Loss: 0.3019 - Accuracy: 0.7740\n",
      "Batch 760 - Loss: 0.4014 - Accuracy: 0.7739\n",
      "Batch 770 - Loss: 0.4679 - Accuracy: 0.7750\n",
      "Batch 780 - Loss: 0.2887 - Accuracy: 0.7755\n",
      "Batch 790 - Loss: 0.4490 - Accuracy: 0.7758\n",
      "Batch 800 - Loss: 0.5663 - Accuracy: 0.7758\n",
      "Batch 810 - Loss: 0.3139 - Accuracy: 0.7770\n",
      "Batch 820 - Loss: 0.5074 - Accuracy: 0.7768\n",
      "Batch 830 - Loss: 0.6005 - Accuracy: 0.7768\n",
      "Batch 840 - Loss: 0.3467 - Accuracy: 0.7771\n",
      "Batch 850 - Loss: 0.3193 - Accuracy: 0.7779\n",
      "Batch 860 - Loss: 0.3088 - Accuracy: 0.7777\n",
      "Batch 870 - Loss: 0.4501 - Accuracy: 0.7781\n",
      "Batch 880 - Loss: 0.3583 - Accuracy: 0.7784\n",
      "Batch 890 - Loss: 0.2459 - Accuracy: 0.7783\n",
      "Batch 900 - Loss: 0.4801 - Accuracy: 0.7782\n",
      "Batch 910 - Loss: 0.3759 - Accuracy: 0.7789\n",
      "Batch 920 - Loss: 0.9384 - Accuracy: 0.7788\n",
      "Batch 930 - Loss: 0.2862 - Accuracy: 0.7789\n",
      "Batch 940 - Loss: 0.3219 - Accuracy: 0.7794\n",
      "Batch 950 - Loss: 0.6951 - Accuracy: 0.7798\n",
      "Batch 960 - Loss: 0.6835 - Accuracy: 0.7801\n",
      "Batch 970 - Loss: 0.5178 - Accuracy: 0.7803\n",
      "Batch 980 - Loss: 0.2332 - Accuracy: 0.7805\n",
      "Batch 990 - Loss: 0.5957 - Accuracy: 0.7805\n",
      "Epoch 1 - Loss: 0.4631 - Accuracy: 0.7809\n",
      "\n",
      "Epoch 2/5\n",
      "Batch 0 - Loss: 0.3278 - Accuracy: 0.8125\n",
      "Batch 10 - Loss: 0.4510 - Accuracy: 0.8466\n",
      "Batch 20 - Loss: 0.3355 - Accuracy: 0.8363\n",
      "Batch 30 - Loss: 0.5596 - Accuracy: 0.8327\n",
      "Batch 40 - Loss: 0.4946 - Accuracy: 0.8155\n",
      "Batch 50 - Loss: 0.2870 - Accuracy: 0.8248\n",
      "Batch 60 - Loss: 0.4940 - Accuracy: 0.8207\n",
      "Batch 70 - Loss: 0.5101 - Accuracy: 0.8213\n",
      "Batch 80 - Loss: 0.2435 - Accuracy: 0.8241\n",
      "Batch 90 - Loss: 0.4364 - Accuracy: 0.8242\n",
      "Batch 100 - Loss: 0.3450 - Accuracy: 0.8292\n",
      "Batch 110 - Loss: 0.5083 - Accuracy: 0.8283\n",
      "Batch 120 - Loss: 0.3597 - Accuracy: 0.8326\n",
      "Batch 130 - Loss: 0.5219 - Accuracy: 0.8340\n",
      "Batch 140 - Loss: 0.3524 - Accuracy: 0.8342\n",
      "Batch 150 - Loss: 0.3653 - Accuracy: 0.8320\n",
      "Batch 160 - Loss: 0.2179 - Accuracy: 0.8373\n",
      "Batch 170 - Loss: 0.2240 - Accuracy: 0.8406\n",
      "Batch 180 - Loss: 0.2118 - Accuracy: 0.8436\n",
      "Batch 190 - Loss: 0.5342 - Accuracy: 0.8452\n",
      "Batch 200 - Loss: 0.2897 - Accuracy: 0.8476\n",
      "Batch 210 - Loss: 0.3355 - Accuracy: 0.8463\n",
      "Batch 220 - Loss: 0.1956 - Accuracy: 0.8467\n",
      "Batch 230 - Loss: 0.4840 - Accuracy: 0.8469\n",
      "Batch 240 - Loss: 0.2427 - Accuracy: 0.8467\n",
      "Batch 250 - Loss: 0.1714 - Accuracy: 0.8489\n",
      "Batch 260 - Loss: 0.3323 - Accuracy: 0.8479\n",
      "Batch 270 - Loss: 0.3422 - Accuracy: 0.8464\n",
      "Batch 280 - Loss: 0.2545 - Accuracy: 0.8494\n",
      "Batch 290 - Loss: 0.1569 - Accuracy: 0.8507\n",
      "Batch 300 - Loss: 0.3916 - Accuracy: 0.8495\n",
      "Batch 310 - Loss: 0.2027 - Accuracy: 0.8513\n",
      "Batch 320 - Loss: 0.2593 - Accuracy: 0.8512\n",
      "Batch 330 - Loss: 0.2604 - Accuracy: 0.8525\n",
      "Batch 340 - Loss: 0.2455 - Accuracy: 0.8528\n",
      "Batch 350 - Loss: 0.1424 - Accuracy: 0.8538\n",
      "Batch 360 - Loss: 0.2916 - Accuracy: 0.8547\n",
      "Batch 370 - Loss: 0.2974 - Accuracy: 0.8553\n",
      "Batch 380 - Loss: 0.3411 - Accuracy: 0.8545\n",
      "Batch 390 - Loss: 0.1568 - Accuracy: 0.8558\n",
      "Batch 400 - Loss: 0.2926 - Accuracy: 0.8566\n",
      "Batch 410 - Loss: 0.2984 - Accuracy: 0.8566\n",
      "Batch 420 - Loss: 0.5149 - Accuracy: 0.8572\n",
      "Batch 430 - Loss: 0.3775 - Accuracy: 0.8573\n",
      "Batch 440 - Loss: 0.1313 - Accuracy: 0.8574\n",
      "Batch 450 - Loss: 0.2533 - Accuracy: 0.8567\n",
      "Batch 460 - Loss: 0.2205 - Accuracy: 0.8571\n",
      "Batch 470 - Loss: 0.4342 - Accuracy: 0.8571\n",
      "Batch 480 - Loss: 0.2751 - Accuracy: 0.8564\n",
      "Batch 490 - Loss: 0.2227 - Accuracy: 0.8565\n",
      "Batch 500 - Loss: 0.3250 - Accuracy: 0.8568\n",
      "Batch 510 - Loss: 0.5299 - Accuracy: 0.8568\n",
      "Batch 520 - Loss: 0.1999 - Accuracy: 0.8568\n",
      "Batch 530 - Loss: 0.3109 - Accuracy: 0.8559\n",
      "Batch 540 - Loss: 0.4321 - Accuracy: 0.8566\n",
      "Batch 550 - Loss: 0.3846 - Accuracy: 0.8567\n",
      "Batch 560 - Loss: 0.2901 - Accuracy: 0.8571\n",
      "Batch 570 - Loss: 0.2981 - Accuracy: 0.8573\n",
      "Batch 580 - Loss: 0.2749 - Accuracy: 0.8567\n",
      "Batch 590 - Loss: 0.1694 - Accuracy: 0.8572\n",
      "Batch 600 - Loss: 0.1918 - Accuracy: 0.8576\n",
      "Batch 610 - Loss: 0.1903 - Accuracy: 0.8580\n",
      "Batch 620 - Loss: 0.0867 - Accuracy: 0.8588\n",
      "Batch 630 - Loss: 0.1769 - Accuracy: 0.8586\n",
      "Batch 640 - Loss: 0.3289 - Accuracy: 0.8583\n",
      "Batch 650 - Loss: 0.3523 - Accuracy: 0.8586\n",
      "Batch 660 - Loss: 0.3583 - Accuracy: 0.8581\n",
      "Batch 670 - Loss: 0.1661 - Accuracy: 0.8584\n",
      "Batch 680 - Loss: 0.2787 - Accuracy: 0.8585\n",
      "Batch 690 - Loss: 0.3480 - Accuracy: 0.8586\n",
      "Batch 700 - Loss: 0.1864 - Accuracy: 0.8590\n",
      "Batch 710 - Loss: 0.3335 - Accuracy: 0.8592\n",
      "Batch 720 - Loss: 0.3030 - Accuracy: 0.8599\n",
      "Batch 730 - Loss: 0.3109 - Accuracy: 0.8602\n",
      "Batch 740 - Loss: 0.6148 - Accuracy: 0.8607\n",
      "Batch 750 - Loss: 0.1196 - Accuracy: 0.8611\n",
      "Batch 760 - Loss: 0.2850 - Accuracy: 0.8613\n",
      "Batch 770 - Loss: 0.2353 - Accuracy: 0.8621\n",
      "Batch 780 - Loss: 0.2455 - Accuracy: 0.8623\n",
      "Batch 790 - Loss: 0.3680 - Accuracy: 0.8624\n",
      "Batch 800 - Loss: 0.4548 - Accuracy: 0.8620\n",
      "Batch 810 - Loss: 0.2697 - Accuracy: 0.8625\n",
      "Batch 820 - Loss: 0.3578 - Accuracy: 0.8621\n",
      "Batch 830 - Loss: 0.5823 - Accuracy: 0.8619\n",
      "Batch 840 - Loss: 0.2112 - Accuracy: 0.8618\n",
      "Batch 850 - Loss: 0.2075 - Accuracy: 0.8620\n",
      "Batch 860 - Loss: 0.2671 - Accuracy: 0.8616\n",
      "Batch 870 - Loss: 0.3987 - Accuracy: 0.8620\n",
      "Batch 880 - Loss: 0.3152 - Accuracy: 0.8622\n",
      "Batch 890 - Loss: 0.1133 - Accuracy: 0.8620\n",
      "Batch 900 - Loss: 0.4848 - Accuracy: 0.8618\n",
      "Batch 910 - Loss: 0.2560 - Accuracy: 0.8624\n",
      "Batch 920 - Loss: 0.7685 - Accuracy: 0.8623\n",
      "Batch 930 - Loss: 0.1948 - Accuracy: 0.8619\n",
      "Batch 940 - Loss: 0.2252 - Accuracy: 0.8624\n",
      "Batch 950 - Loss: 0.6326 - Accuracy: 0.8628\n",
      "Batch 960 - Loss: 0.4421 - Accuracy: 0.8628\n",
      "Batch 970 - Loss: 0.4038 - Accuracy: 0.8627\n",
      "Batch 980 - Loss: 0.2879 - Accuracy: 0.8629\n",
      "Batch 990 - Loss: 0.3073 - Accuracy: 0.8630\n",
      "Epoch 2 - Loss: 0.3277 - Accuracy: 0.8633\n",
      "\n",
      "Epoch 3/5\n",
      "Batch 0 - Loss: 0.2477 - Accuracy: 0.8750\n",
      "Batch 10 - Loss: 0.2493 - Accuracy: 0.8750\n",
      "Batch 20 - Loss: 0.1263 - Accuracy: 0.8958\n",
      "Batch 30 - Loss: 0.3373 - Accuracy: 0.8931\n",
      "Batch 40 - Loss: 0.4321 - Accuracy: 0.8765\n",
      "Batch 50 - Loss: 0.1668 - Accuracy: 0.8860\n",
      "Batch 60 - Loss: 0.2587 - Accuracy: 0.8822\n",
      "Batch 70 - Loss: 0.3688 - Accuracy: 0.8847\n",
      "Batch 80 - Loss: 0.0948 - Accuracy: 0.8843\n",
      "Batch 90 - Loss: 0.3429 - Accuracy: 0.8819\n",
      "Batch 100 - Loss: 0.2396 - Accuracy: 0.8806\n",
      "Batch 110 - Loss: 0.3010 - Accuracy: 0.8818\n",
      "Batch 120 - Loss: 0.1422 - Accuracy: 0.8822\n",
      "Batch 130 - Loss: 0.3732 - Accuracy: 0.8841\n",
      "Batch 140 - Loss: 0.1318 - Accuracy: 0.8870\n",
      "Batch 150 - Loss: 0.2920 - Accuracy: 0.8878\n",
      "Batch 160 - Loss: 0.1725 - Accuracy: 0.8909\n",
      "Batch 170 - Loss: 0.1193 - Accuracy: 0.8933\n",
      "Batch 180 - Loss: 0.1963 - Accuracy: 0.8954\n",
      "Batch 190 - Loss: 0.4240 - Accuracy: 0.8979\n",
      "Batch 200 - Loss: 0.2208 - Accuracy: 0.9002\n",
      "Batch 210 - Loss: 0.2070 - Accuracy: 0.8981\n",
      "Batch 220 - Loss: 0.1979 - Accuracy: 0.8982\n",
      "Batch 230 - Loss: 0.4223 - Accuracy: 0.8994\n",
      "Batch 240 - Loss: 0.0697 - Accuracy: 0.9002\n",
      "Batch 250 - Loss: 0.1259 - Accuracy: 0.9011\n",
      "Batch 260 - Loss: 0.2685 - Accuracy: 0.9011\n",
      "Batch 270 - Loss: 0.1187 - Accuracy: 0.8999\n",
      "Batch 280 - Loss: 0.1566 - Accuracy: 0.9010\n",
      "Batch 290 - Loss: 0.0968 - Accuracy: 0.9018\n",
      "Batch 300 - Loss: 0.2035 - Accuracy: 0.9022\n",
      "Batch 310 - Loss: 0.0734 - Accuracy: 0.9025\n",
      "Batch 320 - Loss: 0.3235 - Accuracy: 0.9023\n",
      "Batch 330 - Loss: 0.1015 - Accuracy: 0.9029\n",
      "Batch 340 - Loss: 0.1672 - Accuracy: 0.9036\n",
      "Batch 350 - Loss: 0.0565 - Accuracy: 0.9042\n",
      "Batch 360 - Loss: 0.2081 - Accuracy: 0.9050\n",
      "Batch 370 - Loss: 0.1269 - Accuracy: 0.9060\n",
      "Batch 380 - Loss: 0.3610 - Accuracy: 0.9055\n",
      "Batch 390 - Loss: 0.0702 - Accuracy: 0.9065\n",
      "Batch 400 - Loss: 0.1940 - Accuracy: 0.9066\n",
      "Batch 410 - Loss: 0.1613 - Accuracy: 0.9071\n",
      "Batch 420 - Loss: 0.4344 - Accuracy: 0.9066\n",
      "Batch 430 - Loss: 0.3257 - Accuracy: 0.9068\n",
      "Batch 440 - Loss: 0.1005 - Accuracy: 0.9067\n",
      "Batch 450 - Loss: 0.1189 - Accuracy: 0.9073\n",
      "Batch 460 - Loss: 0.0575 - Accuracy: 0.9071\n",
      "Batch 470 - Loss: 0.2266 - Accuracy: 0.9075\n",
      "Batch 480 - Loss: 0.1611 - Accuracy: 0.9080\n",
      "Batch 490 - Loss: 0.1013 - Accuracy: 0.9084\n",
      "Batch 500 - Loss: 0.2899 - Accuracy: 0.9084\n",
      "Batch 510 - Loss: 0.3961 - Accuracy: 0.9083\n",
      "Batch 520 - Loss: 0.1282 - Accuracy: 0.9080\n",
      "Batch 530 - Loss: 0.2806 - Accuracy: 0.9067\n",
      "Batch 540 - Loss: 0.3272 - Accuracy: 0.9064\n",
      "Batch 550 - Loss: 0.2741 - Accuracy: 0.9064\n",
      "Batch 560 - Loss: 0.1573 - Accuracy: 0.9062\n",
      "Batch 570 - Loss: 0.2745 - Accuracy: 0.9070\n",
      "Batch 580 - Loss: 0.0798 - Accuracy: 0.9066\n",
      "Batch 590 - Loss: 0.1166 - Accuracy: 0.9074\n",
      "Batch 600 - Loss: 0.2186 - Accuracy: 0.9073\n",
      "Batch 610 - Loss: 0.1831 - Accuracy: 0.9078\n",
      "Batch 620 - Loss: 0.0778 - Accuracy: 0.9086\n",
      "Batch 630 - Loss: 0.1357 - Accuracy: 0.9086\n",
      "Batch 640 - Loss: 0.3358 - Accuracy: 0.9083\n",
      "Batch 650 - Loss: 0.1789 - Accuracy: 0.9084\n",
      "Batch 660 - Loss: 0.1828 - Accuracy: 0.9081\n",
      "Batch 670 - Loss: 0.0556 - Accuracy: 0.9090\n",
      "Batch 680 - Loss: 0.3115 - Accuracy: 0.9094\n",
      "Batch 690 - Loss: 0.0770 - Accuracy: 0.9101\n",
      "Batch 700 - Loss: 0.0707 - Accuracy: 0.9102\n",
      "Batch 710 - Loss: 0.1631 - Accuracy: 0.9106\n",
      "Batch 720 - Loss: 0.2616 - Accuracy: 0.9113\n",
      "Batch 730 - Loss: 0.0680 - Accuracy: 0.9121\n",
      "Batch 740 - Loss: 0.0513 - Accuracy: 0.9124\n",
      "Batch 750 - Loss: 0.0774 - Accuracy: 0.9126\n",
      "Batch 760 - Loss: 0.2232 - Accuracy: 0.9128\n",
      "Batch 770 - Loss: 0.0458 - Accuracy: 0.9133\n",
      "Batch 780 - Loss: 0.2780 - Accuracy: 0.9139\n",
      "Batch 790 - Loss: 0.3254 - Accuracy: 0.9140\n",
      "Batch 800 - Loss: 0.3071 - Accuracy: 0.9138\n",
      "Batch 810 - Loss: 0.1265 - Accuracy: 0.9141\n",
      "Batch 820 - Loss: 0.0692 - Accuracy: 0.9138\n",
      "Batch 830 - Loss: 0.4716 - Accuracy: 0.9141\n",
      "Batch 840 - Loss: 0.1374 - Accuracy: 0.9142\n",
      "Batch 850 - Loss: 0.1413 - Accuracy: 0.9141\n",
      "Batch 860 - Loss: 0.1088 - Accuracy: 0.9140\n",
      "Batch 870 - Loss: 0.1058 - Accuracy: 0.9143\n",
      "Batch 880 - Loss: 0.3262 - Accuracy: 0.9143\n",
      "Batch 890 - Loss: 0.1358 - Accuracy: 0.9144\n",
      "Batch 900 - Loss: 0.5489 - Accuracy: 0.9138\n",
      "Batch 910 - Loss: 0.1848 - Accuracy: 0.9140\n",
      "Batch 920 - Loss: 0.6595 - Accuracy: 0.9139\n",
      "Batch 930 - Loss: 0.0994 - Accuracy: 0.9137\n",
      "Batch 940 - Loss: 0.0831 - Accuracy: 0.9140\n",
      "Batch 950 - Loss: 0.3205 - Accuracy: 0.9143\n",
      "Batch 960 - Loss: 0.3253 - Accuracy: 0.9145\n",
      "Batch 970 - Loss: 0.3356 - Accuracy: 0.9147\n",
      "Batch 980 - Loss: 0.1173 - Accuracy: 0.9148\n",
      "Batch 990 - Loss: 0.2039 - Accuracy: 0.9149\n",
      "Epoch 3 - Loss: 0.2153 - Accuracy: 0.9149\n",
      "\n",
      "Epoch 4/5\n",
      "Batch 0 - Loss: 0.1371 - Accuracy: 1.0000\n",
      "Batch 10 - Loss: 0.1125 - Accuracy: 0.9318\n",
      "Batch 20 - Loss: 0.1068 - Accuracy: 0.9464\n",
      "Batch 30 - Loss: 0.0907 - Accuracy: 0.9415\n",
      "Batch 40 - Loss: 0.1935 - Accuracy: 0.9466\n",
      "Batch 50 - Loss: 0.0237 - Accuracy: 0.9522\n",
      "Batch 60 - Loss: 0.0786 - Accuracy: 0.9508\n",
      "Batch 70 - Loss: 0.2874 - Accuracy: 0.9463\n",
      "Batch 80 - Loss: 0.1216 - Accuracy: 0.9444\n",
      "Batch 90 - Loss: 0.0650 - Accuracy: 0.9396\n",
      "Batch 100 - Loss: 0.2007 - Accuracy: 0.9381\n",
      "Batch 110 - Loss: 0.2019 - Accuracy: 0.9364\n",
      "Batch 120 - Loss: 0.0779 - Accuracy: 0.9396\n",
      "Batch 130 - Loss: 0.0989 - Accuracy: 0.9408\n",
      "Batch 140 - Loss: 0.1193 - Accuracy: 0.9406\n",
      "Batch 150 - Loss: 0.1064 - Accuracy: 0.9425\n",
      "Batch 160 - Loss: 0.0096 - Accuracy: 0.9441\n",
      "Batch 170 - Loss: 0.0361 - Accuracy: 0.9452\n",
      "Batch 180 - Loss: 0.3412 - Accuracy: 0.9468\n",
      "Batch 190 - Loss: 0.1269 - Accuracy: 0.9473\n",
      "Batch 200 - Loss: 0.1107 - Accuracy: 0.9478\n",
      "Batch 210 - Loss: 0.0373 - Accuracy: 0.9488\n",
      "Batch 220 - Loss: 0.0887 - Accuracy: 0.9497\n",
      "Batch 230 - Loss: 0.4007 - Accuracy: 0.9491\n",
      "Batch 240 - Loss: 0.0695 - Accuracy: 0.9494\n",
      "Batch 250 - Loss: 0.0583 - Accuracy: 0.9497\n",
      "Batch 260 - Loss: 0.0268 - Accuracy: 0.9502\n",
      "Batch 270 - Loss: 0.1677 - Accuracy: 0.9490\n",
      "Batch 280 - Loss: 0.0306 - Accuracy: 0.9504\n",
      "Batch 290 - Loss: 0.0221 - Accuracy: 0.9508\n",
      "Batch 300 - Loss: 0.1576 - Accuracy: 0.9500\n",
      "Batch 310 - Loss: 0.0821 - Accuracy: 0.9510\n",
      "Batch 320 - Loss: 0.0627 - Accuracy: 0.9511\n",
      "Batch 330 - Loss: 0.0149 - Accuracy: 0.9503\n",
      "Batch 340 - Loss: 0.2268 - Accuracy: 0.9500\n",
      "Batch 350 - Loss: 0.0392 - Accuracy: 0.9505\n",
      "Batch 360 - Loss: 0.0194 - Accuracy: 0.9515\n",
      "Batch 370 - Loss: 0.0905 - Accuracy: 0.9522\n",
      "Batch 380 - Loss: 0.0525 - Accuracy: 0.9529\n",
      "Batch 390 - Loss: 0.0347 - Accuracy: 0.9535\n",
      "Batch 400 - Loss: 0.0279 - Accuracy: 0.9536\n",
      "Batch 410 - Loss: 0.0349 - Accuracy: 0.9532\n",
      "Batch 420 - Loss: 0.2049 - Accuracy: 0.9532\n",
      "Batch 430 - Loss: 0.0397 - Accuracy: 0.9533\n",
      "Batch 440 - Loss: 0.1604 - Accuracy: 0.9534\n",
      "Batch 450 - Loss: 0.0297 - Accuracy: 0.9530\n",
      "Batch 460 - Loss: 0.0118 - Accuracy: 0.9536\n",
      "Batch 470 - Loss: 0.0374 - Accuracy: 0.9541\n",
      "Batch 480 - Loss: 0.0666 - Accuracy: 0.9539\n",
      "Batch 490 - Loss: 0.0872 - Accuracy: 0.9540\n",
      "Batch 500 - Loss: 0.0236 - Accuracy: 0.9541\n",
      "Batch 510 - Loss: 0.2671 - Accuracy: 0.9536\n",
      "Batch 520 - Loss: 0.2283 - Accuracy: 0.9537\n",
      "Batch 530 - Loss: 0.1500 - Accuracy: 0.9529\n",
      "Batch 540 - Loss: 0.2885 - Accuracy: 0.9530\n",
      "Batch 550 - Loss: 0.2692 - Accuracy: 0.9530\n",
      "Batch 560 - Loss: 0.1674 - Accuracy: 0.9529\n",
      "Batch 570 - Loss: 0.5698 - Accuracy: 0.9529\n",
      "Batch 580 - Loss: 0.1362 - Accuracy: 0.9531\n",
      "Batch 590 - Loss: 0.0508 - Accuracy: 0.9530\n",
      "Batch 600 - Loss: 0.0238 - Accuracy: 0.9529\n",
      "Batch 610 - Loss: 0.0433 - Accuracy: 0.9529\n",
      "Batch 620 - Loss: 0.0577 - Accuracy: 0.9529\n",
      "Batch 630 - Loss: 0.2620 - Accuracy: 0.9528\n",
      "Batch 640 - Loss: 0.0689 - Accuracy: 0.9523\n",
      "Batch 650 - Loss: 0.1594 - Accuracy: 0.9524\n",
      "Batch 660 - Loss: 0.1574 - Accuracy: 0.9526\n",
      "Batch 670 - Loss: 0.0502 - Accuracy: 0.9530\n",
      "Batch 680 - Loss: 0.0508 - Accuracy: 0.9533\n",
      "Batch 690 - Loss: 0.0210 - Accuracy: 0.9533\n",
      "Batch 700 - Loss: 0.0363 - Accuracy: 0.9535\n",
      "Batch 710 - Loss: 0.1293 - Accuracy: 0.9535\n",
      "Batch 720 - Loss: 0.2402 - Accuracy: 0.9536\n",
      "Batch 730 - Loss: 0.3086 - Accuracy: 0.9534\n",
      "Batch 740 - Loss: 0.2712 - Accuracy: 0.9530\n",
      "Batch 750 - Loss: 0.0258 - Accuracy: 0.9528\n",
      "Batch 760 - Loss: 0.2729 - Accuracy: 0.9529\n",
      "Batch 770 - Loss: 0.0184 - Accuracy: 0.9531\n",
      "Batch 780 - Loss: 0.2190 - Accuracy: 0.9535\n",
      "Batch 790 - Loss: 0.4381 - Accuracy: 0.9534\n",
      "Batch 800 - Loss: 0.0974 - Accuracy: 0.9535\n",
      "Batch 810 - Loss: 0.1961 - Accuracy: 0.9536\n",
      "Batch 820 - Loss: 0.0959 - Accuracy: 0.9536\n",
      "Batch 830 - Loss: 0.0297 - Accuracy: 0.9537\n",
      "Batch 840 - Loss: 0.1095 - Accuracy: 0.9536\n",
      "Batch 850 - Loss: 0.0526 - Accuracy: 0.9537\n",
      "Batch 860 - Loss: 0.1340 - Accuracy: 0.9538\n",
      "Batch 870 - Loss: 0.1267 - Accuracy: 0.9540\n",
      "Batch 880 - Loss: 0.1007 - Accuracy: 0.9540\n",
      "Batch 890 - Loss: 0.0715 - Accuracy: 0.9540\n",
      "Batch 900 - Loss: 0.3691 - Accuracy: 0.9539\n",
      "Batch 910 - Loss: 0.0134 - Accuracy: 0.9542\n",
      "Batch 920 - Loss: 0.3922 - Accuracy: 0.9543\n",
      "Batch 930 - Loss: 0.0530 - Accuracy: 0.9544\n",
      "Batch 940 - Loss: 0.0268 - Accuracy: 0.9545\n",
      "Batch 950 - Loss: 0.2560 - Accuracy: 0.9548\n",
      "Batch 960 - Loss: 0.0794 - Accuracy: 0.9551\n",
      "Batch 970 - Loss: 0.2080 - Accuracy: 0.9553\n",
      "Batch 980 - Loss: 0.0676 - Accuracy: 0.9553\n",
      "Batch 990 - Loss: 0.2097 - Accuracy: 0.9551\n",
      "Epoch 4 - Loss: 0.1263 - Accuracy: 0.9554\n",
      "\n",
      "Epoch 5/5\n",
      "Batch 0 - Loss: 0.1538 - Accuracy: 0.9375\n",
      "Batch 10 - Loss: 0.0475 - Accuracy: 0.9318\n",
      "Batch 20 - Loss: 0.0467 - Accuracy: 0.9375\n",
      "Batch 30 - Loss: 0.0376 - Accuracy: 0.9456\n",
      "Batch 40 - Loss: 0.1055 - Accuracy: 0.9543\n",
      "Batch 50 - Loss: 0.0419 - Accuracy: 0.9583\n",
      "Batch 60 - Loss: 0.0633 - Accuracy: 0.9590\n",
      "Batch 70 - Loss: 0.2542 - Accuracy: 0.9613\n",
      "Batch 80 - Loss: 0.0635 - Accuracy: 0.9606\n",
      "Batch 90 - Loss: 0.0299 - Accuracy: 0.9609\n",
      "Batch 100 - Loss: 0.0184 - Accuracy: 0.9616\n",
      "Batch 110 - Loss: 0.0921 - Accuracy: 0.9617\n",
      "Batch 120 - Loss: 0.0254 - Accuracy: 0.9633\n",
      "Batch 130 - Loss: 0.0095 - Accuracy: 0.9647\n",
      "Batch 140 - Loss: 0.0129 - Accuracy: 0.9659\n",
      "Batch 150 - Loss: 0.0164 - Accuracy: 0.9656\n",
      "Batch 160 - Loss: 0.0157 - Accuracy: 0.9670\n",
      "Batch 170 - Loss: 0.0081 - Accuracy: 0.9664\n",
      "Batch 180 - Loss: 0.0115 - Accuracy: 0.9675\n",
      "Batch 190 - Loss: 0.0159 - Accuracy: 0.9666\n",
      "Batch 200 - Loss: 0.0392 - Accuracy: 0.9661\n",
      "Batch 210 - Loss: 0.0131 - Accuracy: 0.9668\n",
      "Batch 220 - Loss: 0.0941 - Accuracy: 0.9669\n",
      "Batch 230 - Loss: 0.3106 - Accuracy: 0.9675\n",
      "Batch 240 - Loss: 0.0077 - Accuracy: 0.9681\n",
      "Batch 250 - Loss: 0.0113 - Accuracy: 0.9681\n",
      "Batch 260 - Loss: 0.2946 - Accuracy: 0.9682\n",
      "Batch 270 - Loss: 0.0438 - Accuracy: 0.9670\n",
      "Batch 280 - Loss: 0.0063 - Accuracy: 0.9677\n",
      "Batch 290 - Loss: 0.0189 - Accuracy: 0.9678\n",
      "Batch 300 - Loss: 0.0179 - Accuracy: 0.9678\n",
      "Batch 310 - Loss: 0.0064 - Accuracy: 0.9680\n",
      "Batch 320 - Loss: 0.0348 - Accuracy: 0.9679\n",
      "Batch 330 - Loss: 0.0134 - Accuracy: 0.9681\n",
      "Batch 340 - Loss: 0.0539 - Accuracy: 0.9685\n",
      "Batch 350 - Loss: 0.0260 - Accuracy: 0.9685\n",
      "Batch 360 - Loss: 0.0171 - Accuracy: 0.9688\n",
      "Batch 370 - Loss: 0.0157 - Accuracy: 0.9693\n",
      "Batch 380 - Loss: 0.0491 - Accuracy: 0.9698\n",
      "Batch 390 - Loss: 0.1066 - Accuracy: 0.9701\n",
      "Batch 400 - Loss: 0.0192 - Accuracy: 0.9702\n",
      "Batch 410 - Loss: 0.0206 - Accuracy: 0.9699\n",
      "Batch 420 - Loss: 0.2232 - Accuracy: 0.9694\n",
      "Batch 430 - Loss: 0.0757 - Accuracy: 0.9697\n",
      "Batch 440 - Loss: 0.0376 - Accuracy: 0.9695\n",
      "Batch 450 - Loss: 0.0088 - Accuracy: 0.9695\n",
      "Batch 460 - Loss: 0.0062 - Accuracy: 0.9696\n",
      "Batch 470 - Loss: 0.0324 - Accuracy: 0.9699\n",
      "Batch 480 - Loss: 0.0778 - Accuracy: 0.9704\n",
      "Batch 490 - Loss: 0.0355 - Accuracy: 0.9703\n",
      "Batch 500 - Loss: 0.0165 - Accuracy: 0.9704\n",
      "Batch 510 - Loss: 0.0295 - Accuracy: 0.9702\n",
      "Batch 520 - Loss: 0.0115 - Accuracy: 0.9704\n",
      "Batch 530 - Loss: 0.1595 - Accuracy: 0.9705\n",
      "Batch 540 - Loss: 0.0310 - Accuracy: 0.9705\n",
      "Batch 550 - Loss: 0.2575 - Accuracy: 0.9706\n",
      "Batch 560 - Loss: 0.0699 - Accuracy: 0.9708\n",
      "Batch 570 - Loss: 0.2004 - Accuracy: 0.9708\n",
      "Batch 580 - Loss: 0.0127 - Accuracy: 0.9705\n",
      "Batch 590 - Loss: 0.0821 - Accuracy: 0.9706\n",
      "Batch 600 - Loss: 0.0180 - Accuracy: 0.9704\n",
      "Batch 610 - Loss: 0.0257 - Accuracy: 0.9704\n",
      "Batch 620 - Loss: 0.2226 - Accuracy: 0.9705\n",
      "Batch 630 - Loss: 0.0105 - Accuracy: 0.9704\n",
      "Batch 640 - Loss: 0.0825 - Accuracy: 0.9706\n",
      "Batch 650 - Loss: 0.0135 - Accuracy: 0.9705\n",
      "Batch 660 - Loss: 0.0631 - Accuracy: 0.9703\n",
      "Batch 670 - Loss: 0.0184 - Accuracy: 0.9703\n",
      "Batch 680 - Loss: 0.0394 - Accuracy: 0.9705\n",
      "Batch 690 - Loss: 0.0109 - Accuracy: 0.9706\n",
      "Batch 700 - Loss: 0.0188 - Accuracy: 0.9708\n",
      "Batch 710 - Loss: 0.0067 - Accuracy: 0.9709\n",
      "Batch 720 - Loss: 0.1966 - Accuracy: 0.9712\n",
      "Batch 730 - Loss: 0.0909 - Accuracy: 0.9714\n",
      "Batch 740 - Loss: 0.2909 - Accuracy: 0.9714\n",
      "Batch 750 - Loss: 0.2441 - Accuracy: 0.9715\n",
      "Batch 760 - Loss: 0.1564 - Accuracy: 0.9710\n",
      "Batch 770 - Loss: 0.0251 - Accuracy: 0.9709\n",
      "Batch 780 - Loss: 0.0434 - Accuracy: 0.9710\n",
      "Batch 790 - Loss: 0.0933 - Accuracy: 0.9709\n",
      "Batch 800 - Loss: 0.0127 - Accuracy: 0.9710\n",
      "Batch 810 - Loss: 0.0120 - Accuracy: 0.9712\n",
      "Batch 820 - Loss: 0.0181 - Accuracy: 0.9715\n",
      "Batch 830 - Loss: 0.1533 - Accuracy: 0.9716\n",
      "Batch 840 - Loss: 0.0058 - Accuracy: 0.9715\n",
      "Batch 850 - Loss: 0.0233 - Accuracy: 0.9714\n",
      "Batch 860 - Loss: 0.0069 - Accuracy: 0.9713\n",
      "Batch 870 - Loss: 0.0317 - Accuracy: 0.9713\n",
      "Batch 880 - Loss: 0.1033 - Accuracy: 0.9714\n",
      "Batch 890 - Loss: 0.0852 - Accuracy: 0.9714\n",
      "Batch 900 - Loss: 0.1369 - Accuracy: 0.9713\n",
      "Batch 910 - Loss: 0.1924 - Accuracy: 0.9712\n",
      "Batch 920 - Loss: 0.4473 - Accuracy: 0.9710\n",
      "Batch 930 - Loss: 0.0085 - Accuracy: 0.9710\n",
      "Batch 940 - Loss: 0.0276 - Accuracy: 0.9710\n",
      "Batch 950 - Loss: 0.0459 - Accuracy: 0.9711\n",
      "Batch 960 - Loss: 0.0227 - Accuracy: 0.9711\n",
      "Batch 970 - Loss: 0.0104 - Accuracy: 0.9712\n",
      "Batch 980 - Loss: 0.0139 - Accuracy: 0.9712\n",
      "Batch 990 - Loss: 0.0100 - Accuracy: 0.9711\n",
      "Epoch 5 - Loss: 0.0843 - Accuracy: 0.9712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/23 01:40:39 INFO mlflow.tracking._tracking_service.client: 🏃 View run BERT at: http://mlflow-server:5000/#/experiments/6/runs/4c4a6a811ebc4328903983e4cb188778.\n",
      "2024/11/23 01:40:39 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://mlflow-server:5000/#/experiments/6.\n"
     ]
    }
   ],
   "source": [
    "# Configuration MLflow\n",
    "experiment = mlflow.set_experiment(\"BERT\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"BERT\", nested=True):\n",
    "    # Enregistrement des hyperparamètres\n",
    "    mlflow.log_param(\"model_name\", \"bert-base-uncased\")\n",
    "    mlflow.log_param(\"num_epochs\", epochs)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"learning_rate\", 1e-5)\n",
    "    \n",
    "    # Boucle d'entraînement\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        train_accuracy_metric.reset_state()  # Réinitialiser l'accuracy pour chaque epoch\n",
    "        epoch_loss = []  # Réinitialiser la liste des pertes pour chaque epoch\n",
    "\n",
    "        for i in range(0, len(train_input_ids), batch_size):\n",
    "            # Obtenir un batch de données\n",
    "            batch_input_ids = train_input_ids[i:i + batch_size]\n",
    "            batch_attention_masks = train_attention_masks[i:i + batch_size]\n",
    "            batch_labels = train_labels[i:i + batch_size]\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                # Faire des prédictions\n",
    "                outputs = model(\n",
    "                    input_ids=batch_input_ids,\n",
    "                    attention_mask=batch_attention_masks,\n",
    "                    training=True\n",
    "                )\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Calculer la perte\n",
    "                loss = loss_fn(batch_labels, logits)\n",
    "            \n",
    "            # Calculer et appliquer les gradients\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "            # Mettre à jour l'accuracy et accumuler la perte\n",
    "            train_accuracy_metric.update_state(batch_labels, logits)\n",
    "            epoch_loss.append(loss.numpy())\n",
    "            \n",
    "            # Afficher la perte et l'accuracy toutes les 10 batches\n",
    "            if i % (batch_size * 10) == 0:\n",
    "                train_accuracy = train_accuracy_metric.result().numpy()\n",
    "                print(f\"Batch {i//batch_size} - Loss: {loss.numpy():.4f} - Accuracy: {train_accuracy:.4f}\")\n",
    "                mlflow.log_metric(\"batch_train_loss\", loss.numpy(), step=i//batch_size)\n",
    "                mlflow.log_metric(\"batch_train_accuracy\", train_accuracy, step=i//batch_size)\n",
    "        \n",
    "        # Enregistrement des métriques de l'époque\n",
    "        epoch_accuracy = train_accuracy_metric.result().numpy()\n",
    "        epoch_loss_avg = np.mean(epoch_loss)\n",
    "        print(f\"Epoch {epoch + 1} - Loss: {epoch_loss_avg:.4f} - Accuracy: {epoch_accuracy:.4f}\")\n",
    "        mlflow.log_metric(\"epoch_train_loss\", epoch_loss_avg, step=epoch)\n",
    "        mlflow.log_metric(\"epoch_train_accuracy\", epoch_accuracy, step=epoch)\n",
    "\n",
    "    # Libérer la mémoire GPU/CPU non utilisée avant l'évaluation\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "def log_plot_to_mlflow(figure, artifact_name):\n",
    "    \"\"\"\n",
    "    Enregistre une figure matplotlib dans MLflow en utilisant un buffer en mémoire.\n",
    "\n",
    "    :param figure: la figure matplotlib à sauvegarder\n",
    "    :param artifact_name: le nom de l'artefact pour MLflow (inclure \".png\")\n",
    "    \"\"\"\n",
    "    # Utilisation d'un buffer en mémoire\n",
    "    buffer = BytesIO()\n",
    "    figure.savefig(buffer, format=\"png\")\n",
    "    buffer.seek(0)\n",
    "    \n",
    "    # Sauvegarder dans MLflow à partir du buffer\n",
    "    with open(artifact_name, \"wb\") as f:\n",
    "        f.write(buffer.getvalue())\n",
    "    mlflow.log_artifact(artifact_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/23 09:50:07 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.11/site-packages/transformers/generation/tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n",
      "2024/11/23 09:50:13 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpjf0iw5im/model, flavor: keras). Fall back to return ['keras==3.6.0']. Set logging level to DEBUG to see the full traceback. \n",
      "2024/11/23 09:50:13 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Registered model 'bert-base-uncased' already exists. Creating a new version of this model...\n",
      "2024/11/23 09:50:13 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: bert-base-uncased, version 11\n",
      "Created version '11' of model 'bert-base-uncased'.\n",
      "2024/11/23 09:50:13 INFO mlflow.tracking._tracking_service.client: 🏃 View run sassy-hound-2 at: http://mlflow-server:5000/#/experiments/6/runs/e32580dc0917460d89806e404ec5d9f7.\n",
      "2024/11/23 09:50:13 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://mlflow-server:5000/#/experiments/6.\n"
     ]
    }
   ],
   "source": [
    " # Calcul et stockage des métriques dans un dictionnaire\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc, log_loss\n",
    "all_validation_metrics = []   \n",
    "# Évaluation du modèle sur le set de validation en mini-batches\n",
    "batch_size = 8  # Taille réduite pour l'évaluation\n",
    "num_batches = len(val_input_ids) // batch_size\n",
    "all_predictions = []\n",
    "# Initialisation du dictionnaire des métriques\n",
    "metrics_dict = {}\n",
    "all_probs = []\n",
    "# Initialisation d'une liste pour stocker les log loss par batch (optionnel, pour analyse batch-wise)\n",
    "log_losses = []\n",
    "\n",
    "for i in range(num_batches):\n",
    "    # Obtenir un batch de validation\n",
    "    batch_input_ids = val_input_ids[i * batch_size : (i + 1) * batch_size]\n",
    "    batch_attention_masks = val_attention_masks[i * batch_size : (i + 1) * batch_size]\n",
    "    batch_labels = val_labels[i * batch_size : (i + 1) * batch_size]  # Récupérer les labels correspondants\n",
    "\n",
    "\n",
    "    # Calcul des logits pour le batch\n",
    "    batch_logits = model(\n",
    "        input_ids=batch_input_ids,\n",
    "        attention_mask=batch_attention_masks,\n",
    "        training=False\n",
    "    ).logits\n",
    "\n",
    "    # Stocker les prédictions\n",
    "    batch_predictions = tf.argmax(batch_logits, axis=1)\n",
    "    all_predictions.append(batch_predictions)\n",
    "    \n",
    "     # Convertir logits en probabilités\n",
    "    batch_probabilities = tf.nn.softmax(batch_logits, axis=1)[:, 1]  # Probabilités pour la classe 1\n",
    "    all_probs.append(batch_probabilities)\n",
    "    \n",
    "    # Calcul du log loss pour ce batch (en explicitant les classes [0, 1])\n",
    "    batch_log_loss = log_loss(batch_labels, batch_probabilities, labels=[0, 1])\n",
    "    log_losses.append(batch_log_loss)\n",
    "\n",
    "\n",
    "# Concaténer toutes les prédictions\n",
    "all_predictions = tf.concat(all_predictions, axis=0)\n",
    "\n",
    "# Concaténer toutes les probabilités\n",
    "all_probs = tf.concat(all_probs, axis=0).numpy()\n",
    "\n",
    "\n",
    "\n",
    "# Calcul du log loss total sur l'ensemble des données\n",
    "# total_log_loss = log_loss(val_labels[:len(all_probs)], all_probs, labels=[0, 1])\n",
    "# validation_metrics[\"Validation Log Loss\"] = round(total_log_loss, 3)\n",
    "# print(f\"Validation Log Loss: {total_log_loss:.4f}\")\n",
    "\n",
    "# # Tracer le log loss par batch\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(range(len(log_losses)), log_losses, marker='o', label=\"Log Loss par batch\")\n",
    "# plt.axhline(total_log_loss, color='red', linestyle='--', label=f\"Log Loss total ({total_log_loss:.4f})\")\n",
    "# plt.xlabel(\"Index des batches\")\n",
    "# plt.ylabel(\"Log Loss\")\n",
    "# plt.title(\"Logarithmic Loss par Batch\")\n",
    "# plt.legend()\n",
    "# log_plot_to_mlflow(plt.gcf(), \"log_loss_batches.png\")\n",
    "# plt.show()\n",
    "\n",
    "# # Ajout du log loss dans MLflow\n",
    "# mlflow.log_metric(\"val_log_loss\", total_log_loss)\n",
    "\n",
    "\n",
    "\n",
    "# Calcul de la courbe ROC\n",
    "fpr, tpr, thresholds = roc_curve(val_labels[:len(all_probs)], all_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Tracer la courbe ROC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random guess\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Bert ROC Curve - Validation Set\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "log_plot_to_mlflow(plt.gcf(), \"roc_curve_val.png\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "# Calcul de l'accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(all_predictions == val_labels[:len(all_predictions)], dtype=tf.float32))\n",
    "print(f\"Validation Accuracy: {accuracy.numpy():.4f}\")\n",
    "\n",
    "validation_metrics = {\n",
    "    \"Validation Accuracy\": round(float(accuracy.numpy()), 3),\n",
    "    \"Validation ROC AUC\": round(roc_auc, 3),\n",
    "    \"Validation Precision\": round(precision_score(val_labels[:len(all_predictions)], all_predictions.numpy()), 3),\n",
    "    \"Validation Recall\": round(recall_score(val_labels[:len(all_predictions)], all_predictions.numpy()), 3),\n",
    "    \"Validation F1\": round(f1_score(val_labels[:len(all_predictions)], all_predictions.numpy()), 3)\n",
    "}\n",
    "\n",
    "metrics_dict.update(validation_metrics)\n",
    "# Log de l'accuracy finale de validation et des autres métriques dans MLflow\n",
    "mlflow.log_metrics(metrics_dict)\n",
    "\n",
    "# Log de l'accuracy finale de validation dans MLflow\n",
    "mlflow.log_metric(\"val_accuracy\", accuracy.numpy())\n",
    "\n",
    "# Enregistrement du modèle\n",
    "mlflow.keras.log_model(model, \"bert_model\")\n",
    "\n",
    "run_id = mlflow.active_run().info.run_id\n",
    "result = mlflow.register_model(\n",
    "    model_uri=f\"runs:/{run_id}/model\",\n",
    "    name=f\"bert-base-uncased\"\n",
    ")\n",
    "\n",
    "# Construire le lien MLflow correspondant (en supposant que vous avez l'URL de votre serveur MLflow)\n",
    "active_run = mlflow.active_run()\n",
    "run_id = active_run.info.run_id\n",
    "#run_name = active_run.data.tags.get(\"mlflow.runName\")  # Obtenir le nom du run depuis les tags\n",
    "\n",
    "# Récupérer l'ID de l'expérience active\n",
    "experiment_id = active_run.info.experiment_id\n",
    "\n",
    "# Définir l'URL de votre serveur MLflow\n",
    "mlflow_server_url = \"http://localhost:5000\"  # Remplacez par l'URL réel de votre serveur MLflow\n",
    "\n",
    "# Construire le lien complet vers le run dans l'interface MLflow\n",
    "run_link = f\"{mlflow_server_url}/#/experiments/{experiment_id}/runs/{run_id}\"\n",
    "\n",
    "# Ajouter les métriques de validation à la liste\n",
    "all_validation_metrics.append({\n",
    "    \"run_name\": f\"bert-base-uncased-trained\",\n",
    "    **validation_metrics,\n",
    "    \"run_id\": run_link\n",
    "})\n",
    "mlflow.log_table(pd.DataFrame(all_validation_metrics), \"3-bert-base-uncased.json\")\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Validation Accuracy': 0.812,\n",
       " 'Validation ROC AUC': 0.89,\n",
       " 'Validation Precision': 0.787,\n",
       " 'Validation Recall': 0.852,\n",
       " 'Validation F1': 0.818}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(validation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Validation ROC AUC</th>\n",
       "      <th>Validation Precision</th>\n",
       "      <th>Validation Recall</th>\n",
       "      <th>Validation F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.812</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Validation Accuracy  Validation ROC AUC  Validation Precision  \\\n",
       "0                0.812                0.89                 0.787   \n",
       "\n",
       "   Validation Recall  Validation F1  \n",
       "0              0.852          0.818  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(metrics_dict, index=[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Globale\n",
    "\n",
    "## Points Forts\n",
    "- Le modèle montre une **bonne capacité de généralisation** avec :\n",
    "  - Une **Accuracy** respectable de 81.2%.\n",
    "  - Un **AUC-ROC élevé** de 0.89, indiquant une bonne capacité à distinguer les classes.\n",
    "- Le **Rappel élevé (85.2%)** montre que le modèle détecte efficacement les vrais positifs, \n",
    "  ce qui est souvent crucial dans des applications critiques.\n",
    "\n",
    "## Points d'Amélioration\n",
    "- La **Précision (78.7%)** est légèrement inférieure au Rappel, ce qui suggère la présence \n",
    "  de quelques **faux positifs**.\n",
    "\n",
    "## Questions à Explorer\n",
    "1. **Pourquoi un déséquilibre entre précision et rappel ?**\n",
    "   - Le modèle semble favoriser le rappel (capturer les vrais positifs) au détriment de la précision (Tweet neutre?).\n",
    "2. **Quel est l'impact métier des faux positifs et des faux négatifs ?**\n",
    "   - Comprendre les coûts spécifiques de ces erreurs est crucial pour affiner le modèle.\n",
    "\n",
    "---\n",
    "\n",
    "# Recommandations\n",
    "\n",
    "1. **Inspecter la matrice de confusion :**\n",
    "   - Cela permettra de mieux comprendre les types d'erreurs commises \n",
    "     (faux positifs et faux négatifs).\n",
    "\n",
    "2. **Ajuster le seuil de classification :**\n",
    "   - Modifier le seuil peut équilibrer la Précision et le Rappel selon les priorités métier.\n",
    "\n",
    "3. **Tester sur d'autres métriques métier :**\n",
    "   - Par exemple, le coût pondéré des erreurs ou le taux de faux positifs \n",
    "     peut être plus pertinent pour des besoins spécifiques.\n",
    "\n",
    "4. **Analyse approfondie des erreurs :**\n",
    "   - Étudier les tweets mal classifiés pour identifier des tendances \n",
    "     ou des motifs que le modèle ne capte pas correctement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Le modèle **`bert-base-uncased`** a montré des performances prometteuses sur notre jeu de données, surpassant les résultats obtenus avec certains modèles spécialisés comme ceux adaptés à Twitter. Toutefois, la gestion des faux positifs et l'équilibrage entre Précision et Rappel nécessitent une attention particulière. Une étude plus poussée des erreurs et des ajustements de seuil pourraient encore améliorer ses performances et sa pertinence dans le contexte métier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
