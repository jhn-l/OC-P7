{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 12:12:04.035759: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732018324.048966  559654 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732018324.052872  559654 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-19 12:12:04.066926: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlflow.set_tracking_uri(\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "Transformers version: 4.46.2\n"
     ]
    }
   ],
   "source": [
    "# Affichage des versions pour v√©rification\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   target  20000 non-null  int64 \n",
      " 1   ids     20000 non-null  int64 \n",
      " 2   date    20000 non-null  object\n",
      " 3   flag    20000 non-null  object\n",
      " 4   user    20000 non-null  object\n",
      " 5   text    20000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 937.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Recharger le DataFrame depuis le fichier pickle\n",
    "df_sample = pd.read_pickle('download/df_sample_20000.pkl')\n",
    "df_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    10000\n",
       "1    10000\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " #V√©rifier si le DataFrame a au moins 16 000 lignes\n",
    "if len(df_sample) != 20000:\n",
    "    raise ValueError(\"Le DataFrame contient moins de 16 000 lignes.\")\n",
    "\n",
    "# # Calculer la proportion n√©cessaire pour obtenir 16 000 lignes\n",
    "# sample_size = 16000 / len(df_sample)\n",
    "\n",
    "# # Utiliser train_test_split pour s√©lectionner un √©chantillon √©quilibr√© de 16 000 lignes\n",
    "# df_16000, _ = train_test_split(df_sample, train_size=sample_size, stratify=df_sample['target'], random_state=42)\n",
    "\n",
    "# # V√©rifier le nombre d'√©l√©ments et l'√©quilibre des classes\n",
    "# print(f\"Nombre d'√©chantillons conserv√©s: {len(df_16000)}\")\n",
    "# print(df_16000['target'].value_counts(normalize=True))  # V√©rifier l'√©quilibre des classes\n",
    "\n",
    "display(df_sample['target'].value_counts())\n",
    "df = df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_559654/3865627404.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sample = df.groupby('target').apply(lambda x: x.sample(n=samples_per_class)).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    500\n",
       "1    500\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filtrer pour un sous-ensemble\n",
    "# Taille de l'√©chantillon d√©sir√©\n",
    "sample_size = 1000\n",
    "\n",
    "# Calcul du nombre d'√©chantillons par classe\n",
    "classes = df['target'].unique()\n",
    "samples_per_class = sample_size // len(classes)\n",
    "\n",
    "# √âchantillonnage stratifi√©\n",
    "df_sample = df.groupby('target').apply(lambda x: x.sample(n=samples_per_class)).reset_index(drop=True)\n",
    "display(df_sample['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis were not used when initializing TFRobertaModel: ['classifier']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFRobertaModel were not initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis and are newly initialized: ['roberta/pooler/dense/bias:0', 'roberta/pooler/dense/kernel:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-11-19 13:12:26.843461: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024/11/19 13:12:26 WARNING mlflow.data.tensorflow_dataset: Failed to infer schema for TensorFlow dataset. Exception: Failed to infer schema for tf.data.Dataset. Schemas can only be inferred if the dataset consists of tensors. Ragged tensors, tensor arrays, and other types are not supported. Additionally, datasets with nested tensors are not supported.\n",
      "2024-11-19 13:12:26.875601: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024/11/19 13:12:26 WARNING mlflow.data.tensorflow_dataset: Failed to infer schema for TensorFlow dataset. Exception: Failed to infer schema for tf.data.Dataset. Schemas can only be inferred if the dataset consists of tensors. Ragged tensors, tensor arrays, and other types are not supported. Additionally, datasets with nested tensors are not supported.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.11/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['input_ids', 'attention_mask']. Received: the structure of inputs={'input_ids': '*', 'token_type_ids': '*', 'attention_mask': '*'}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684ms/step - accuracy: 0.4953 - loss: 0.6986"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 892ms/step - accuracy: 0.4953 - loss: 0.6986 - val_accuracy: 0.4800 - val_loss: 0.7189\n",
      "Epoch 2/5\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662ms/step - accuracy: 0.5053 - loss: 0.7028"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 828ms/step - accuracy: 0.5049 - loss: 0.7028 - val_accuracy: 0.5200 - val_loss: 0.6944\n",
      "Epoch 3/5\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 868ms/step - accuracy: 0.4983 - loss: 0.7003 - val_accuracy: 0.4800 - val_loss: 0.7042\n",
      "Epoch 4/5\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657ms/step - accuracy: 0.4785 - loss: 0.7051"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 824ms/step - accuracy: 0.4785 - loss: 0.7052 - val_accuracy: 0.5200 - val_loss: 0.6924\n",
      "Epoch 5/5\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 823ms/step - accuracy: 0.5102 - loss: 0.6945 - val_accuracy: 0.4800 - val_loss: 0.6946\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/19 13:16:09 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmp9xabzx03/model, flavor: tensorflow). Fall back to return ['tensorflow==2.18.0', 'cloudpickle==3.1.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 782ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "2024/11/19 13:16:20 INFO mlflow.tracking._tracking_service.client: üèÉ View run Fine-tuning finiteautomata/bertweet-base-sentiment-analysis at: http://mlflow-server:5000/#/experiments/6/runs/94882ee1affe45fc8f2aed9528e7085e.\n",
      "2024/11/19 13:16:20 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://mlflow-server:5000/#/experiments/6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.48\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      1.00      0.65        96\n",
      "           1       0.00      0.00      0.00       104\n",
      "\n",
      "    accuracy                           0.48       200\n",
      "   macro avg       0.24      0.50      0.32       200\n",
      "weighted avg       0.23      0.48      0.31       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import emoji\n",
    "from transformers import AutoTokenizer, TFAutoModel, AutoConfig\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "\n",
    "# Pr√©traitement des tweets\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    tweet = emoji.demojize(tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'[^a-zA-Z0-9\\s]', '', tweet)\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    return tweet\n",
    "\n",
    "# Nettoyage des tweets\n",
    "# df_sample = df_sample.copy().sample(frac=0.1, random_state=42)  # Utilisez 10% des donn√©es\n",
    "# display(df_sample['target'].value_counts())\n",
    "df_sample['cleaned_text'] = df_sample['text'].apply(preprocess_tweet)\n",
    "\n",
    "# Diviser les donn√©es en train et test\n",
    "documents = df_sample['cleaned_text']\n",
    "labels = df_sample['target']\n",
    "X_train, X_val, y_train, y_val = train_test_split(documents, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Charger le tokenizer\n",
    "model_name = 'finiteautomata/bertweet-base-sentiment-analysis'\n",
    "#model_name = 'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenisation des donn√©es\n",
    "def tokenize_texts(texts, max_length=128):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_texts(X_train, max_length=80)\n",
    "val_encodings = tokenize_texts(X_val, max_length=80)\n",
    "\n",
    "# Pr√©parer les datasets TensorFlow\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    y_train.values\n",
    ")).shuffle(len(X_train)).batch(16)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    y_val.values\n",
    ")).batch(16)\n",
    "\n",
    "# Charger la configuration et le mod√®le pr√©-entra√Æn√© sans la couche de classification\n",
    "config = AutoConfig.from_pretrained(model_name, output_hidden_states=False)\n",
    "base_model = TFAutoModel.from_pretrained(model_name, config=config)\n",
    "\n",
    "# Geler les 10 premi√®res couches du mod√®le pour acc√©l√©rer l'entra√Ænement\n",
    "# for layer in base_model.layers[:-1]:\n",
    "#     layer.trainable = False\n",
    "# base_model.layers[-1].trainable = True\n",
    "\n",
    "\n",
    "# Cr√©er un nouveau mod√®le Keras\n",
    "input_ids = Input(shape=(80,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = Input(shape=(80,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# Utiliser Lambda pour appeler le mod√®le pr√©-entra√Æn√© avec output_shape sp√©cifi√©\n",
    "sequence_output = Lambda(\n",
    "    lambda inputs: base_model(input_ids=inputs[0], attention_mask=inputs[1])[0],\n",
    "    output_shape=(80, 768)\n",
    ")([input_ids, attention_mask])\n",
    "\n",
    "cls_token = Lambda(lambda x: x[:, 0, :], output_shape=(768,))(sequence_output)  # Extraire le token [CLS]\n",
    "\n",
    "# Ajouter une couche dense pour la classification binaire\n",
    "output = Dense(1, activation=\"sigmoid\")(cls_token)\n",
    "\n",
    "# Construire le mod√®le final\n",
    "model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "# Optimiseur et perte\n",
    "learning_rate = 1e-5\n",
    "optimizer = Adam()\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "metric = tf.keras.metrics.BinaryAccuracy('accuracy')\n",
    "\n",
    "# Compiler le mod√®le\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# Int√©gration avec MLflow\n",
    "#mlflow.tensorflow.autolog()\n",
    "\n",
    "with mlflow.start_run(run_name=f\"Fine-tuning {model_name}\", nested=True):\n",
    "    mlflow.set_tag(\"model_name\", model_name)\n",
    "    mlflow.log_param(\"epochs\", 5)\n",
    "    mlflow.log_param(\"batch_size\", 16)\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "\n",
    "    # D√©sactiver l'autolog de TensorFlow\n",
    "    #mlflow.tensorflow.autolog(disable=True)\n",
    "\n",
    "    # Entra√Æner le mod√®le\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=5\n",
    "    )\n",
    "\n",
    "    # Pr√©dictions sur le jeu de validation\n",
    "    val_logits = model.predict(val_dataset)\n",
    "    y_val_pred = (val_logits > 0.5).astype(int)\n",
    "\n",
    "    # √âvaluer les performances\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    print(\"Validation Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "    # # Log des m√©triques finales dans MLflow\n",
    "    # mlflow.log_metric(\"Validation Accuracy\", accuracy)\n",
    "    # mlflow.log_dict(\n",
    "    #     classification_report(y_val, y_val_pred, output_dict=True),\n",
    "    #     \"classification_report.json\"\n",
    "    # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    500\n",
       "1    500\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_sample['target'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annexe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Annexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Charger le tokenizer et le mod√®le\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de donn√©es pour 'documents' et 'labels'\n",
    "documents = df_sample['text']  # Liste de textes √† analyser\n",
    "labels = df_sample['target']   # Liste de labels (0 ou 1 pour la classification binaire)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization des donn√©es\n",
    "def tokenize_data(documents):\n",
    "    return tokenizer(\n",
    "        documents.tolist(),\n",
    "        max_length=128, padding=True, truncation=True, return_tensors='tf'\n",
    "    )\n",
    "tokens = tokenize_data(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir en NumPy pour train_test_split\n",
    "input_ids_np = tokens['input_ids'].numpy()\n",
    "attention_masks_np = tokens['attention_mask'].numpy()\n",
    "labels_np = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©parer les ensembles de donn√©es\n",
    "train_input_ids, val_input_ids, train_labels, val_labels = train_test_split(input_ids_np, labels_np, test_size=0.2, random_state=42)\n",
    "train_attention_masks, val_attention_masks = train_test_split(attention_masks_np, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurer la fonction de perte et l'optimiseur\n",
    "loss_fn = SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = Adam(learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir le nombre d'√©poques et la taille du batch\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "\n",
    "# D√©finir l'accuracy\n",
    "train_accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "Batch 0 - Loss: 0.7571 - Accuracy: 0.3750\n",
      "Batch 10 - Loss: 0.6762 - Accuracy: 0.5852\n",
      "Batch 20 - Loss: 0.6756 - Accuracy: 0.6012\n",
      "Batch 30 - Loss: 0.7363 - Accuracy: 0.6129\n",
      "Batch 40 - Loss: 0.6564 - Accuracy: 0.6174\n",
      "Epoch 1 - Loss: 0.6640 - Accuracy: 0.6175\n",
      "\n",
      "Epoch 2/5\n",
      "Batch 0 - Loss: 0.6280 - Accuracy: 0.6875\n",
      "Batch 10 - Loss: 0.5860 - Accuracy: 0.7614\n",
      "Batch 20 - Loss: 0.5801 - Accuracy: 0.7381\n",
      "Batch 30 - Loss: 0.5177 - Accuracy: 0.7520\n",
      "Batch 40 - Loss: 0.5065 - Accuracy: 0.7637\n",
      "Epoch 2 - Loss: 0.5328 - Accuracy: 0.7763\n",
      "\n",
      "Epoch 3/5\n",
      "Batch 0 - Loss: 0.3318 - Accuracy: 0.9375\n",
      "Batch 10 - Loss: 0.3797 - Accuracy: 0.8693\n",
      "Batch 20 - Loss: 0.3074 - Accuracy: 0.8690\n",
      "Batch 30 - Loss: 0.2170 - Accuracy: 0.8669\n",
      "Batch 40 - Loss: 0.2157 - Accuracy: 0.8750\n",
      "Epoch 3 - Loss: 0.3613 - Accuracy: 0.8788\n",
      "\n",
      "Epoch 4/5\n",
      "Batch 0 - Loss: 0.1782 - Accuracy: 1.0000\n",
      "Batch 10 - Loss: 0.3427 - Accuracy: 0.9318\n",
      "Batch 20 - Loss: 0.1576 - Accuracy: 0.9405\n",
      "Batch 30 - Loss: 0.1141 - Accuracy: 0.9435\n",
      "Batch 40 - Loss: 0.1427 - Accuracy: 0.9390\n",
      "Epoch 4 - Loss: 0.2268 - Accuracy: 0.9375\n",
      "\n",
      "Epoch 5/5\n",
      "Batch 0 - Loss: 0.1175 - Accuracy: 1.0000\n",
      "Batch 10 - Loss: 0.2146 - Accuracy: 0.9545\n",
      "Batch 20 - Loss: 0.1394 - Accuracy: 0.9524\n",
      "Batch 30 - Loss: 0.1696 - Accuracy: 0.9536\n",
      "Batch 40 - Loss: 0.0650 - Accuracy: 0.9604\n",
      "Epoch 5 - Loss: 0.1478 - Accuracy: 0.9600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/19 13:58:25 INFO mlflow.tracking._tracking_service.client: üèÉ View run BERT at: http://mlflow-server:5000/#/experiments/6/runs/7f74236345d4450d808e78ef5ccde02c.\n",
      "2024/11/19 13:58:25 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://mlflow-server:5000/#/experiments/6.\n"
     ]
    }
   ],
   "source": [
    "# Configuration MLflow\n",
    "experiment = mlflow.set_experiment(\"BERT\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"BERT\", nested=True):\n",
    "    # Enregistrement des hyperparam√®tres\n",
    "    mlflow.log_param(\"model_name\", \"bert-base-uncased\")\n",
    "    mlflow.log_param(\"num_epochs\", epochs)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"learning_rate\", 1e-5)\n",
    "    \n",
    "    # Boucle d'entra√Ænement\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        train_accuracy_metric.reset_state()  # R√©initialiser l'accuracy pour chaque epoch\n",
    "        epoch_loss = []  # R√©initialiser la liste des pertes pour chaque epoch\n",
    "\n",
    "        for i in range(0, len(train_input_ids), batch_size):\n",
    "            # Obtenir un batch de donn√©es\n",
    "            batch_input_ids = train_input_ids[i:i + batch_size]\n",
    "            batch_attention_masks = train_attention_masks[i:i + batch_size]\n",
    "            batch_labels = train_labels[i:i + batch_size]\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                # Faire des pr√©dictions\n",
    "                outputs = model(\n",
    "                    input_ids=batch_input_ids,\n",
    "                    attention_mask=batch_attention_masks,\n",
    "                    training=True\n",
    "                )\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Calculer la perte\n",
    "                loss = loss_fn(batch_labels, logits)\n",
    "            \n",
    "            # Calculer et appliquer les gradients\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "            # Mettre √† jour l'accuracy et accumuler la perte\n",
    "            train_accuracy_metric.update_state(batch_labels, logits)\n",
    "            epoch_loss.append(loss.numpy())\n",
    "            \n",
    "            # Afficher la perte et l'accuracy toutes les 10 batches\n",
    "            if i % (batch_size * 10) == 0:\n",
    "                train_accuracy = train_accuracy_metric.result().numpy()\n",
    "                print(f\"Batch {i//batch_size} - Loss: {loss.numpy():.4f} - Accuracy: {train_accuracy:.4f}\")\n",
    "                mlflow.log_metric(\"batch_train_loss\", loss.numpy(), step=i//batch_size)\n",
    "                mlflow.log_metric(\"batch_train_accuracy\", train_accuracy, step=i//batch_size)\n",
    "        \n",
    "        # Enregistrement des m√©triques de l'√©poque\n",
    "        epoch_accuracy = train_accuracy_metric.result().numpy()\n",
    "        epoch_loss_avg = np.mean(epoch_loss)\n",
    "        print(f\"Epoch {epoch + 1} - Loss: {epoch_loss_avg:.4f} - Accuracy: {epoch_accuracy:.4f}\")\n",
    "        mlflow.log_metric(\"epoch_train_loss\", epoch_loss_avg, step=epoch)\n",
    "        mlflow.log_metric(\"epoch_train_accuracy\", epoch_accuracy, step=epoch)\n",
    "\n",
    "    # Lib√©rer la m√©moire GPU/CPU non utilis√©e avant l'√©valuation\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "def log_plot_to_mlflow(figure, artifact_name):\n",
    "    \"\"\"\n",
    "    Enregistre une figure matplotlib dans MLflow en utilisant un buffer en m√©moire.\n",
    "\n",
    "    :param figure: la figure matplotlib √† sauvegarder\n",
    "    :param artifact_name: le nom de l'artefact pour MLflow (inclure \".png\")\n",
    "    \"\"\"\n",
    "    # Utilisation d'un buffer en m√©moire\n",
    "    buffer = BytesIO()\n",
    "    figure.savefig(buffer, format=\"png\")\n",
    "    buffer.seek(0)\n",
    "    \n",
    "    # Sauvegarder dans MLflow √† partir du buffer\n",
    "    with open(artifact_name, \"wb\") as f:\n",
    "        f.write(buffer.getvalue())\n",
    "    mlflow.log_artifact(artifact_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/19 13:58:45 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.11/site-packages/transformers/generation/tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n",
      "2024/11/19 13:58:51 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmph_8ytx4l/model, flavor: keras). Fall back to return ['keras==3.6.0']. Set logging level to DEBUG to see the full traceback. \n",
      "2024/11/19 13:58:51 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Registered model 'bert-base-uncased' already exists. Creating a new version of this model...\n",
      "2024/11/19 13:58:53 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: bert-base-uncased, version 3\n",
      "Created version '3' of model 'bert-base-uncased'.\n"
     ]
    }
   ],
   "source": [
    " # Calcul et stockage des m√©triques dans un dictionnaire\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc\n",
    "    \n",
    "# √âvaluation du mod√®le sur le set de validation en mini-batches\n",
    "batch_size = 8  # Taille r√©duite pour l'√©valuation\n",
    "num_batches = len(val_input_ids) // batch_size\n",
    "all_predictions = []\n",
    "# Initialisation du dictionnaire des m√©triques\n",
    "metrics_dict = {}\n",
    "all_probs = []\n",
    "\n",
    "for i in range(num_batches):\n",
    "    # Obtenir un batch de validation\n",
    "    batch_input_ids = val_input_ids[i * batch_size : (i + 1) * batch_size]\n",
    "    batch_attention_masks = val_attention_masks[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "    # Calcul des logits pour le batch\n",
    "    batch_logits = model(\n",
    "        input_ids=batch_input_ids,\n",
    "        attention_mask=batch_attention_masks,\n",
    "        training=False\n",
    "    ).logits\n",
    "\n",
    "    # Stocker les pr√©dictions\n",
    "    batch_predictions = tf.argmax(batch_logits, axis=1)\n",
    "    all_predictions.append(batch_predictions)\n",
    "    \n",
    "     # Convertir logits en probabilit√©s\n",
    "    batch_probabilities = tf.nn.softmax(batch_logits, axis=1)[:, 1]  # Probabilit√©s pour la classe 1\n",
    "    all_probs.append(batch_probabilities)\n",
    "\n",
    "# Concat√©ner toutes les pr√©dictions\n",
    "all_predictions = tf.concat(all_predictions, axis=0)\n",
    "\n",
    "# Concat√©ner toutes les probabilit√©s\n",
    "all_probs = tf.concat(all_probs, axis=0).numpy()\n",
    "\n",
    "# Calcul de la courbe ROC\n",
    "fpr, tpr, thresholds = roc_curve(val_labels[:len(all_probs)], all_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Tracer la courbe ROC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random guess\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Bert ROC Curve - Validation Set\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "log_plot_to_mlflow(plt.gcf(), \"roc_curve_val.png\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "# Calcul de l'accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(all_predictions == val_labels[:len(all_predictions)], dtype=tf.float32))\n",
    "print(f\"Validation Accuracy: {accuracy.numpy():.4f}\")\n",
    "\n",
    "validation_metrics = {\n",
    "    \"Validation Accuracy\": round(float(accuracy.numpy()), 3),\n",
    "    \"Validation ROC AUC\": round(roc_auc, 3),\n",
    "    \"Validation Precision\": round(precision_score(val_labels[:len(all_predictions)], all_predictions.numpy()), 3),\n",
    "    \"Validation Recall\": round(recall_score(val_labels[:len(all_predictions)], all_predictions.numpy()), 3),\n",
    "    \"Validation F1\": round(f1_score(val_labels[:len(all_predictions)], all_predictions.numpy()), 3)\n",
    "}\n",
    "\n",
    "metrics_dict.update(validation_metrics)\n",
    "# Log de l'accuracy finale de validation et des autres m√©triques dans MLflow\n",
    "mlflow.log_metrics(metrics_dict)\n",
    "\n",
    "# Log de l'accuracy finale de validation dans MLflow\n",
    "mlflow.log_metric(\"val_accuracy\", accuracy.numpy())\n",
    "\n",
    "# Enregistrement du mod√®le\n",
    "mlflow.keras.log_model(model, \"bert_model\")\n",
    "\n",
    "run_id = mlflow.active_run().info.run_id\n",
    "result = mlflow.register_model(\n",
    "    model_uri=f\"runs:/{run_id}/model\",\n",
    "    name=f\"bert-base-uncased\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Validation Accuracy': 0.75,\n",
       " 'Validation ROC AUC': 0.875,\n",
       " 'Validation Precision': 0.821,\n",
       " 'Validation Recall': 0.663,\n",
       " 'Validation F1': 0.734}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(validation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Validation ROC AUC</th>\n",
       "      <th>Validation Precision</th>\n",
       "      <th>Validation Recall</th>\n",
       "      <th>Validation F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Validation Accuracy  Validation ROC AUC  Validation Precision  \\\n",
       "0                 0.75               0.875                 0.821   \n",
       "\n",
       "   Validation Recall  Validation F1  \n",
       "0              0.663          0.734  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(metrics_dict, index=[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
