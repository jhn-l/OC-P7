{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 12:12:04.035759: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732018324.048966  559654 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732018324.052872  559654 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-19 12:12:04.066926: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlflow.set_tracking_uri(\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "Transformers version: 4.46.2\n"
     ]
    }
   ],
   "source": [
    "# Affichage des versions pour vérification\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   target  20000 non-null  int64 \n",
      " 1   ids     20000 non-null  int64 \n",
      " 2   date    20000 non-null  object\n",
      " 3   flag    20000 non-null  object\n",
      " 4   user    20000 non-null  object\n",
      " 5   text    20000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 937.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Recharger le DataFrame depuis le fichier pickle\n",
    "df_sample = pd.read_pickle('download/df_sample_20000.pkl')\n",
    "df_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    10000\n",
       "1    10000\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " #Vérifier si le DataFrame a au moins 16 000 lignes\n",
    "if len(df_sample) != 20000:\n",
    "    raise ValueError(\"Le DataFrame contient moins de 16 000 lignes.\")\n",
    "\n",
    "# # Calculer la proportion nécessaire pour obtenir 16 000 lignes\n",
    "# sample_size = 16000 / len(df_sample)\n",
    "\n",
    "# # Utiliser train_test_split pour sélectionner un échantillon équilibré de 16 000 lignes\n",
    "# df_16000, _ = train_test_split(df_sample, train_size=sample_size, stratify=df_sample['target'], random_state=42)\n",
    "\n",
    "# # Vérifier le nombre d'éléments et l'équilibre des classes\n",
    "# print(f\"Nombre d'échantillons conservés: {len(df_16000)}\")\n",
    "# print(df_16000['target'].value_counts(normalize=True))  # Vérifier l'équilibre des classes\n",
    "\n",
    "display(df_sample['target'].value_counts())\n",
    "df = df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_559654/3865627404.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sample = df.groupby('target').apply(lambda x: x.sample(n=samples_per_class)).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    500\n",
       "1    500\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filtrer pour un sous-ensemble\n",
    "# Taille de l'échantillon désiré\n",
    "sample_size = 1000\n",
    "\n",
    "# Calcul du nombre d'échantillons par classe\n",
    "classes = df['target'].unique()\n",
    "samples_per_class = sample_size // len(classes)\n",
    "\n",
    "# Échantillonnage stratifié\n",
    "df_sample = df.groupby('target').apply(lambda x: x.sample(n=samples_per_class)).reset_index(drop=True)\n",
    "display(df_sample['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis were not used when initializing TFRobertaModel: ['classifier']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFRobertaModel were not initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis and are newly initialized: ['roberta/pooler/dense/bias:0', 'roberta/pooler/dense/kernel:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-11-19 13:12:26.843461: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024/11/19 13:12:26 WARNING mlflow.data.tensorflow_dataset: Failed to infer schema for TensorFlow dataset. Exception: Failed to infer schema for tf.data.Dataset. Schemas can only be inferred if the dataset consists of tensors. Ragged tensors, tensor arrays, and other types are not supported. Additionally, datasets with nested tensors are not supported.\n",
      "2024-11-19 13:12:26.875601: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024/11/19 13:12:26 WARNING mlflow.data.tensorflow_dataset: Failed to infer schema for TensorFlow dataset. Exception: Failed to infer schema for tf.data.Dataset. Schemas can only be inferred if the dataset consists of tensors. Ragged tensors, tensor arrays, and other types are not supported. Additionally, datasets with nested tensors are not supported.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.11/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['input_ids', 'attention_mask']. Received: the structure of inputs={'input_ids': '*', 'token_type_ids': '*', 'attention_mask': '*'}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684ms/step - accuracy: 0.4953 - loss: 0.6986"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 892ms/step - accuracy: 0.4953 - loss: 0.6986 - val_accuracy: 0.4800 - val_loss: 0.7189\n",
      "Epoch 2/5\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662ms/step - accuracy: 0.5053 - loss: 0.7028"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 828ms/step - accuracy: 0.5049 - loss: 0.7028 - val_accuracy: 0.5200 - val_loss: 0.6944\n",
      "Epoch 3/5\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 868ms/step - accuracy: 0.4983 - loss: 0.7003 - val_accuracy: 0.4800 - val_loss: 0.7042\n",
      "Epoch 4/5\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657ms/step - accuracy: 0.4785 - loss: 0.7051"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 824ms/step - accuracy: 0.4785 - loss: 0.7052 - val_accuracy: 0.5200 - val_loss: 0.6924\n",
      "Epoch 5/5\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 823ms/step - accuracy: 0.5102 - loss: 0.6945 - val_accuracy: 0.4800 - val_loss: 0.6946\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/19 13:16:09 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmp9xabzx03/model, flavor: tensorflow). Fall back to return ['tensorflow==2.18.0', 'cloudpickle==3.1.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 782ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "2024/11/19 13:16:20 INFO mlflow.tracking._tracking_service.client: 🏃 View run Fine-tuning finiteautomata/bertweet-base-sentiment-analysis at: http://mlflow-server:5000/#/experiments/6/runs/94882ee1affe45fc8f2aed9528e7085e.\n",
      "2024/11/19 13:16:20 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://mlflow-server:5000/#/experiments/6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.48\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      1.00      0.65        96\n",
      "           1       0.00      0.00      0.00       104\n",
      "\n",
      "    accuracy                           0.48       200\n",
      "   macro avg       0.24      0.50      0.32       200\n",
      "weighted avg       0.23      0.48      0.31       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import emoji\n",
    "from transformers import AutoTokenizer, TFAutoModel, AutoConfig\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "\n",
    "# Prétraitement des tweets\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    tweet = emoji.demojize(tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'[^a-zA-Z0-9\\s]', '', tweet)\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    return tweet\n",
    "\n",
    "# Nettoyage des tweets\n",
    "# df_sample = df_sample.copy().sample(frac=0.1, random_state=42)  # Utilisez 10% des données\n",
    "# display(df_sample['target'].value_counts())\n",
    "df_sample['cleaned_text'] = df_sample['text'].apply(preprocess_tweet)\n",
    "\n",
    "# Diviser les données en train et test\n",
    "documents = df_sample['cleaned_text']\n",
    "labels = df_sample['target']\n",
    "X_train, X_val, y_train, y_val = train_test_split(documents, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Charger le tokenizer\n",
    "model_name = 'finiteautomata/bertweet-base-sentiment-analysis'\n",
    "#model_name = 'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenisation des données\n",
    "def tokenize_texts(texts, max_length=128):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_texts(X_train, max_length=80)\n",
    "val_encodings = tokenize_texts(X_val, max_length=80)\n",
    "\n",
    "# Préparer les datasets TensorFlow\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    y_train.values\n",
    ")).shuffle(len(X_train)).batch(16)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    y_val.values\n",
    ")).batch(16)\n",
    "\n",
    "# Charger la configuration et le modèle pré-entraîné sans la couche de classification\n",
    "config = AutoConfig.from_pretrained(model_name, output_hidden_states=False)\n",
    "base_model = TFAutoModel.from_pretrained(model_name, config=config)\n",
    "\n",
    "# Geler les 10 premières couches du modèle pour accélérer l'entraînement\n",
    "# for layer in base_model.layers[:-1]:\n",
    "#     layer.trainable = False\n",
    "# base_model.layers[-1].trainable = True\n",
    "\n",
    "\n",
    "# Créer un nouveau modèle Keras\n",
    "input_ids = Input(shape=(80,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = Input(shape=(80,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# Utiliser Lambda pour appeler le modèle pré-entraîné avec output_shape spécifié\n",
    "sequence_output = Lambda(\n",
    "    lambda inputs: base_model(input_ids=inputs[0], attention_mask=inputs[1])[0],\n",
    "    output_shape=(80, 768)\n",
    ")([input_ids, attention_mask])\n",
    "\n",
    "cls_token = Lambda(lambda x: x[:, 0, :], output_shape=(768,))(sequence_output)  # Extraire le token [CLS]\n",
    "\n",
    "# Ajouter une couche dense pour la classification binaire\n",
    "output = Dense(1, activation=\"sigmoid\")(cls_token)\n",
    "\n",
    "# Construire le modèle final\n",
    "model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "# Optimiseur et perte\n",
    "learning_rate = 1e-5\n",
    "optimizer = Adam()\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "metric = tf.keras.metrics.BinaryAccuracy('accuracy')\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# Intégration avec MLflow\n",
    "#mlflow.tensorflow.autolog()\n",
    "\n",
    "with mlflow.start_run(run_name=f\"Fine-tuning {model_name}\", nested=True):\n",
    "    mlflow.set_tag(\"model_name\", model_name)\n",
    "    mlflow.log_param(\"epochs\", 5)\n",
    "    mlflow.log_param(\"batch_size\", 16)\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "\n",
    "    # Désactiver l'autolog de TensorFlow\n",
    "    #mlflow.tensorflow.autolog(disable=True)\n",
    "\n",
    "    # Entraîner le modèle\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=5\n",
    "    )\n",
    "\n",
    "    # Prédictions sur le jeu de validation\n",
    "    val_logits = model.predict(val_dataset)\n",
    "    y_val_pred = (val_logits > 0.5).astype(int)\n",
    "\n",
    "    # Évaluer les performances\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    print(\"Validation Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "    # # Log des métriques finales dans MLflow\n",
    "    # mlflow.log_metric(\"Validation Accuracy\", accuracy)\n",
    "    # mlflow.log_dict(\n",
    "    #     classification_report(y_val, y_val_pred, output_dict=True),\n",
    "    #     \"classification_report.json\"\n",
    "    # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    500\n",
       "1    500\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_sample['target'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annexe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Annexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Charger le tokenizer et le modèle\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de données pour 'documents' et 'labels'\n",
    "documents = df_sample['text']  # Liste de textes à analyser\n",
    "labels = df_sample['target']   # Liste de labels (0 ou 1 pour la classification binaire)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization des données\n",
    "def tokenize_data(documents):\n",
    "    return tokenizer(\n",
    "        documents.tolist(),\n",
    "        max_length=128, padding=True, truncation=True, return_tensors='tf'\n",
    "    )\n",
    "tokens = tokenize_data(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir en NumPy pour train_test_split\n",
    "input_ids_np = tokens['input_ids'].numpy()\n",
    "attention_masks_np = tokens['attention_mask'].numpy()\n",
    "labels_np = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparer les ensembles de données\n",
    "train_input_ids, val_input_ids, train_labels, val_labels = train_test_split(input_ids_np, labels_np, test_size=0.2, random_state=42)\n",
    "train_attention_masks, val_attention_masks = train_test_split(attention_masks_np, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurer la fonction de perte et l'optimiseur\n",
    "loss_fn = SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = Adam(learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir le nombre d'époques et la taille du batch\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "\n",
    "# Définir l'accuracy\n",
    "train_accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "Batch 0 - Loss: 0.7571 - Accuracy: 0.3750\n",
      "Batch 10 - Loss: 0.6762 - Accuracy: 0.5852\n",
      "Batch 20 - Loss: 0.6756 - Accuracy: 0.6012\n",
      "Batch 30 - Loss: 0.7363 - Accuracy: 0.6129\n",
      "Batch 40 - Loss: 0.6564 - Accuracy: 0.6174\n",
      "Epoch 1 - Loss: 0.6640 - Accuracy: 0.6175\n",
      "\n",
      "Epoch 2/5\n",
      "Batch 0 - Loss: 0.6280 - Accuracy: 0.6875\n",
      "Batch 10 - Loss: 0.5860 - Accuracy: 0.7614\n",
      "Batch 20 - Loss: 0.5801 - Accuracy: 0.7381\n",
      "Batch 30 - Loss: 0.5177 - Accuracy: 0.7520\n",
      "Batch 40 - Loss: 0.5065 - Accuracy: 0.7637\n",
      "Epoch 2 - Loss: 0.5328 - Accuracy: 0.7763\n",
      "\n",
      "Epoch 3/5\n",
      "Batch 0 - Loss: 0.3318 - Accuracy: 0.9375\n",
      "Batch 10 - Loss: 0.3797 - Accuracy: 0.8693\n",
      "Batch 20 - Loss: 0.3074 - Accuracy: 0.8690\n",
      "Batch 30 - Loss: 0.2170 - Accuracy: 0.8669\n",
      "Batch 40 - Loss: 0.2157 - Accuracy: 0.8750\n",
      "Epoch 3 - Loss: 0.3613 - Accuracy: 0.8788\n",
      "\n",
      "Epoch 4/5\n",
      "Batch 0 - Loss: 0.1782 - Accuracy: 1.0000\n",
      "Batch 10 - Loss: 0.3427 - Accuracy: 0.9318\n",
      "Batch 20 - Loss: 0.1576 - Accuracy: 0.9405\n",
      "Batch 30 - Loss: 0.1141 - Accuracy: 0.9435\n",
      "Batch 40 - Loss: 0.1427 - Accuracy: 0.9390\n",
      "Epoch 4 - Loss: 0.2268 - Accuracy: 0.9375\n",
      "\n",
      "Epoch 5/5\n",
      "Batch 0 - Loss: 0.1175 - Accuracy: 1.0000\n",
      "Batch 10 - Loss: 0.2146 - Accuracy: 0.9545\n",
      "Batch 20 - Loss: 0.1394 - Accuracy: 0.9524\n",
      "Batch 30 - Loss: 0.1696 - Accuracy: 0.9536\n",
      "Batch 40 - Loss: 0.0650 - Accuracy: 0.9604\n",
      "Epoch 5 - Loss: 0.1478 - Accuracy: 0.9600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/19 13:58:25 INFO mlflow.tracking._tracking_service.client: 🏃 View run BERT at: http://mlflow-server:5000/#/experiments/6/runs/7f74236345d4450d808e78ef5ccde02c.\n",
      "2024/11/19 13:58:25 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://mlflow-server:5000/#/experiments/6.\n"
     ]
    }
   ],
   "source": [
    "# Configuration MLflow\n",
    "experiment = mlflow.set_experiment(\"BERT\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"BERT\", nested=True):\n",
    "    # Enregistrement des hyperparamètres\n",
    "    mlflow.log_param(\"model_name\", \"bert-base-uncased\")\n",
    "    mlflow.log_param(\"num_epochs\", epochs)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"learning_rate\", 1e-5)\n",
    "    \n",
    "    # Boucle d'entraînement\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        train_accuracy_metric.reset_state()  # Réinitialiser l'accuracy pour chaque epoch\n",
    "        epoch_loss = []  # Réinitialiser la liste des pertes pour chaque epoch\n",
    "\n",
    "        for i in range(0, len(train_input_ids), batch_size):\n",
    "            # Obtenir un batch de données\n",
    "            batch_input_ids = train_input_ids[i:i + batch_size]\n",
    "            batch_attention_masks = train_attention_masks[i:i + batch_size]\n",
    "            batch_labels = train_labels[i:i + batch_size]\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                # Faire des prédictions\n",
    "                outputs = model(\n",
    "                    input_ids=batch_input_ids,\n",
    "                    attention_mask=batch_attention_masks,\n",
    "                    training=True\n",
    "                )\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Calculer la perte\n",
    "                loss = loss_fn(batch_labels, logits)\n",
    "            \n",
    "            # Calculer et appliquer les gradients\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "            # Mettre à jour l'accuracy et accumuler la perte\n",
    "            train_accuracy_metric.update_state(batch_labels, logits)\n",
    "            epoch_loss.append(loss.numpy())\n",
    "            \n",
    "            # Afficher la perte et l'accuracy toutes les 10 batches\n",
    "            if i % (batch_size * 10) == 0:\n",
    "                train_accuracy = train_accuracy_metric.result().numpy()\n",
    "                print(f\"Batch {i//batch_size} - Loss: {loss.numpy():.4f} - Accuracy: {train_accuracy:.4f}\")\n",
    "                mlflow.log_metric(\"batch_train_loss\", loss.numpy(), step=i//batch_size)\n",
    "                mlflow.log_metric(\"batch_train_accuracy\", train_accuracy, step=i//batch_size)\n",
    "        \n",
    "        # Enregistrement des métriques de l'époque\n",
    "        epoch_accuracy = train_accuracy_metric.result().numpy()\n",
    "        epoch_loss_avg = np.mean(epoch_loss)\n",
    "        print(f\"Epoch {epoch + 1} - Loss: {epoch_loss_avg:.4f} - Accuracy: {epoch_accuracy:.4f}\")\n",
    "        mlflow.log_metric(\"epoch_train_loss\", epoch_loss_avg, step=epoch)\n",
    "        mlflow.log_metric(\"epoch_train_accuracy\", epoch_accuracy, step=epoch)\n",
    "\n",
    "    # Libérer la mémoire GPU/CPU non utilisée avant l'évaluation\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "def log_plot_to_mlflow(figure, artifact_name):\n",
    "    \"\"\"\n",
    "    Enregistre une figure matplotlib dans MLflow en utilisant un buffer en mémoire.\n",
    "\n",
    "    :param figure: la figure matplotlib à sauvegarder\n",
    "    :param artifact_name: le nom de l'artefact pour MLflow (inclure \".png\")\n",
    "    \"\"\"\n",
    "    # Utilisation d'un buffer en mémoire\n",
    "    buffer = BytesIO()\n",
    "    figure.savefig(buffer, format=\"png\")\n",
    "    buffer.seek(0)\n",
    "    \n",
    "    # Sauvegarder dans MLflow à partir du buffer\n",
    "    with open(artifact_name, \"wb\") as f:\n",
    "        f.write(buffer.getvalue())\n",
    "    mlflow.log_artifact(artifact_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/19 13:58:45 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.11/site-packages/transformers/generation/tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n",
      "2024/11/19 13:58:51 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmph_8ytx4l/model, flavor: keras). Fall back to return ['keras==3.6.0']. Set logging level to DEBUG to see the full traceback. \n",
      "2024/11/19 13:58:51 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Registered model 'bert-base-uncased' already exists. Creating a new version of this model...\n",
      "2024/11/19 13:58:53 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: bert-base-uncased, version 3\n",
      "Created version '3' of model 'bert-base-uncased'.\n"
     ]
    }
   ],
   "source": [
    " # Calcul et stockage des métriques dans un dictionnaire\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc\n",
    "    \n",
    "# Évaluation du modèle sur le set de validation en mini-batches\n",
    "batch_size = 8  # Taille réduite pour l'évaluation\n",
    "num_batches = len(val_input_ids) // batch_size\n",
    "all_predictions = []\n",
    "# Initialisation du dictionnaire des métriques\n",
    "metrics_dict = {}\n",
    "all_probs = []\n",
    "\n",
    "for i in range(num_batches):\n",
    "    # Obtenir un batch de validation\n",
    "    batch_input_ids = val_input_ids[i * batch_size : (i + 1) * batch_size]\n",
    "    batch_attention_masks = val_attention_masks[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "    # Calcul des logits pour le batch\n",
    "    batch_logits = model(\n",
    "        input_ids=batch_input_ids,\n",
    "        attention_mask=batch_attention_masks,\n",
    "        training=False\n",
    "    ).logits\n",
    "\n",
    "    # Stocker les prédictions\n",
    "    batch_predictions = tf.argmax(batch_logits, axis=1)\n",
    "    all_predictions.append(batch_predictions)\n",
    "    \n",
    "     # Convertir logits en probabilités\n",
    "    batch_probabilities = tf.nn.softmax(batch_logits, axis=1)[:, 1]  # Probabilités pour la classe 1\n",
    "    all_probs.append(batch_probabilities)\n",
    "\n",
    "# Concaténer toutes les prédictions\n",
    "all_predictions = tf.concat(all_predictions, axis=0)\n",
    "\n",
    "# Concaténer toutes les probabilités\n",
    "all_probs = tf.concat(all_probs, axis=0).numpy()\n",
    "\n",
    "# Calcul de la courbe ROC\n",
    "fpr, tpr, thresholds = roc_curve(val_labels[:len(all_probs)], all_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Tracer la courbe ROC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random guess\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Bert ROC Curve - Validation Set\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "log_plot_to_mlflow(plt.gcf(), \"roc_curve_val.png\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "# Calcul de l'accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(all_predictions == val_labels[:len(all_predictions)], dtype=tf.float32))\n",
    "print(f\"Validation Accuracy: {accuracy.numpy():.4f}\")\n",
    "\n",
    "validation_metrics = {\n",
    "    \"Validation Accuracy\": round(float(accuracy.numpy()), 3),\n",
    "    \"Validation ROC AUC\": round(roc_auc, 3),\n",
    "    \"Validation Precision\": round(precision_score(val_labels[:len(all_predictions)], all_predictions.numpy()), 3),\n",
    "    \"Validation Recall\": round(recall_score(val_labels[:len(all_predictions)], all_predictions.numpy()), 3),\n",
    "    \"Validation F1\": round(f1_score(val_labels[:len(all_predictions)], all_predictions.numpy()), 3)\n",
    "}\n",
    "\n",
    "metrics_dict.update(validation_metrics)\n",
    "# Log de l'accuracy finale de validation et des autres métriques dans MLflow\n",
    "mlflow.log_metrics(metrics_dict)\n",
    "\n",
    "# Log de l'accuracy finale de validation dans MLflow\n",
    "mlflow.log_metric(\"val_accuracy\", accuracy.numpy())\n",
    "\n",
    "# Enregistrement du modèle\n",
    "mlflow.keras.log_model(model, \"bert_model\")\n",
    "\n",
    "run_id = mlflow.active_run().info.run_id\n",
    "result = mlflow.register_model(\n",
    "    model_uri=f\"runs:/{run_id}/model\",\n",
    "    name=f\"bert-base-uncased\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Validation Accuracy': 0.75,\n",
       " 'Validation ROC AUC': 0.875,\n",
       " 'Validation Precision': 0.821,\n",
       " 'Validation Recall': 0.663,\n",
       " 'Validation F1': 0.734}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(validation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Validation ROC AUC</th>\n",
       "      <th>Validation Precision</th>\n",
       "      <th>Validation Recall</th>\n",
       "      <th>Validation F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Validation Accuracy  Validation ROC AUC  Validation Precision  \\\n",
       "0                 0.75               0.875                 0.821   \n",
       "\n",
       "   Validation Recall  Validation F1  \n",
       "0              0.663          0.734  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(metrics_dict, index=[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
