{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 16:35:32.059903: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732293332.074200  543575 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732293332.078647  543575 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-22 16:35:32.092105: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlflow.set_tracking_uri(\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "Transformers version: 4.46.2\n"
     ]
    }
   ],
   "source": [
    "# Affichage des versions pour v√©rification\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   target  20000 non-null  int64 \n",
      " 1   ids     20000 non-null  int64 \n",
      " 2   date    20000 non-null  object\n",
      " 3   flag    20000 non-null  object\n",
      " 4   user    20000 non-null  object\n",
      " 5   text    20000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 937.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Recharger le DataFrame depuis le fichier pickle\n",
    "df_sample = pd.read_pickle('download/df_sample_20000.pkl')\n",
    "df_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    10000\n",
       "1    10000\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " #V√©rifier si le DataFrame a au moins 20 000 lignes\n",
    "if len(df_sample) != 20000:\n",
    "    raise ValueError(\"Le DataFrame ne contient pas 20 000 lignes.\")\n",
    "\n",
    "display(df_sample['target'].value_counts())\n",
    "df = df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification du Fine-Tuning\n",
    "\n",
    "#### R√©sultats Prometteurs avec un Mod√®le Pr√©entra√Æn√©\n",
    "\n",
    "Lors des exp√©rimentations initiales, le mod√®le pr√©entra√Æn√© **`finiteautomata/bertweet-base-sentiment-analysis`** a montr√© des performances prometteuses sur des t√¢ches similaires d'analyse de sentiments. En effet, ce mod√®le, con√ßu sp√©cifiquement pour traiter des donn√©es provenant de Twitter, est particuli√®rement bien adapt√© pour capturer les nuances linguistiques, les abr√©viations, et le langage informel souvent pr√©sents dans les tweets.\n",
    "\n",
    "Ces performances encourageantes ont motiv√© l'id√©e de pousser davantage l'optimisation en effectuant un **fine-tuning** sur notre corpus sp√©cifique de tweets, dans le but de mieux adapter le mod√®le aux particularit√©s de nos donn√©es et de maximiser la performance sur la t√¢che cibl√©e.\n",
    "\n",
    "En somme, cette approche vise √† exploiter le potentiel d√©j√† d√©montr√© du mod√®le tout en le personnalisant davantage pour r√©pondre aux besoins sp√©cifiques de notre probl√©matique m√©tier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 16:35:44.214349: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "Some layers from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis were not used when initializing TFRobertaModel: ['classifier']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFRobertaModel were not initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis and are newly initialized: ['roberta/pooler/dense/bias:0', 'roberta/pooler/dense/kernel:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.11/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['input_ids', 'attention_mask']. Received: the structure of inputs={'input_ids': '*', 'token_type_ids': '*', 'attention_mask': '*'}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m841s\u001b[0m 833ms/step - accuracy: 0.4907 - loss: 0.7013 - val_accuracy: 0.5048 - val_loss: 0.6975\n",
      "Epoch 2/5\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m783s\u001b[0m 783ms/step - accuracy: 0.5028 - loss: 0.6982 - val_accuracy: 0.5048 - val_loss: 0.6932\n",
      "Epoch 3/5\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m778s\u001b[0m 778ms/step - accuracy: 0.5068 - loss: 0.6993 - val_accuracy: 0.5048 - val_loss: 0.7102\n",
      "Epoch 4/5\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m894s\u001b[0m 870ms/step - accuracy: 0.4992 - loss: 0.6972 - val_accuracy: 0.5048 - val_loss: 0.7068\n",
      "Epoch 5/5\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m833s\u001b[0m 833ms/step - accuracy: 0.4999 - loss: 0.6979 - val_accuracy: 0.5048 - val_loss: 0.6950\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 639ms/step\n",
      "Validation Metrics:\n",
      "Accuracy: 0.505\n",
      "ROC_AUC: 0.501\n",
      "F1: 0.0\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "Predict Time: 160.789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Registered model 'Fine-tuned finiteautomata/bertweet-base-sentiment-analysis' already exists. Creating a new version of this model...\n",
      "2024/11/22 17:47:15 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Fine-tuned finiteautomata/bertweet-base-sentiment-analysis, version 6\n",
      "Created version '6' of model 'Fine-tuned finiteautomata/bertweet-base-sentiment-analysis'.\n",
      "2024/11/22 17:47:15 INFO mlflow.tracking._tracking_service.client: üèÉ View run Fine-tuning finiteautomata/bertweet-base-sentiment-analysis at: http://mlflow-server:5000/#/experiments/6/runs/704f7d5b38a14981a3441ce7fbe30595.\n",
      "2024/11/22 17:47:15 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://mlflow-server:5000/#/experiments/6.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import emoji\n",
    "from transformers import AutoTokenizer, TFAutoModel, AutoConfig\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, f1_score, precision_score, recall_score\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import time\n",
    "\n",
    "# Pr√©traitement des tweets\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    tweet = emoji.demojize(tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'[^a-zA-Z0-9\\s]', '', tweet)\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    return tweet\n",
    "\n",
    "# Nettoyage des tweets\n",
    "df_sample['cleaned_text'] = df_sample['text'].apply(preprocess_tweet)\n",
    "\n",
    "# Diviser les donn√©es en train et test\n",
    "documents = df_sample['cleaned_text']\n",
    "labels = df_sample['target']\n",
    "X_train, X_val, y_train, y_val = train_test_split(documents, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Charger le tokenizer\n",
    "model_name = 'finiteautomata/bertweet-base-sentiment-analysis'\n",
    "#model_name = 'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenisation des donn√©es\n",
    "def tokenize_texts(texts, max_length=128):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_texts(X_train, max_length=80)\n",
    "val_encodings = tokenize_texts(X_val, max_length=80)\n",
    "\n",
    "# Pr√©parer les datasets TensorFlow\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    y_train.values\n",
    ")).shuffle(len(X_train)).batch(16)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    y_val.values\n",
    ")).batch(16)\n",
    "\n",
    "# Charger la configuration et le mod√®le pr√©-entra√Æn√© sans la couche de classification\n",
    "config = AutoConfig.from_pretrained(model_name, output_hidden_states=False)\n",
    "base_model = TFAutoModel.from_pretrained(model_name, config=config)\n",
    "\n",
    "# D√©g√®le de la dernie√®re couche du mod√®le pour acc√©l√©rer l'entra√Ænement\n",
    "for layer in base_model.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "# Cr√©er un nouveau mod√®le Keras\n",
    "input_ids = Input(shape=(80,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = Input(shape=(80,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# Utiliser Lambda pour appeler le mod√®le pr√©-entra√Æn√© avec output_shape sp√©cifi√©\n",
    "sequence_output = Lambda(\n",
    "    lambda inputs: base_model(input_ids=inputs[0], attention_mask=inputs[1])[0],\n",
    "    output_shape=(80, 768)\n",
    ")([input_ids, attention_mask])\n",
    "\n",
    "cls_token = Lambda(lambda x: x[:, 0, :], output_shape=(768,))(sequence_output)  # Extraire le token [CLS]\n",
    "\n",
    "# Ajouter une couche dense pour la classification binaire\n",
    "output = Dense(1, activation=\"sigmoid\")(cls_token)\n",
    "\n",
    "# Construire le mod√®le final\n",
    "model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "# Optimiseur et perte\n",
    "learning_rate = 1e-5\n",
    "optimizer = Adam()\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "metric = tf.keras.metrics.BinaryAccuracy('accuracy')\n",
    "\n",
    "# Compiler le mod√®le\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# Int√©gration avec MLflow\n",
    "#mlflow.tensorflow.autolog()\n",
    "# Configuration MLflow\n",
    "experiment = mlflow.set_experiment(\"BERT\")\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=f\"Fine-tuning {model_name}\",  nested=True) as run:\n",
    "    mlflow.set_tag(\"model_name\", model_name)\n",
    "    mlflow.log_param(\"epochs\", 5)\n",
    "    mlflow.log_param(\"batch_size\", 16)\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "\n",
    "    # D√©sactiver l'autolog de TensorFlow\n",
    "    #mlflow.tensorflow.autolog(disable=True)\n",
    "    all_validation_metrics = []\n",
    "    # Entra√Æner le mod√®le\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=5\n",
    "    )\n",
    "\n",
    "    # Pr√©dictions sur le jeu de validation\n",
    "    start_time = time.time()\n",
    "    val_logits = model.predict(val_dataset)\n",
    "    predict_time = time.time() - start_time\n",
    "    \n",
    "    # Convertir les probabilit√©s en classes binaires\n",
    "    y_val_prob = val_logits.flatten()  # Pour roc_auc_score, garder les probabilit√©s\n",
    "    y_val_pred = (y_val_prob > 0.5).astype(int) \n",
    "\n",
    "    # Calcul des m√©triques\n",
    "    validation_metrics = {\n",
    "        \"Accuracy\": round(accuracy_score(y_val, y_val_pred), 3),\n",
    "        \"ROC_AUC\": round(roc_auc_score(y_val, y_val_prob), 3),\n",
    "        \"F1\": round(f1_score(y_val, y_val_pred), 3),\n",
    "        \"Precision\": round(precision_score(y_val, y_val_pred), 3),\n",
    "        \"Recall\": round(recall_score(y_val, y_val_pred), 3),\n",
    "        \"Predict Time\": round(predict_time, 3)\n",
    "    }\n",
    "\n",
    "    # Affichage des m√©triques\n",
    "    print(\"Validation Metrics:\")\n",
    "    for metric_name, metric_value in validation_metrics.items():\n",
    "        print(f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "    # Log des m√©triques dans MLflow\n",
    "    for metric_name, metric_value in validation_metrics.items():\n",
    "        mlflow.log_metric(metric_name, metric_value)\n",
    "\n",
    "    mlflow.log_table(validation_metrics, \"3-BERT-ft.json\")\n",
    "    # # Log des m√©triques finales dans MLflow\n",
    "    # mlflow.log_metric(\"Validation Accuracy\", accuracy)\n",
    "    # mlflow.log_dict(\n",
    "    #     classification_report(y_val, y_val_pred, output_dict=True),\n",
    "    #     \"classification_report.json\"\n",
    "    # )\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    result = mlflow.register_model(\n",
    "        model_uri=f\"runs:/{run_id}/model\",\n",
    "        name=f\"Fine-tuned {model_name}\"\n",
    "    )\n",
    "    \n",
    "    # Construire le lien MLflow correspondant (en supposant que vous avez l'URL de votre serveur MLflow)\n",
    "    active_run = mlflow.active_run()\n",
    "    run_id = active_run.info.run_id\n",
    "    #run_name = active_run.data.tags.get(\"mlflow.runName\")  # Obtenir le nom du run depuis les tags\n",
    "\n",
    "    # R√©cup√©rer l'ID de l'exp√©rience active\n",
    "    experiment_id = active_run.info.experiment_id\n",
    "\n",
    "    # D√©finir l'URL de votre serveur MLflow\n",
    "    mlflow_server_url = \"http://localhost:5000\"  # Remplacez par l'URL r√©el de votre serveur MLflow\n",
    "\n",
    "    # Construire le lien complet vers le run dans l'interface MLflow\n",
    "    run_link = f\"{mlflow_server_url}/#/experiments/{experiment_id}/runs/{run_id}\"\n",
    "    \n",
    "        # Exemple d'ajout d'un dictionnaire pour chaque configuration\n",
    "    # Ajouter les m√©triques de validation √† la liste\n",
    "    all_validation_metrics.append({\n",
    "        \"run_name\": f\"{model_name} - Fine-tuned\",\n",
    "        **validation_metrics,\n",
    "        \"run_id\": run_link\n",
    "    })\n",
    "    mlflow.log_table(pd.DataFrame(all_validation_metrics), \"3-ft-finiteautomata.json\")\n",
    "    \n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion sur le Fine-Tuning\n",
    "\n",
    "Apr√®s avoir tent√© de fine-tuner le mod√®le **`finiteautomata/bertweet-base-sentiment-analysis`**, les r√©sultats obtenus n'ont pas permis d'am√©liorer les performances initiales du mod√®le pr√©-entra√Æn√©. En effet, les m√©triques telles que l'**accuracy**, le **F1-score**, la **pr√©cision**, et le **rappel** se sont significativement d√©grad√©es apr√®s le fine-tuning.\n",
    "\n",
    "Ces r√©sultats sugg√®rent que le mod√®le pr√©-entra√Æn√© est d√©j√† tr√®s bien ajust√© pour la t√¢che g√©n√©rale d'analyse de sentiments sur des tweets, et qu'il capture efficacement les caract√©ristiques linguistiques propres √† ce type de donn√©es. Toutefois, le fine-tuning n'a pas permis d'exploiter pleinement les sp√©cificit√©s de notre corpus.\n",
    "\n",
    "#### Perspectives d'Am√©lioration\n",
    "\n",
    "Pour aller au-del√† des performances actuelles, une √©tude plus approfondie serait n√©cessaire, incluant :\n",
    "- Une analyse d√©taill√©e des **donn√©es d'entra√Ænement**, notamment leur qualit√© et leur distribution.\n",
    "- Une exploration syst√©matique des **hyperparam√®tres** (learning rate, batch size, nombre de couches d√©g√©l√©es, etc.).\n",
    "- L'int√©gration de techniques avanc√©es comme l'**augmentation de donn√©es** pour enrichir le corpus.\n",
    "- L'utilisation d'approches comme l'**apprentissage multit√¢che** ou le **transfer learning sur des t√¢ches adjacentes** pour tirer parti des donn√©es disponibles.\n",
    "\n",
    "En r√©sum√©, bien que le mod√®le pr√©-entra√Æn√© reste performant et adapt√© √† notre cas d'usage, il serait pertinent d'approfondir les analyses et les ajustements pour maximiser son potentiel dans un contexte sp√©cifique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    10000\n",
       "1    10000\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_sample['target'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement mod√®le BERT\n",
    "### Exp√©rimentation avec `bert-base-uncased`\n",
    "\n",
    "Dans le cadre de ce projet, j'ai choisi d'entra√Æner le mod√®le **`bert-base-uncased`** sur mon propre jeu de donn√©es. Cette exp√©rimentation visait √† √©valuer si un mod√®le g√©n√©raliste pr√©-entra√Æn√© sur un large corpus de textes en anglais pouvait mieux performer que des mod√®les sp√©cialis√©s comme **`finiteautomata/bertweet-base-sentiment-analysis`**, qui est sp√©cifiquement con√ßu pour des donn√©es issues de Twitter.\n",
    "\n",
    "L'objectif √©tait de partir d'un mod√®le robuste et polyvalent, mais sans pr√©jug√©s sp√©cifiques li√©s au domaine ou au type de donn√©es (comme les tweets), pour voir s'il pourrait mieux s'adapter √† mon corpus gr√¢ce √† un entra√Ænement complet. Cette d√©marche a permis de comparer les r√©sultats entre un fine-tuning sur un mod√®le sp√©cialis√© et un entra√Ænement complet sur un mod√®le g√©n√©raliste, tout en testant leur capacit√© respective √† capturer les nuances propres √† mon jeu de donn√©es.\n",
    "\n",
    "Les r√©sultats obtenus avec **`bert-base-uncased`** serviront √©galement de point de r√©f√©rence pour juger de l'impact du choix du mod√®le initial et de la qualit√© de l'entra√Ænement r√©alis√©.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Charger le tokenizer et le mod√®le\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de donn√©es pour 'documents' et 'labels'\n",
    "documents = df_sample['text']  # Liste de textes √† analyser\n",
    "labels = df_sample['target']   # Liste de labels (0 ou 1 pour la classification binaire)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization des donn√©es\n",
    "def tokenize_data(documents):\n",
    "    return tokenizer(\n",
    "        documents.tolist(),\n",
    "        max_length=128, padding=True, truncation=True, return_tensors='tf'\n",
    "    )\n",
    "tokens = tokenize_data(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir en NumPy pour train_test_split\n",
    "input_ids_np = tokens['input_ids'].numpy()\n",
    "attention_masks_np = tokens['attention_mask'].numpy()\n",
    "labels_np = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©parer les ensembles de donn√©es\n",
    "train_input_ids, val_input_ids, train_labels, val_labels = train_test_split(input_ids_np, labels_np, test_size=0.2, random_state=42)\n",
    "train_attention_masks, val_attention_masks = train_test_split(attention_masks_np, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurer la fonction de perte et l'optimiseur\n",
    "loss_fn = SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = Adam(learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir le nombre d'√©poques et la taille du batch\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "\n",
    "# D√©finir l'accuracy\n",
    "train_accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "Batch 0 - Loss: 0.6211 - Accuracy: 0.7500\n",
      "Batch 10 - Loss: 0.6312 - Accuracy: 0.5682\n",
      "Batch 20 - Loss: 0.6607 - Accuracy: 0.5952\n",
      "Batch 30 - Loss: 0.6653 - Accuracy: 0.6008\n",
      "Batch 40 - Loss: 0.5985 - Accuracy: 0.6113\n",
      "Batch 50 - Loss: 0.7243 - Accuracy: 0.6140\n",
      "Batch 60 - Loss: 0.5404 - Accuracy: 0.6168\n",
      "Batch 70 - Loss: 0.5786 - Accuracy: 0.6276\n",
      "Batch 80 - Loss: 0.5692 - Accuracy: 0.6427\n",
      "Batch 90 - Loss: 0.5841 - Accuracy: 0.6449\n",
      "Batch 100 - Loss: 0.5579 - Accuracy: 0.6498\n",
      "Batch 110 - Loss: 0.6012 - Accuracy: 0.6548\n",
      "Batch 120 - Loss: 0.6841 - Accuracy: 0.6643\n",
      "Batch 130 - Loss: 0.5828 - Accuracy: 0.6751\n",
      "Batch 140 - Loss: 0.3950 - Accuracy: 0.6826\n",
      "Batch 150 - Loss: 0.6086 - Accuracy: 0.6858\n",
      "Batch 160 - Loss: 0.3317 - Accuracy: 0.6949\n",
      "Batch 170 - Loss: 0.4097 - Accuracy: 0.7036\n",
      "Batch 180 - Loss: 0.4473 - Accuracy: 0.7082\n",
      "Batch 190 - Loss: 0.6451 - Accuracy: 0.7114\n",
      "Batch 200 - Loss: 0.5220 - Accuracy: 0.7170\n",
      "Batch 210 - Loss: 0.4565 - Accuracy: 0.7207\n",
      "Batch 220 - Loss: 0.2578 - Accuracy: 0.7223\n",
      "Batch 230 - Loss: 0.4558 - Accuracy: 0.7267\n",
      "Batch 240 - Loss: 0.6790 - Accuracy: 0.7280\n",
      "Batch 250 - Loss: 0.5306 - Accuracy: 0.7308\n",
      "Batch 260 - Loss: 0.4908 - Accuracy: 0.7316\n",
      "Batch 270 - Loss: 0.4003 - Accuracy: 0.7327\n",
      "Batch 280 - Loss: 0.3969 - Accuracy: 0.7353\n",
      "Batch 290 - Loss: 0.2646 - Accuracy: 0.7399\n",
      "Batch 300 - Loss: 0.5494 - Accuracy: 0.7411\n",
      "Batch 310 - Loss: 0.3747 - Accuracy: 0.7438\n",
      "Batch 320 - Loss: 0.4696 - Accuracy: 0.7457\n",
      "Batch 330 - Loss: 0.5908 - Accuracy: 0.7474\n",
      "Batch 340 - Loss: 0.3948 - Accuracy: 0.7489\n",
      "Batch 350 - Loss: 0.3132 - Accuracy: 0.7511\n",
      "Batch 360 - Loss: 0.3950 - Accuracy: 0.7536\n",
      "Batch 370 - Loss: 0.4074 - Accuracy: 0.7542\n",
      "Batch 380 - Loss: 0.5388 - Accuracy: 0.7539\n",
      "Batch 390 - Loss: 0.1801 - Accuracy: 0.7562\n",
      "Batch 400 - Loss: 0.4253 - Accuracy: 0.7573\n",
      "Batch 410 - Loss: 0.4744 - Accuracy: 0.7581\n",
      "Batch 420 - Loss: 0.5592 - Accuracy: 0.7580\n",
      "Batch 430 - Loss: 0.4693 - Accuracy: 0.7580\n",
      "Batch 440 - Loss: 0.3796 - Accuracy: 0.7589\n",
      "Batch 450 - Loss: 0.4387 - Accuracy: 0.7580\n",
      "Batch 460 - Loss: 0.3422 - Accuracy: 0.7587\n",
      "Batch 470 - Loss: 0.5395 - Accuracy: 0.7588\n",
      "Batch 480 - Loss: 0.4830 - Accuracy: 0.7583\n",
      "Batch 490 - Loss: 0.3336 - Accuracy: 0.7590\n",
      "Batch 500 - Loss: 0.4606 - Accuracy: 0.7606\n",
      "Batch 510 - Loss: 0.6817 - Accuracy: 0.7614\n",
      "Batch 520 - Loss: 0.3601 - Accuracy: 0.7621\n",
      "Batch 530 - Loss: 0.6716 - Accuracy: 0.7615\n",
      "Batch 540 - Loss: 0.5961 - Accuracy: 0.7618\n",
      "Batch 550 - Loss: 0.4559 - Accuracy: 0.7621\n",
      "Batch 560 - Loss: 0.5435 - Accuracy: 0.7627\n",
      "Batch 570 - Loss: 0.3833 - Accuracy: 0.7632\n",
      "Batch 580 - Loss: 0.3997 - Accuracy: 0.7629\n",
      "Batch 590 - Loss: 0.3363 - Accuracy: 0.7637\n",
      "Batch 600 - Loss: 0.3018 - Accuracy: 0.7650\n",
      "Batch 610 - Loss: 0.2754 - Accuracy: 0.7658\n",
      "Batch 620 - Loss: 0.2202 - Accuracy: 0.7671\n",
      "Batch 630 - Loss: 0.1909 - Accuracy: 0.7676\n",
      "Batch 640 - Loss: 0.5474 - Accuracy: 0.7671\n",
      "Batch 650 - Loss: 0.5081 - Accuracy: 0.7680\n",
      "Batch 660 - Loss: 0.5078 - Accuracy: 0.7677\n",
      "Batch 670 - Loss: 0.2394 - Accuracy: 0.7689\n",
      "Batch 680 - Loss: 0.4352 - Accuracy: 0.7692\n",
      "Batch 690 - Loss: 0.4086 - Accuracy: 0.7696\n",
      "Batch 700 - Loss: 0.3861 - Accuracy: 0.7701\n",
      "Batch 710 - Loss: 0.4005 - Accuracy: 0.7713\n",
      "Batch 720 - Loss: 0.4830 - Accuracy: 0.7723\n",
      "Batch 730 - Loss: 0.3468 - Accuracy: 0.7733\n",
      "Batch 740 - Loss: 0.5377 - Accuracy: 0.7740\n",
      "Batch 750 - Loss: 0.3019 - Accuracy: 0.7740\n",
      "Batch 760 - Loss: 0.4014 - Accuracy: 0.7739\n",
      "Batch 770 - Loss: 0.4679 - Accuracy: 0.7750\n",
      "Batch 780 - Loss: 0.2887 - Accuracy: 0.7755\n",
      "Batch 790 - Loss: 0.4490 - Accuracy: 0.7758\n",
      "Batch 800 - Loss: 0.5663 - Accuracy: 0.7758\n",
      "Batch 810 - Loss: 0.3139 - Accuracy: 0.7770\n",
      "Batch 820 - Loss: 0.5074 - Accuracy: 0.7768\n",
      "Batch 830 - Loss: 0.6005 - Accuracy: 0.7768\n",
      "Batch 840 - Loss: 0.3467 - Accuracy: 0.7771\n",
      "Batch 850 - Loss: 0.3193 - Accuracy: 0.7779\n",
      "Batch 860 - Loss: 0.3088 - Accuracy: 0.7777\n",
      "Batch 870 - Loss: 0.4501 - Accuracy: 0.7781\n",
      "Batch 880 - Loss: 0.3583 - Accuracy: 0.7784\n",
      "Batch 890 - Loss: 0.2459 - Accuracy: 0.7783\n",
      "Batch 900 - Loss: 0.4801 - Accuracy: 0.7782\n",
      "Batch 910 - Loss: 0.3759 - Accuracy: 0.7789\n",
      "Batch 920 - Loss: 0.9384 - Accuracy: 0.7788\n",
      "Batch 930 - Loss: 0.2862 - Accuracy: 0.7789\n",
      "Batch 940 - Loss: 0.3219 - Accuracy: 0.7794\n",
      "Batch 950 - Loss: 0.6951 - Accuracy: 0.7798\n",
      "Batch 960 - Loss: 0.6835 - Accuracy: 0.7801\n",
      "Batch 970 - Loss: 0.5178 - Accuracy: 0.7803\n",
      "Batch 980 - Loss: 0.2332 - Accuracy: 0.7805\n",
      "Batch 990 - Loss: 0.5957 - Accuracy: 0.7805\n",
      "Epoch 1 - Loss: 0.4631 - Accuracy: 0.7809\n",
      "\n",
      "Epoch 2/5\n",
      "Batch 0 - Loss: 0.3278 - Accuracy: 0.8125\n",
      "Batch 10 - Loss: 0.4510 - Accuracy: 0.8466\n",
      "Batch 20 - Loss: 0.3355 - Accuracy: 0.8363\n",
      "Batch 30 - Loss: 0.5596 - Accuracy: 0.8327\n",
      "Batch 40 - Loss: 0.4946 - Accuracy: 0.8155\n",
      "Batch 50 - Loss: 0.2870 - Accuracy: 0.8248\n",
      "Batch 60 - Loss: 0.4940 - Accuracy: 0.8207\n",
      "Batch 70 - Loss: 0.5101 - Accuracy: 0.8213\n",
      "Batch 80 - Loss: 0.2435 - Accuracy: 0.8241\n",
      "Batch 90 - Loss: 0.4364 - Accuracy: 0.8242\n",
      "Batch 100 - Loss: 0.3450 - Accuracy: 0.8292\n",
      "Batch 110 - Loss: 0.5083 - Accuracy: 0.8283\n",
      "Batch 120 - Loss: 0.3597 - Accuracy: 0.8326\n",
      "Batch 130 - Loss: 0.5219 - Accuracy: 0.8340\n",
      "Batch 140 - Loss: 0.3524 - Accuracy: 0.8342\n",
      "Batch 150 - Loss: 0.3653 - Accuracy: 0.8320\n",
      "Batch 160 - Loss: 0.2179 - Accuracy: 0.8373\n",
      "Batch 170 - Loss: 0.2240 - Accuracy: 0.8406\n",
      "Batch 180 - Loss: 0.2118 - Accuracy: 0.8436\n",
      "Batch 190 - Loss: 0.5342 - Accuracy: 0.8452\n",
      "Batch 200 - Loss: 0.2897 - Accuracy: 0.8476\n",
      "Batch 210 - Loss: 0.3355 - Accuracy: 0.8463\n",
      "Batch 220 - Loss: 0.1956 - Accuracy: 0.8467\n",
      "Batch 230 - Loss: 0.4840 - Accuracy: 0.8469\n",
      "Batch 240 - Loss: 0.2427 - Accuracy: 0.8467\n",
      "Batch 250 - Loss: 0.1714 - Accuracy: 0.8489\n",
      "Batch 260 - Loss: 0.3323 - Accuracy: 0.8479\n",
      "Batch 270 - Loss: 0.3422 - Accuracy: 0.8464\n",
      "Batch 280 - Loss: 0.2545 - Accuracy: 0.8494\n",
      "Batch 290 - Loss: 0.1569 - Accuracy: 0.8507\n",
      "Batch 300 - Loss: 0.3916 - Accuracy: 0.8495\n",
      "Batch 310 - Loss: 0.2027 - Accuracy: 0.8513\n",
      "Batch 320 - Loss: 0.2593 - Accuracy: 0.8512\n",
      "Batch 330 - Loss: 0.2604 - Accuracy: 0.8525\n",
      "Batch 340 - Loss: 0.2455 - Accuracy: 0.8528\n",
      "Batch 350 - Loss: 0.1424 - Accuracy: 0.8538\n",
      "Batch 360 - Loss: 0.2916 - Accuracy: 0.8547\n",
      "Batch 370 - Loss: 0.2974 - Accuracy: 0.8553\n",
      "Batch 380 - Loss: 0.3411 - Accuracy: 0.8545\n",
      "Batch 390 - Loss: 0.1568 - Accuracy: 0.8558\n",
      "Batch 400 - Loss: 0.2926 - Accuracy: 0.8566\n",
      "Batch 410 - Loss: 0.2984 - Accuracy: 0.8566\n",
      "Batch 420 - Loss: 0.5149 - Accuracy: 0.8572\n",
      "Batch 430 - Loss: 0.3775 - Accuracy: 0.8573\n",
      "Batch 440 - Loss: 0.1313 - Accuracy: 0.8574\n",
      "Batch 450 - Loss: 0.2533 - Accuracy: 0.8567\n",
      "Batch 460 - Loss: 0.2205 - Accuracy: 0.8571\n",
      "Batch 470 - Loss: 0.4342 - Accuracy: 0.8571\n",
      "Batch 480 - Loss: 0.2751 - Accuracy: 0.8564\n",
      "Batch 490 - Loss: 0.2227 - Accuracy: 0.8565\n",
      "Batch 500 - Loss: 0.3250 - Accuracy: 0.8568\n",
      "Batch 510 - Loss: 0.5299 - Accuracy: 0.8568\n",
      "Batch 520 - Loss: 0.1999 - Accuracy: 0.8568\n",
      "Batch 530 - Loss: 0.3109 - Accuracy: 0.8559\n",
      "Batch 540 - Loss: 0.4321 - Accuracy: 0.8566\n",
      "Batch 550 - Loss: 0.3846 - Accuracy: 0.8567\n",
      "Batch 560 - Loss: 0.2901 - Accuracy: 0.8571\n",
      "Batch 570 - Loss: 0.2981 - Accuracy: 0.8573\n",
      "Batch 580 - Loss: 0.2749 - Accuracy: 0.8567\n",
      "Batch 590 - Loss: 0.1694 - Accuracy: 0.8572\n",
      "Batch 600 - Loss: 0.1918 - Accuracy: 0.8576\n",
      "Batch 610 - Loss: 0.1903 - Accuracy: 0.8580\n",
      "Batch 620 - Loss: 0.0867 - Accuracy: 0.8588\n",
      "Batch 630 - Loss: 0.1769 - Accuracy: 0.8586\n",
      "Batch 640 - Loss: 0.3289 - Accuracy: 0.8583\n",
      "Batch 650 - Loss: 0.3523 - Accuracy: 0.8586\n",
      "Batch 660 - Loss: 0.3583 - Accuracy: 0.8581\n",
      "Batch 670 - Loss: 0.1661 - Accuracy: 0.8584\n",
      "Batch 680 - Loss: 0.2787 - Accuracy: 0.8585\n",
      "Batch 690 - Loss: 0.3480 - Accuracy: 0.8586\n",
      "Batch 700 - Loss: 0.1864 - Accuracy: 0.8590\n",
      "Batch 710 - Loss: 0.3335 - Accuracy: 0.8592\n",
      "Batch 720 - Loss: 0.3030 - Accuracy: 0.8599\n",
      "Batch 730 - Loss: 0.3109 - Accuracy: 0.8602\n",
      "Batch 740 - Loss: 0.6148 - Accuracy: 0.8607\n",
      "Batch 750 - Loss: 0.1196 - Accuracy: 0.8611\n",
      "Batch 760 - Loss: 0.2850 - Accuracy: 0.8613\n",
      "Batch 770 - Loss: 0.2353 - Accuracy: 0.8621\n",
      "Batch 780 - Loss: 0.2455 - Accuracy: 0.8623\n",
      "Batch 790 - Loss: 0.3680 - Accuracy: 0.8624\n",
      "Batch 800 - Loss: 0.4548 - Accuracy: 0.8620\n",
      "Batch 810 - Loss: 0.2697 - Accuracy: 0.8625\n",
      "Batch 820 - Loss: 0.3578 - Accuracy: 0.8621\n",
      "Batch 830 - Loss: 0.5823 - Accuracy: 0.8619\n",
      "Batch 840 - Loss: 0.2112 - Accuracy: 0.8618\n",
      "Batch 850 - Loss: 0.2075 - Accuracy: 0.8620\n",
      "Batch 860 - Loss: 0.2671 - Accuracy: 0.8616\n",
      "Batch 870 - Loss: 0.3987 - Accuracy: 0.8620\n",
      "Batch 880 - Loss: 0.3152 - Accuracy: 0.8622\n",
      "Batch 890 - Loss: 0.1133 - Accuracy: 0.8620\n",
      "Batch 900 - Loss: 0.4848 - Accuracy: 0.8618\n",
      "Batch 910 - Loss: 0.2560 - Accuracy: 0.8624\n",
      "Batch 920 - Loss: 0.7685 - Accuracy: 0.8623\n",
      "Batch 930 - Loss: 0.1948 - Accuracy: 0.8619\n",
      "Batch 940 - Loss: 0.2252 - Accuracy: 0.8624\n",
      "Batch 950 - Loss: 0.6326 - Accuracy: 0.8628\n",
      "Batch 960 - Loss: 0.4421 - Accuracy: 0.8628\n",
      "Batch 970 - Loss: 0.4038 - Accuracy: 0.8627\n",
      "Batch 980 - Loss: 0.2879 - Accuracy: 0.8629\n",
      "Batch 990 - Loss: 0.3073 - Accuracy: 0.8630\n",
      "Epoch 2 - Loss: 0.3277 - Accuracy: 0.8633\n",
      "\n",
      "Epoch 3/5\n",
      "Batch 0 - Loss: 0.2477 - Accuracy: 0.8750\n",
      "Batch 10 - Loss: 0.2493 - Accuracy: 0.8750\n",
      "Batch 20 - Loss: 0.1263 - Accuracy: 0.8958\n",
      "Batch 30 - Loss: 0.3373 - Accuracy: 0.8931\n",
      "Batch 40 - Loss: 0.4321 - Accuracy: 0.8765\n",
      "Batch 50 - Loss: 0.1668 - Accuracy: 0.8860\n",
      "Batch 60 - Loss: 0.2587 - Accuracy: 0.8822\n",
      "Batch 70 - Loss: 0.3688 - Accuracy: 0.8847\n",
      "Batch 80 - Loss: 0.0948 - Accuracy: 0.8843\n",
      "Batch 90 - Loss: 0.3429 - Accuracy: 0.8819\n",
      "Batch 100 - Loss: 0.2396 - Accuracy: 0.8806\n",
      "Batch 110 - Loss: 0.3010 - Accuracy: 0.8818\n",
      "Batch 120 - Loss: 0.1422 - Accuracy: 0.8822\n",
      "Batch 130 - Loss: 0.3732 - Accuracy: 0.8841\n",
      "Batch 140 - Loss: 0.1318 - Accuracy: 0.8870\n",
      "Batch 150 - Loss: 0.2920 - Accuracy: 0.8878\n",
      "Batch 160 - Loss: 0.1725 - Accuracy: 0.8909\n",
      "Batch 170 - Loss: 0.1193 - Accuracy: 0.8933\n",
      "Batch 180 - Loss: 0.1963 - Accuracy: 0.8954\n",
      "Batch 190 - Loss: 0.4240 - Accuracy: 0.8979\n",
      "Batch 200 - Loss: 0.2208 - Accuracy: 0.9002\n",
      "Batch 210 - Loss: 0.2070 - Accuracy: 0.8981\n",
      "Batch 220 - Loss: 0.1979 - Accuracy: 0.8982\n",
      "Batch 230 - Loss: 0.4223 - Accuracy: 0.8994\n",
      "Batch 240 - Loss: 0.0697 - Accuracy: 0.9002\n",
      "Batch 250 - Loss: 0.1259 - Accuracy: 0.9011\n",
      "Batch 260 - Loss: 0.2685 - Accuracy: 0.9011\n",
      "Batch 270 - Loss: 0.1187 - Accuracy: 0.8999\n",
      "Batch 280 - Loss: 0.1566 - Accuracy: 0.9010\n",
      "Batch 290 - Loss: 0.0968 - Accuracy: 0.9018\n",
      "Batch 300 - Loss: 0.2035 - Accuracy: 0.9022\n",
      "Batch 310 - Loss: 0.0734 - Accuracy: 0.9025\n",
      "Batch 320 - Loss: 0.3235 - Accuracy: 0.9023\n",
      "Batch 330 - Loss: 0.1015 - Accuracy: 0.9029\n",
      "Batch 340 - Loss: 0.1672 - Accuracy: 0.9036\n",
      "Batch 350 - Loss: 0.0565 - Accuracy: 0.9042\n",
      "Batch 360 - Loss: 0.2081 - Accuracy: 0.9050\n",
      "Batch 370 - Loss: 0.1269 - Accuracy: 0.9060\n",
      "Batch 380 - Loss: 0.3610 - Accuracy: 0.9055\n",
      "Batch 390 - Loss: 0.0702 - Accuracy: 0.9065\n",
      "Batch 400 - Loss: 0.1940 - Accuracy: 0.9066\n",
      "Batch 410 - Loss: 0.1613 - Accuracy: 0.9071\n",
      "Batch 420 - Loss: 0.4344 - Accuracy: 0.9066\n",
      "Batch 430 - Loss: 0.3257 - Accuracy: 0.9068\n",
      "Batch 440 - Loss: 0.1005 - Accuracy: 0.9067\n",
      "Batch 450 - Loss: 0.1189 - Accuracy: 0.9073\n",
      "Batch 460 - Loss: 0.0575 - Accuracy: 0.9071\n",
      "Batch 470 - Loss: 0.2266 - Accuracy: 0.9075\n",
      "Batch 480 - Loss: 0.1611 - Accuracy: 0.9080\n",
      "Batch 490 - Loss: 0.1013 - Accuracy: 0.9084\n",
      "Batch 500 - Loss: 0.2899 - Accuracy: 0.9084\n",
      "Batch 510 - Loss: 0.3961 - Accuracy: 0.9083\n",
      "Batch 520 - Loss: 0.1282 - Accuracy: 0.9080\n",
      "Batch 530 - Loss: 0.2806 - Accuracy: 0.9067\n",
      "Batch 540 - Loss: 0.3272 - Accuracy: 0.9064\n",
      "Batch 550 - Loss: 0.2741 - Accuracy: 0.9064\n",
      "Batch 560 - Loss: 0.1573 - Accuracy: 0.9062\n",
      "Batch 570 - Loss: 0.2745 - Accuracy: 0.9070\n",
      "Batch 580 - Loss: 0.0798 - Accuracy: 0.9066\n",
      "Batch 590 - Loss: 0.1166 - Accuracy: 0.9074\n",
      "Batch 600 - Loss: 0.2186 - Accuracy: 0.9073\n",
      "Batch 610 - Loss: 0.1831 - Accuracy: 0.9078\n",
      "Batch 620 - Loss: 0.0778 - Accuracy: 0.9086\n",
      "Batch 630 - Loss: 0.1357 - Accuracy: 0.9086\n",
      "Batch 640 - Loss: 0.3358 - Accuracy: 0.9083\n",
      "Batch 650 - Loss: 0.1789 - Accuracy: 0.9084\n",
      "Batch 660 - Loss: 0.1828 - Accuracy: 0.9081\n",
      "Batch 670 - Loss: 0.0556 - Accuracy: 0.9090\n",
      "Batch 680 - Loss: 0.3115 - Accuracy: 0.9094\n",
      "Batch 690 - Loss: 0.0770 - Accuracy: 0.9101\n",
      "Batch 700 - Loss: 0.0707 - Accuracy: 0.9102\n",
      "Batch 710 - Loss: 0.1631 - Accuracy: 0.9106\n",
      "Batch 720 - Loss: 0.2616 - Accuracy: 0.9113\n",
      "Batch 730 - Loss: 0.0680 - Accuracy: 0.9121\n",
      "Batch 740 - Loss: 0.0513 - Accuracy: 0.9124\n",
      "Batch 750 - Loss: 0.0774 - Accuracy: 0.9126\n",
      "Batch 760 - Loss: 0.2232 - Accuracy: 0.9128\n",
      "Batch 770 - Loss: 0.0458 - Accuracy: 0.9133\n",
      "Batch 780 - Loss: 0.2780 - Accuracy: 0.9139\n",
      "Batch 790 - Loss: 0.3254 - Accuracy: 0.9140\n",
      "Batch 800 - Loss: 0.3071 - Accuracy: 0.9138\n",
      "Batch 810 - Loss: 0.1265 - Accuracy: 0.9141\n",
      "Batch 820 - Loss: 0.0692 - Accuracy: 0.9138\n",
      "Batch 830 - Loss: 0.4716 - Accuracy: 0.9141\n",
      "Batch 840 - Loss: 0.1374 - Accuracy: 0.9142\n",
      "Batch 850 - Loss: 0.1413 - Accuracy: 0.9141\n",
      "Batch 860 - Loss: 0.1088 - Accuracy: 0.9140\n",
      "Batch 870 - Loss: 0.1058 - Accuracy: 0.9143\n",
      "Batch 880 - Loss: 0.3262 - Accuracy: 0.9143\n",
      "Batch 890 - Loss: 0.1358 - Accuracy: 0.9144\n",
      "Batch 900 - Loss: 0.5489 - Accuracy: 0.9138\n",
      "Batch 910 - Loss: 0.1848 - Accuracy: 0.9140\n",
      "Batch 920 - Loss: 0.6595 - Accuracy: 0.9139\n",
      "Batch 930 - Loss: 0.0994 - Accuracy: 0.9137\n",
      "Batch 940 - Loss: 0.0831 - Accuracy: 0.9140\n",
      "Batch 950 - Loss: 0.3205 - Accuracy: 0.9143\n",
      "Batch 960 - Loss: 0.3253 - Accuracy: 0.9145\n",
      "Batch 970 - Loss: 0.3356 - Accuracy: 0.9147\n",
      "Batch 980 - Loss: 0.1173 - Accuracy: 0.9148\n",
      "Batch 990 - Loss: 0.2039 - Accuracy: 0.9149\n",
      "Epoch 3 - Loss: 0.2153 - Accuracy: 0.9149\n",
      "\n",
      "Epoch 4/5\n",
      "Batch 0 - Loss: 0.1371 - Accuracy: 1.0000\n",
      "Batch 10 - Loss: 0.1125 - Accuracy: 0.9318\n",
      "Batch 20 - Loss: 0.1068 - Accuracy: 0.9464\n",
      "Batch 30 - Loss: 0.0907 - Accuracy: 0.9415\n",
      "Batch 40 - Loss: 0.1935 - Accuracy: 0.9466\n",
      "Batch 50 - Loss: 0.0237 - Accuracy: 0.9522\n",
      "Batch 60 - Loss: 0.0786 - Accuracy: 0.9508\n",
      "Batch 70 - Loss: 0.2874 - Accuracy: 0.9463\n",
      "Batch 80 - Loss: 0.1216 - Accuracy: 0.9444\n",
      "Batch 90 - Loss: 0.0650 - Accuracy: 0.9396\n",
      "Batch 100 - Loss: 0.2007 - Accuracy: 0.9381\n",
      "Batch 110 - Loss: 0.2019 - Accuracy: 0.9364\n",
      "Batch 120 - Loss: 0.0779 - Accuracy: 0.9396\n",
      "Batch 130 - Loss: 0.0989 - Accuracy: 0.9408\n",
      "Batch 140 - Loss: 0.1193 - Accuracy: 0.9406\n",
      "Batch 150 - Loss: 0.1064 - Accuracy: 0.9425\n",
      "Batch 160 - Loss: 0.0096 - Accuracy: 0.9441\n",
      "Batch 170 - Loss: 0.0361 - Accuracy: 0.9452\n",
      "Batch 180 - Loss: 0.3412 - Accuracy: 0.9468\n",
      "Batch 190 - Loss: 0.1269 - Accuracy: 0.9473\n",
      "Batch 200 - Loss: 0.1107 - Accuracy: 0.9478\n",
      "Batch 210 - Loss: 0.0373 - Accuracy: 0.9488\n",
      "Batch 220 - Loss: 0.0887 - Accuracy: 0.9497\n",
      "Batch 230 - Loss: 0.4007 - Accuracy: 0.9491\n",
      "Batch 240 - Loss: 0.0695 - Accuracy: 0.9494\n",
      "Batch 250 - Loss: 0.0583 - Accuracy: 0.9497\n",
      "Batch 260 - Loss: 0.0268 - Accuracy: 0.9502\n",
      "Batch 270 - Loss: 0.1677 - Accuracy: 0.9490\n",
      "Batch 280 - Loss: 0.0306 - Accuracy: 0.9504\n",
      "Batch 290 - Loss: 0.0221 - Accuracy: 0.9508\n",
      "Batch 300 - Loss: 0.1576 - Accuracy: 0.9500\n",
      "Batch 310 - Loss: 0.0821 - Accuracy: 0.9510\n",
      "Batch 320 - Loss: 0.0627 - Accuracy: 0.9511\n",
      "Batch 330 - Loss: 0.0149 - Accuracy: 0.9503\n",
      "Batch 340 - Loss: 0.2268 - Accuracy: 0.9500\n",
      "Batch 350 - Loss: 0.0392 - Accuracy: 0.9505\n",
      "Batch 360 - Loss: 0.0194 - Accuracy: 0.9515\n",
      "Batch 370 - Loss: 0.0905 - Accuracy: 0.9522\n",
      "Batch 380 - Loss: 0.0525 - Accuracy: 0.9529\n",
      "Batch 390 - Loss: 0.0347 - Accuracy: 0.9535\n",
      "Batch 400 - Loss: 0.0279 - Accuracy: 0.9536\n",
      "Batch 410 - Loss: 0.0349 - Accuracy: 0.9532\n",
      "Batch 420 - Loss: 0.2049 - Accuracy: 0.9532\n",
      "Batch 430 - Loss: 0.0397 - Accuracy: 0.9533\n",
      "Batch 440 - Loss: 0.1604 - Accuracy: 0.9534\n",
      "Batch 450 - Loss: 0.0297 - Accuracy: 0.9530\n",
      "Batch 460 - Loss: 0.0118 - Accuracy: 0.9536\n",
      "Batch 470 - Loss: 0.0374 - Accuracy: 0.9541\n",
      "Batch 480 - Loss: 0.0666 - Accuracy: 0.9539\n",
      "Batch 490 - Loss: 0.0872 - Accuracy: 0.9540\n",
      "Batch 500 - Loss: 0.0236 - Accuracy: 0.9541\n",
      "Batch 510 - Loss: 0.2671 - Accuracy: 0.9536\n",
      "Batch 520 - Loss: 0.2283 - Accuracy: 0.9537\n",
      "Batch 530 - Loss: 0.1500 - Accuracy: 0.9529\n",
      "Batch 540 - Loss: 0.2885 - Accuracy: 0.9530\n",
      "Batch 550 - Loss: 0.2692 - Accuracy: 0.9530\n",
      "Batch 560 - Loss: 0.1674 - Accuracy: 0.9529\n",
      "Batch 570 - Loss: 0.5698 - Accuracy: 0.9529\n",
      "Batch 580 - Loss: 0.1362 - Accuracy: 0.9531\n",
      "Batch 590 - Loss: 0.0508 - Accuracy: 0.9530\n",
      "Batch 600 - Loss: 0.0238 - Accuracy: 0.9529\n",
      "Batch 610 - Loss: 0.0433 - Accuracy: 0.9529\n",
      "Batch 620 - Loss: 0.0577 - Accuracy: 0.9529\n",
      "Batch 630 - Loss: 0.2620 - Accuracy: 0.9528\n",
      "Batch 640 - Loss: 0.0689 - Accuracy: 0.9523\n",
      "Batch 650 - Loss: 0.1594 - Accuracy: 0.9524\n",
      "Batch 660 - Loss: 0.1574 - Accuracy: 0.9526\n",
      "Batch 670 - Loss: 0.0502 - Accuracy: 0.9530\n",
      "Batch 680 - Loss: 0.0508 - Accuracy: 0.9533\n",
      "Batch 690 - Loss: 0.0210 - Accuracy: 0.9533\n",
      "Batch 700 - Loss: 0.0363 - Accuracy: 0.9535\n",
      "Batch 710 - Loss: 0.1293 - Accuracy: 0.9535\n",
      "Batch 720 - Loss: 0.2402 - Accuracy: 0.9536\n",
      "Batch 730 - Loss: 0.3086 - Accuracy: 0.9534\n",
      "Batch 740 - Loss: 0.2712 - Accuracy: 0.9530\n",
      "Batch 750 - Loss: 0.0258 - Accuracy: 0.9528\n",
      "Batch 760 - Loss: 0.2729 - Accuracy: 0.9529\n",
      "Batch 770 - Loss: 0.0184 - Accuracy: 0.9531\n",
      "Batch 780 - Loss: 0.2190 - Accuracy: 0.9535\n",
      "Batch 790 - Loss: 0.4381 - Accuracy: 0.9534\n",
      "Batch 800 - Loss: 0.0974 - Accuracy: 0.9535\n",
      "Batch 810 - Loss: 0.1961 - Accuracy: 0.9536\n",
      "Batch 820 - Loss: 0.0959 - Accuracy: 0.9536\n",
      "Batch 830 - Loss: 0.0297 - Accuracy: 0.9537\n",
      "Batch 840 - Loss: 0.1095 - Accuracy: 0.9536\n",
      "Batch 850 - Loss: 0.0526 - Accuracy: 0.9537\n",
      "Batch 860 - Loss: 0.1340 - Accuracy: 0.9538\n",
      "Batch 870 - Loss: 0.1267 - Accuracy: 0.9540\n",
      "Batch 880 - Loss: 0.1007 - Accuracy: 0.9540\n",
      "Batch 890 - Loss: 0.0715 - Accuracy: 0.9540\n",
      "Batch 900 - Loss: 0.3691 - Accuracy: 0.9539\n",
      "Batch 910 - Loss: 0.0134 - Accuracy: 0.9542\n",
      "Batch 920 - Loss: 0.3922 - Accuracy: 0.9543\n",
      "Batch 930 - Loss: 0.0530 - Accuracy: 0.9544\n",
      "Batch 940 - Loss: 0.0268 - Accuracy: 0.9545\n",
      "Batch 950 - Loss: 0.2560 - Accuracy: 0.9548\n",
      "Batch 960 - Loss: 0.0794 - Accuracy: 0.9551\n",
      "Batch 970 - Loss: 0.2080 - Accuracy: 0.9553\n",
      "Batch 980 - Loss: 0.0676 - Accuracy: 0.9553\n",
      "Batch 990 - Loss: 0.2097 - Accuracy: 0.9551\n",
      "Epoch 4 - Loss: 0.1263 - Accuracy: 0.9554\n",
      "\n",
      "Epoch 5/5\n",
      "Batch 0 - Loss: 0.1538 - Accuracy: 0.9375\n",
      "Batch 10 - Loss: 0.0475 - Accuracy: 0.9318\n",
      "Batch 20 - Loss: 0.0467 - Accuracy: 0.9375\n",
      "Batch 30 - Loss: 0.0376 - Accuracy: 0.9456\n",
      "Batch 40 - Loss: 0.1055 - Accuracy: 0.9543\n",
      "Batch 50 - Loss: 0.0419 - Accuracy: 0.9583\n",
      "Batch 60 - Loss: 0.0633 - Accuracy: 0.9590\n",
      "Batch 70 - Loss: 0.2542 - Accuracy: 0.9613\n",
      "Batch 80 - Loss: 0.0635 - Accuracy: 0.9606\n",
      "Batch 90 - Loss: 0.0299 - Accuracy: 0.9609\n",
      "Batch 100 - Loss: 0.0184 - Accuracy: 0.9616\n",
      "Batch 110 - Loss: 0.0921 - Accuracy: 0.9617\n",
      "Batch 120 - Loss: 0.0254 - Accuracy: 0.9633\n",
      "Batch 130 - Loss: 0.0095 - Accuracy: 0.9647\n",
      "Batch 140 - Loss: 0.0129 - Accuracy: 0.9659\n",
      "Batch 150 - Loss: 0.0164 - Accuracy: 0.9656\n",
      "Batch 160 - Loss: 0.0157 - Accuracy: 0.9670\n",
      "Batch 170 - Loss: 0.0081 - Accuracy: 0.9664\n",
      "Batch 180 - Loss: 0.0115 - Accuracy: 0.9675\n",
      "Batch 190 - Loss: 0.0159 - Accuracy: 0.9666\n",
      "Batch 200 - Loss: 0.0392 - Accuracy: 0.9661\n",
      "Batch 210 - Loss: 0.0131 - Accuracy: 0.9668\n",
      "Batch 220 - Loss: 0.0941 - Accuracy: 0.9669\n",
      "Batch 230 - Loss: 0.3106 - Accuracy: 0.9675\n",
      "Batch 240 - Loss: 0.0077 - Accuracy: 0.9681\n",
      "Batch 250 - Loss: 0.0113 - Accuracy: 0.9681\n",
      "Batch 260 - Loss: 0.2946 - Accuracy: 0.9682\n",
      "Batch 270 - Loss: 0.0438 - Accuracy: 0.9670\n",
      "Batch 280 - Loss: 0.0063 - Accuracy: 0.9677\n",
      "Batch 290 - Loss: 0.0189 - Accuracy: 0.9678\n",
      "Batch 300 - Loss: 0.0179 - Accuracy: 0.9678\n",
      "Batch 310 - Loss: 0.0064 - Accuracy: 0.9680\n",
      "Batch 320 - Loss: 0.0348 - Accuracy: 0.9679\n",
      "Batch 330 - Loss: 0.0134 - Accuracy: 0.9681\n",
      "Batch 340 - Loss: 0.0539 - Accuracy: 0.9685\n",
      "Batch 350 - Loss: 0.0260 - Accuracy: 0.9685\n",
      "Batch 360 - Loss: 0.0171 - Accuracy: 0.9688\n",
      "Batch 370 - Loss: 0.0157 - Accuracy: 0.9693\n",
      "Batch 380 - Loss: 0.0491 - Accuracy: 0.9698\n",
      "Batch 390 - Loss: 0.1066 - Accuracy: 0.9701\n",
      "Batch 400 - Loss: 0.0192 - Accuracy: 0.9702\n",
      "Batch 410 - Loss: 0.0206 - Accuracy: 0.9699\n",
      "Batch 420 - Loss: 0.2232 - Accuracy: 0.9694\n",
      "Batch 430 - Loss: 0.0757 - Accuracy: 0.9697\n",
      "Batch 440 - Loss: 0.0376 - Accuracy: 0.9695\n",
      "Batch 450 - Loss: 0.0088 - Accuracy: 0.9695\n",
      "Batch 460 - Loss: 0.0062 - Accuracy: 0.9696\n",
      "Batch 470 - Loss: 0.0324 - Accuracy: 0.9699\n",
      "Batch 480 - Loss: 0.0778 - Accuracy: 0.9704\n",
      "Batch 490 - Loss: 0.0355 - Accuracy: 0.9703\n",
      "Batch 500 - Loss: 0.0165 - Accuracy: 0.9704\n",
      "Batch 510 - Loss: 0.0295 - Accuracy: 0.9702\n",
      "Batch 520 - Loss: 0.0115 - Accuracy: 0.9704\n",
      "Batch 530 - Loss: 0.1595 - Accuracy: 0.9705\n",
      "Batch 540 - Loss: 0.0310 - Accuracy: 0.9705\n",
      "Batch 550 - Loss: 0.2575 - Accuracy: 0.9706\n",
      "Batch 560 - Loss: 0.0699 - Accuracy: 0.9708\n",
      "Batch 570 - Loss: 0.2004 - Accuracy: 0.9708\n",
      "Batch 580 - Loss: 0.0127 - Accuracy: 0.9705\n",
      "Batch 590 - Loss: 0.0821 - Accuracy: 0.9706\n",
      "Batch 600 - Loss: 0.0180 - Accuracy: 0.9704\n",
      "Batch 610 - Loss: 0.0257 - Accuracy: 0.9704\n",
      "Batch 620 - Loss: 0.2226 - Accuracy: 0.9705\n",
      "Batch 630 - Loss: 0.0105 - Accuracy: 0.9704\n",
      "Batch 640 - Loss: 0.0825 - Accuracy: 0.9706\n",
      "Batch 650 - Loss: 0.0135 - Accuracy: 0.9705\n",
      "Batch 660 - Loss: 0.0631 - Accuracy: 0.9703\n",
      "Batch 670 - Loss: 0.0184 - Accuracy: 0.9703\n",
      "Batch 680 - Loss: 0.0394 - Accuracy: 0.9705\n",
      "Batch 690 - Loss: 0.0109 - Accuracy: 0.9706\n",
      "Batch 700 - Loss: 0.0188 - Accuracy: 0.9708\n",
      "Batch 710 - Loss: 0.0067 - Accuracy: 0.9709\n",
      "Batch 720 - Loss: 0.1966 - Accuracy: 0.9712\n",
      "Batch 730 - Loss: 0.0909 - Accuracy: 0.9714\n",
      "Batch 740 - Loss: 0.2909 - Accuracy: 0.9714\n",
      "Batch 750 - Loss: 0.2441 - Accuracy: 0.9715\n",
      "Batch 760 - Loss: 0.1564 - Accuracy: 0.9710\n",
      "Batch 770 - Loss: 0.0251 - Accuracy: 0.9709\n",
      "Batch 780 - Loss: 0.0434 - Accuracy: 0.9710\n",
      "Batch 790 - Loss: 0.0933 - Accuracy: 0.9709\n",
      "Batch 800 - Loss: 0.0127 - Accuracy: 0.9710\n",
      "Batch 810 - Loss: 0.0120 - Accuracy: 0.9712\n",
      "Batch 820 - Loss: 0.0181 - Accuracy: 0.9715\n",
      "Batch 830 - Loss: 0.1533 - Accuracy: 0.9716\n",
      "Batch 840 - Loss: 0.0058 - Accuracy: 0.9715\n",
      "Batch 850 - Loss: 0.0233 - Accuracy: 0.9714\n",
      "Batch 860 - Loss: 0.0069 - Accuracy: 0.9713\n",
      "Batch 870 - Loss: 0.0317 - Accuracy: 0.9713\n",
      "Batch 880 - Loss: 0.1033 - Accuracy: 0.9714\n",
      "Batch 890 - Loss: 0.0852 - Accuracy: 0.9714\n",
      "Batch 900 - Loss: 0.1369 - Accuracy: 0.9713\n",
      "Batch 910 - Loss: 0.1924 - Accuracy: 0.9712\n",
      "Batch 920 - Loss: 0.4473 - Accuracy: 0.9710\n",
      "Batch 930 - Loss: 0.0085 - Accuracy: 0.9710\n",
      "Batch 940 - Loss: 0.0276 - Accuracy: 0.9710\n",
      "Batch 950 - Loss: 0.0459 - Accuracy: 0.9711\n",
      "Batch 960 - Loss: 0.0227 - Accuracy: 0.9711\n",
      "Batch 970 - Loss: 0.0104 - Accuracy: 0.9712\n",
      "Batch 980 - Loss: 0.0139 - Accuracy: 0.9712\n",
      "Batch 990 - Loss: 0.0100 - Accuracy: 0.9711\n",
      "Epoch 5 - Loss: 0.0843 - Accuracy: 0.9712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/23 01:40:39 INFO mlflow.tracking._tracking_service.client: üèÉ View run BERT at: http://mlflow-server:5000/#/experiments/6/runs/4c4a6a811ebc4328903983e4cb188778.\n",
      "2024/11/23 01:40:39 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://mlflow-server:5000/#/experiments/6.\n"
     ]
    }
   ],
   "source": [
    "# Configuration MLflow\n",
    "experiment = mlflow.set_experiment(\"BERT\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"BERT\", nested=True):\n",
    "    # Enregistrement des hyperparam√®tres\n",
    "    mlflow.log_param(\"model_name\", \"bert-base-uncased\")\n",
    "    mlflow.log_param(\"num_epochs\", epochs)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"learning_rate\", 1e-5)\n",
    "    \n",
    "    # Boucle d'entra√Ænement\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        train_accuracy_metric.reset_state()  # R√©initialiser l'accuracy pour chaque epoch\n",
    "        epoch_loss = []  # R√©initialiser la liste des pertes pour chaque epoch\n",
    "\n",
    "        for i in range(0, len(train_input_ids), batch_size):\n",
    "            # Obtenir un batch de donn√©es\n",
    "            batch_input_ids = train_input_ids[i:i + batch_size]\n",
    "            batch_attention_masks = train_attention_masks[i:i + batch_size]\n",
    "            batch_labels = train_labels[i:i + batch_size]\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                # Faire des pr√©dictions\n",
    "                outputs = model(\n",
    "                    input_ids=batch_input_ids,\n",
    "                    attention_mask=batch_attention_masks,\n",
    "                    training=True\n",
    "                )\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Calculer la perte\n",
    "                loss = loss_fn(batch_labels, logits)\n",
    "            \n",
    "            # Calculer et appliquer les gradients\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "            # Mettre √† jour l'accuracy et accumuler la perte\n",
    "            train_accuracy_metric.update_state(batch_labels, logits)\n",
    "            epoch_loss.append(loss.numpy())\n",
    "            \n",
    "            # Afficher la perte et l'accuracy toutes les 10 batches\n",
    "            if i % (batch_size * 10) == 0:\n",
    "                train_accuracy = train_accuracy_metric.result().numpy()\n",
    "                print(f\"Batch {i//batch_size} - Loss: {loss.numpy():.4f} - Accuracy: {train_accuracy:.4f}\")\n",
    "                mlflow.log_metric(\"batch_train_loss\", loss.numpy(), step=i//batch_size)\n",
    "                mlflow.log_metric(\"batch_train_accuracy\", train_accuracy, step=i//batch_size)\n",
    "        \n",
    "        # Enregistrement des m√©triques de l'√©poque\n",
    "        epoch_accuracy = train_accuracy_metric.result().numpy()\n",
    "        epoch_loss_avg = np.mean(epoch_loss)\n",
    "        print(f\"Epoch {epoch + 1} - Loss: {epoch_loss_avg:.4f} - Accuracy: {epoch_accuracy:.4f}\")\n",
    "        mlflow.log_metric(\"epoch_train_loss\", epoch_loss_avg, step=epoch)\n",
    "        mlflow.log_metric(\"epoch_train_accuracy\", epoch_accuracy, step=epoch)\n",
    "\n",
    "    # Lib√©rer la m√©moire GPU/CPU non utilis√©e avant l'√©valuation\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "def log_plot_to_mlflow(figure, artifact_name):\n",
    "    \"\"\"\n",
    "    Enregistre une figure matplotlib dans MLflow en utilisant un buffer en m√©moire.\n",
    "\n",
    "    :param figure: la figure matplotlib √† sauvegarder\n",
    "    :param artifact_name: le nom de l'artefact pour MLflow (inclure \".png\")\n",
    "    \"\"\"\n",
    "    # Utilisation d'un buffer en m√©moire\n",
    "    buffer = BytesIO()\n",
    "    figure.savefig(buffer, format=\"png\")\n",
    "    buffer.seek(0)\n",
    "    \n",
    "    # Sauvegarder dans MLflow √† partir du buffer\n",
    "    with open(artifact_name, \"wb\") as f:\n",
    "        f.write(buffer.getvalue())\n",
    "    mlflow.log_artifact(artifact_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/23 09:50:07 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.11/site-packages/transformers/generation/tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n",
      "2024/11/23 09:50:13 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpjf0iw5im/model, flavor: keras). Fall back to return ['keras==3.6.0']. Set logging level to DEBUG to see the full traceback. \n",
      "2024/11/23 09:50:13 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Registered model 'bert-base-uncased' already exists. Creating a new version of this model...\n",
      "2024/11/23 09:50:13 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: bert-base-uncased, version 11\n",
      "Created version '11' of model 'bert-base-uncased'.\n",
      "2024/11/23 09:50:13 INFO mlflow.tracking._tracking_service.client: üèÉ View run sassy-hound-2 at: http://mlflow-server:5000/#/experiments/6/runs/e32580dc0917460d89806e404ec5d9f7.\n",
      "2024/11/23 09:50:13 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://mlflow-server:5000/#/experiments/6.\n"
     ]
    }
   ],
   "source": [
    " # Calcul et stockage des m√©triques dans un dictionnaire\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc, log_loss\n",
    "all_validation_metrics = []   \n",
    "# √âvaluation du mod√®le sur le set de validation en mini-batches\n",
    "batch_size = 8  # Taille r√©duite pour l'√©valuation\n",
    "num_batches = len(val_input_ids) // batch_size\n",
    "all_predictions = []\n",
    "# Initialisation du dictionnaire des m√©triques\n",
    "metrics_dict = {}\n",
    "all_probs = []\n",
    "# Initialisation d'une liste pour stocker les log loss par batch (optionnel, pour analyse batch-wise)\n",
    "log_losses = []\n",
    "\n",
    "for i in range(num_batches):\n",
    "    # Obtenir un batch de validation\n",
    "    batch_input_ids = val_input_ids[i * batch_size : (i + 1) * batch_size]\n",
    "    batch_attention_masks = val_attention_masks[i * batch_size : (i + 1) * batch_size]\n",
    "    batch_labels = val_labels[i * batch_size : (i + 1) * batch_size]  # R√©cup√©rer les labels correspondants\n",
    "\n",
    "\n",
    "    # Calcul des logits pour le batch\n",
    "    batch_logits = model(\n",
    "        input_ids=batch_input_ids,\n",
    "        attention_mask=batch_attention_masks,\n",
    "        training=False\n",
    "    ).logits\n",
    "\n",
    "    # Stocker les pr√©dictions\n",
    "    batch_predictions = tf.argmax(batch_logits, axis=1)\n",
    "    all_predictions.append(batch_predictions)\n",
    "    \n",
    "     # Convertir logits en probabilit√©s\n",
    "    batch_probabilities = tf.nn.softmax(batch_logits, axis=1)[:, 1]  # Probabilit√©s pour la classe 1\n",
    "    all_probs.append(batch_probabilities)\n",
    "    \n",
    "    # Calcul du log loss pour ce batch (en explicitant les classes [0, 1])\n",
    "    batch_log_loss = log_loss(batch_labels, batch_probabilities, labels=[0, 1])\n",
    "    log_losses.append(batch_log_loss)\n",
    "\n",
    "\n",
    "# Concat√©ner toutes les pr√©dictions\n",
    "all_predictions = tf.concat(all_predictions, axis=0)\n",
    "\n",
    "# Concat√©ner toutes les probabilit√©s\n",
    "all_probs = tf.concat(all_probs, axis=0).numpy()\n",
    "\n",
    "\n",
    "\n",
    "# Calcul du log loss total sur l'ensemble des donn√©es\n",
    "# total_log_loss = log_loss(val_labels[:len(all_probs)], all_probs, labels=[0, 1])\n",
    "# validation_metrics[\"Validation Log Loss\"] = round(total_log_loss, 3)\n",
    "# print(f\"Validation Log Loss: {total_log_loss:.4f}\")\n",
    "\n",
    "# # Tracer le log loss par batch\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(range(len(log_losses)), log_losses, marker='o', label=\"Log Loss par batch\")\n",
    "# plt.axhline(total_log_loss, color='red', linestyle='--', label=f\"Log Loss total ({total_log_loss:.4f})\")\n",
    "# plt.xlabel(\"Index des batches\")\n",
    "# plt.ylabel(\"Log Loss\")\n",
    "# plt.title(\"Logarithmic Loss par Batch\")\n",
    "# plt.legend()\n",
    "# log_plot_to_mlflow(plt.gcf(), \"log_loss_batches.png\")\n",
    "# plt.show()\n",
    "\n",
    "# # Ajout du log loss dans MLflow\n",
    "# mlflow.log_metric(\"val_log_loss\", total_log_loss)\n",
    "\n",
    "\n",
    "\n",
    "# Calcul de la courbe ROC\n",
    "fpr, tpr, thresholds = roc_curve(val_labels[:len(all_probs)], all_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Tracer la courbe ROC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random guess\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Bert ROC Curve - Validation Set\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "log_plot_to_mlflow(plt.gcf(), \"roc_curve_val.png\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "# Calcul de l'accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(all_predictions == val_labels[:len(all_predictions)], dtype=tf.float32))\n",
    "print(f\"Validation Accuracy: {accuracy.numpy():.4f}\")\n",
    "\n",
    "validation_metrics = {\n",
    "    \"Validation Accuracy\": round(float(accuracy.numpy()), 3),\n",
    "    \"Validation ROC AUC\": round(roc_auc, 3),\n",
    "    \"Validation Precision\": round(precision_score(val_labels[:len(all_predictions)], all_predictions.numpy()), 3),\n",
    "    \"Validation Recall\": round(recall_score(val_labels[:len(all_predictions)], all_predictions.numpy()), 3),\n",
    "    \"Validation F1\": round(f1_score(val_labels[:len(all_predictions)], all_predictions.numpy()), 3)\n",
    "}\n",
    "\n",
    "metrics_dict.update(validation_metrics)\n",
    "# Log de l'accuracy finale de validation et des autres m√©triques dans MLflow\n",
    "mlflow.log_metrics(metrics_dict)\n",
    "\n",
    "# Log de l'accuracy finale de validation dans MLflow\n",
    "mlflow.log_metric(\"val_accuracy\", accuracy.numpy())\n",
    "\n",
    "# Enregistrement du mod√®le\n",
    "mlflow.keras.log_model(model, \"bert_model\")\n",
    "\n",
    "run_id = mlflow.active_run().info.run_id\n",
    "result = mlflow.register_model(\n",
    "    model_uri=f\"runs:/{run_id}/model\",\n",
    "    name=f\"bert-base-uncased\"\n",
    ")\n",
    "\n",
    "# Construire le lien MLflow correspondant (en supposant que vous avez l'URL de votre serveur MLflow)\n",
    "active_run = mlflow.active_run()\n",
    "run_id = active_run.info.run_id\n",
    "#run_name = active_run.data.tags.get(\"mlflow.runName\")  # Obtenir le nom du run depuis les tags\n",
    "\n",
    "# R√©cup√©rer l'ID de l'exp√©rience active\n",
    "experiment_id = active_run.info.experiment_id\n",
    "\n",
    "# D√©finir l'URL de votre serveur MLflow\n",
    "mlflow_server_url = \"http://localhost:5000\"  # Remplacez par l'URL r√©el de votre serveur MLflow\n",
    "\n",
    "# Construire le lien complet vers le run dans l'interface MLflow\n",
    "run_link = f\"{mlflow_server_url}/#/experiments/{experiment_id}/runs/{run_id}\"\n",
    "\n",
    "# Ajouter les m√©triques de validation √† la liste\n",
    "all_validation_metrics.append({\n",
    "    \"run_name\": f\"bert-base-uncased-trained\",\n",
    "    **validation_metrics,\n",
    "    \"run_id\": run_link\n",
    "})\n",
    "mlflow.log_table(pd.DataFrame(all_validation_metrics), \"3-bert-base-uncased.json\")\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Validation Accuracy': 0.812,\n",
       " 'Validation ROC AUC': 0.89,\n",
       " 'Validation Precision': 0.787,\n",
       " 'Validation Recall': 0.852,\n",
       " 'Validation F1': 0.818}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(validation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Validation ROC AUC</th>\n",
       "      <th>Validation Precision</th>\n",
       "      <th>Validation Recall</th>\n",
       "      <th>Validation F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.812</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Validation Accuracy  Validation ROC AUC  Validation Precision  \\\n",
       "0                0.812                0.89                 0.787   \n",
       "\n",
       "   Validation Recall  Validation F1  \n",
       "0              0.852          0.818  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(metrics_dict, index=[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Globale\n",
    "\n",
    "## Points Forts\n",
    "- Le mod√®le montre une **bonne capacit√© de g√©n√©ralisation** avec :\n",
    "  - Une **Accuracy** respectable de 81.2%.\n",
    "  - Un **AUC-ROC √©lev√©** de 0.89, indiquant une bonne capacit√© √† distinguer les classes.\n",
    "- Le **Rappel √©lev√© (85.2%)** montre que le mod√®le d√©tecte efficacement les vrais positifs, \n",
    "  ce qui est souvent crucial dans des applications critiques.\n",
    "\n",
    "## Points d'Am√©lioration\n",
    "- La **Pr√©cision (78.7%)** est l√©g√®rement inf√©rieure au Rappel, ce qui sugg√®re la pr√©sence \n",
    "  de quelques **faux positifs**.\n",
    "\n",
    "## Questions √† Explorer\n",
    "1. **Pourquoi un d√©s√©quilibre entre pr√©cision et rappel ?**\n",
    "   - Le mod√®le semble favoriser le rappel (capturer les vrais positifs) au d√©triment de la pr√©cision (Tweet neutre?).\n",
    "2. **Quel est l'impact m√©tier des faux positifs et des faux n√©gatifs ?**\n",
    "   - Comprendre les co√ªts sp√©cifiques de ces erreurs est crucial pour affiner le mod√®le.\n",
    "\n",
    "---\n",
    "\n",
    "# Recommandations\n",
    "\n",
    "1. **Inspecter la matrice de confusion :**\n",
    "   - Cela permettra de mieux comprendre les types d'erreurs commises \n",
    "     (faux positifs et faux n√©gatifs).\n",
    "\n",
    "2. **Ajuster le seuil de classification :**\n",
    "   - Modifier le seuil peut √©quilibrer la Pr√©cision et le Rappel selon les priorit√©s m√©tier.\n",
    "\n",
    "3. **Tester sur d'autres m√©triques m√©tier :**\n",
    "   - Par exemple, le co√ªt pond√©r√© des erreurs ou le taux de faux positifs \n",
    "     peut √™tre plus pertinent pour des besoins sp√©cifiques.\n",
    "\n",
    "4. **Analyse approfondie des erreurs :**\n",
    "   - √âtudier les tweets mal classifi√©s pour identifier des tendances \n",
    "     ou des motifs que le mod√®le ne capte pas correctement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Le mod√®le **`bert-base-uncased`** a montr√© des performances prometteuses sur notre jeu de donn√©es, surpassant les r√©sultats obtenus avec certains mod√®les sp√©cialis√©s comme ceux adapt√©s √† Twitter. Toutefois, la gestion des faux positifs et l'√©quilibrage entre Pr√©cision et Rappel n√©cessitent une attention particuli√®re. Une √©tude plus pouss√©e des erreurs et des ajustements de seuil pourraient encore am√©liorer ses performances et sa pertinence dans le contexte m√©tier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
